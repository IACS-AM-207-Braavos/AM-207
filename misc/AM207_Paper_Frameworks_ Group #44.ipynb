{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APMTH 207: Advanced Scientific Computing: \n",
    "## Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "## Group Project -- Final Paper\n",
    "**Harvard University**<br>\n",
    "**Fall 2018**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date: ** Tuesday, December 11th, 2018 at 11:59pm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesians take on the Belamies\n",
    "\n",
    "##### Paper: \"Bayesian GAN\" by Yunus Saatchi and Andrew Gordon Wilson\n",
    "\n",
    "##### Url: https://arxiv.org/pdf/1705.09558.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Members of the Group (give us the names and emails of all collaborators):**\n",
    "\n",
    "-- Collaborator 1: Dylan Randle dylanrandle@g.harvard.edu\n",
    "\n",
    "-- Collaborator 2: Michael S. Emanuel mse999@g.harvard.edu\n",
    "\n",
    "-- Collaborator 3: Anna Davydova davydova@g.harvard.edu\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Group(Name of Project Group in Canvas):**\n",
    "\n",
    "-- Project Group Name (FAS): PaperTutorial 44\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gratuitous Titular Reference:\n",
    "The painting below is titled [Potrait of Edmond Belamy](https://www.christies.com/features/A-collaboration-between-two-artists-one-human-one-a-machine-9332-1.aspx) by Obvious, 2018. This portrait is part of the La Famille de Belamy series created via GAN. It was the first artwork created by AI to be auctioned and sold at Christie's for USD434,000 in October of 2018. In the same auction, several of Andy Warhol's prints sold under USD50,000 and Roy Lichtenstein's Crying Girl sold for [USD87,500](https://www.christies.com/prints-and-multiples-27814.aspx?saletitle=). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"400\" length='700' alt=\"portfolio_view\" src=Edmond_Belamy1.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract: \n",
    "\n",
    "In their paper, Yunus Saatchi and Andrew Gordon Wilson argue that applying Bayesian framework to general adversarial network (GAN) would help prevent mode collapse and result in a more straightforward and accurate model without feature matching or mini-batch discrimination. Specifically, they tackle GAN through a lens of fully probabilistic inference and marginalize the weights of the generator and discriminator using stochastic gradient Hamiltonian Monte Carlo. Our analysis below follows closely in the author's footsteps as we re-create their framework for Bayesian GAN and apply their findings to several examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "Â \n",
    "#### GANs Defined: \n",
    "Generative Adversarial Networks (GANs) are contained within a deep neural network architecture where two nets - Discriminator and Generator - battle it out. The discriminator usually takes form of a convolutional network that aims to correctly identify the label given a group of asscociated features. In other words the discriminator estimates p(y|X).  The generator, on the other hand, attempts to predict the features given a label (i.e. p(X|y)).  Thus, the generator learns the distribution of the data and then creates new data, while the disriminator attemps to correctly identify the label authenticity given the true data and the fake data coming from the generator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](OReillyGAN.png  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At it's core, the generator aims to generate data that resemble the truth so closely that it fools the discriminator into classifying them as authentic.  At the same time, the discrimator is facing an opposite goal of catching generator's fakes. This internal standoff between these two powerful and evolving algorithms has been used for various visualizations, image enhancement, as well as to produce high quality fake images/video content. Since this is a zero-sum game we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}L^{(G)}=-L^{(D)}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\mathop{min}_{\\textbf{G}}\n",
    "\\mathop{max}_{\\textbf{D}}\n",
    "E_{x\\sim P_{real}}[log(D(x))]+E_{h}[log(1-D(G(h)))]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Research Motivation: \n",
    "While GANs have proven themselves to be quite powerful, they do have an Achilles heel. Specifically, instances of mode collapse have been noted when the generator ends up memorizing a handfull of training examples. In addition, the authors of the paper highlight the need for meaningful intervention to ensure stability of the network, that requires feature matching, label smoothing and mini-batch discrimination.  While we are less familiar with this terminology, several papers have been written on this topic by [Radfod, Metz and Chintala](https://arxiv.org/abs/1511.06434) as well as [Salimans et al.](https://arxiv.org/abs/1606.03498).  The authors also note that most of the research aimed at fixing these issues has focused on finding a better divergence metric for the GAN model (i.e. using Wasserstein or f-divergences instead of Jensen-Shannon).  Yunus Saatchi and Andrew Gordon Wilson take a different approach and propose a Bayesian route toward a more stable GAN architecture. Specifically, the authors apply stochastic gradient Hamiltonian Monte Carlo methods to marginalize conditional posteriors over the weights of the generator and discriminator (more on this below).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian GAN Theory:\n",
    "#### Assumptions:\n",
    "Given our data set is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "D=\\{x^{(i)}\\};\n",
    "x^{(i)} \\sim p_{data}(x^{(i)})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking for:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " p_{data}(x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the white noise vector from the diagram in the introduction. In this case the authors propose that we transform white noise z$\\sim$p(z) through our generator. The authors define the Generator and the Discriminator as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " G(z;\\theta_g)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " D(x;\\theta_d)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G and D are essentially neural networks with weight vectors $\\theta_g$ and $\\theta_d$ respectively.  Here we are looking at the distributions of these $\\theta$'s and will conduct the sampling process from our priors as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " \\theta_g \\sim p(\\theta_g)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " z^1,.....,z^n \\sim p(z)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " \\tilde{x}^j=G(z^j;\\theta_g)\\sim p_{generator}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors then propose two approaches for posterior inference over our $\\theta$'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Unsupervised Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the authors sample from the following conditional posteriors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " p(\\theta_g|z,\\theta_d)\\propto\\big(\\prod_{i=1}^{n_g}D(G(z^i;\\theta_g);\\theta_d)\\big)p(\\theta_g|\\alpha_g)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " p(\\theta_d|z,X,\\theta_g)\\propto\\prod_{i=1}^{n_d}D(x^i;\\theta_d)*\\prod_{i=1}^{n_g}(1-D(G(z^i,\\theta_g);\\theta_d))* p(\\theta_d|\\alpha_d)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $p(\\theta_g|\\alpha_g)$ and $p(\\theta_d|\\alpha_d)$ are priors and $\\alpha$'s and $n$'s are the corresponding hyperparameters and mini-batch samples respectively. The authors define $X$ as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " X = \\{x^i\\}_{i=i}^{n_d}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors use Monte Carlo to marginalize the noise component z from the posterior updates.  They demonstrate this as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " p(\\theta_g|\\theta_d)= \\int p(\\theta_g,z|\\theta_d)dz=\\int p(\\theta_g|z,\\theta_d)p(z|\\theta_d)dz\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since $p(x)=p(z|\\theta_d)$, we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " p(\\theta_g|\\theta_d)\\approx\\frac{1}{J_g}\\sum_{j=1}^{J_g}p(\\theta_g|z^j,\\theta_d), z^j\\sim p(z)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by the same token we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " p(\\theta_d|\\theta_g)\\approx\\frac{1}{J_d}\\sum_{j=1}^{J_d}p(\\theta_d|z^j,X,\\theta_g), z^j\\sim p(z)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then iteratively sample posteriors from $p(\\theta_d|\\theta_g)$ and $p(\\theta_g|\\theta_d)$. The authors argue that the varied samples of $\\theta_g$ reduce the risk of mode collapse (i.e. the generators cannot simply memorize the data)and boost the performance of the discriminator. This is then matched by the scope of  $\\theta_d$ samples that further strengthen the adversarial standoff between the generator and the discriminator, making the entire model more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Semi-Supervised Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For semi-supervised approach, we will consider a data set without labels and a much smaller slice of data with labels {1....K}. Here, our model simulatneously learns the distribtuion of x both for labeled and unlabeled observations. The authors believe that this modeling approach will produce better results than fully supervised or unsupervised learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our discriminator as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " D(x^i=y^i;\\theta_d)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the discriminatore now outputs the probability that a given x belongs to a given class y.  The authors use class y=0 for any x values coming from the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then have the following posteriors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " p(\\theta_g|z,\\theta_d)\\propto\\big(\\prod_{i=1}^{n_g}\\sum_{y=1}^{K} D(G(z^i;\\theta_g)=y;\\theta_d)\\big)p(\\theta_g|\\alpha_g)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " p(\\theta_d|z,X,y_s,\\theta_g)\\propto\\prod_{i=1}^{n_d}\\sum_{y=1}^{K} D(x^i=y;\\theta_d)*\\prod_{i=1}^{n_g}(D(G(z^i,\\theta_g)=0;\\theta_d))* \\prod_{i=1}^{N_s}(D(x_s^i=y_s^i;\\theta_d))p(\\theta_d|\\alpha_d)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using three types of samples: 1) $n_g$ are the samples from the generator, 2) $n_d$ are the unlabled samples and 3) $N_s$ are labeled observations that are contained in a slice that is much smaller than the total number of observations $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying what we already know from unsupervised approach above, we marginalize the posteriors over $\\theta_d$ and $\\theta_g$. The authors propose using an average of all the samples with respect to the posterior over $\\theta_d$ to estimate class y for a given instance of x. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " p(y_j|x_j,D)=\\int p(y_j|x_j, \\theta_d)p(\\theta_d|D)d\\theta_d\\approx \\frac{1}{T}\\sum_{k=1}^{T}p(y_j|x_j,\\theta_d^k \\sim p(\\theta_d|D)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application of Stochastic Gradient HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here Anna will provide several bullets for why the authors chose to use HMC to sample from the posteriors for both unsupervised and semi_supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian GAN Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will demonstrate the *key* algorithms that are used to train the Bayesian GAN. There are a few details that we will omit for clarify of exposition, and refer our reader to the original paper for further discussion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Hamiltonian Monte Carlo (SGHMC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](OReillyGAN.png  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Celebrity Face Generation Data Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we run it on Celebrity Face Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we compare results of Bayesian GAN vs. regular GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST Data Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we run it on MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we ocmpare results of Baeysian GAN vs. regular GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Highlight the advantages of using Bayesian GAN vs. regular GAN and where we see disadvantages (authors dont mention any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final takeaways from the paper and its replicability. Some thoughts from the team on the viability of the author's approach. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
