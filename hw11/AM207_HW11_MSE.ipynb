{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 11\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Fall 2018**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date:** Saturday, December 1st, 2018 at 11:59pm\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers in the form of a Jupyter notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborators\n",
    "\n",
    "**Joe Davison<br>\n",
    "Anna Davydova<br>\n",
    "Michael S. Emanuel<br>\n",
    "Dylan Randle**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"answer-separator\">\n",
    "------------------------\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from matplotlib import cm\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Crazy Rich Bayesians Don't Need No Educations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**coding required**\n",
    "\n",
    "In this problem, you will explore how to recast data, tasks and research questions from a variety of different contexts so that an existing model can be applied for analysis.\n",
    "\n",
    "Example 10.1.3 in [\"Statistical Rethinking\"](https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlo4e4ari3r4wd%2Fj9vjyzv62x149%2Fjopa0chtr7ns%2FStatistical_Rethinking_excerpt.pdf), the excerpt of which is included with this assignment, illustrates a study of the effect of an applicant's gender on graduate school admissions to six U.C. Berkeley departments through a comparison of four models. \n",
    "\n",
    "In this problem, you are given data from the [1994 U.S. Census](https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlo4e4ari3r4wd%2Fj9vjyzv62x149%2Fjop9zvsjoscq%2Fcensus_data.csv). The data has been processed so that only a subset of the features are present (for full dataset as well as the description see the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Census+Income)). You will be investigate the effect of gender on a person's yearly income in the dataset. In particular, we want to know how a person's gender effect the likelihood of their yearly salary being above or below \\$50k. \n",
    "\n",
    "1.1. Read the dataset into a dataframe and aggregate the dataset by organizing the  dataframe into seven different categories. \n",
    "\n",
    "The categories we wish to consider are: \n",
    "* 4 year college degree\n",
    "* Some-college or two year academic college degree\n",
    "* High school\n",
    "* Professional, vocational school\n",
    "* Masters \n",
    "* Doctorate\n",
    "* Some or no high school\n",
    "\n",
    "Note that you might have to combine some of the existing education categories in your dataframe. For each category, we suggest that you only keep track of a count of the number of males and females who make above (and resp. below) the crazy rich income of $50k (see the dataset in Example 10.1.3). \n",
    "\n",
    "1.2. Following Example 10.1.3, build two models for the classification of an individual's yearly income (1 being above \\$50k and 0 being below), one of these models should include the effect of gender while the other should not. \n",
    "\n",
    "1.3. Replicate the analysis in 10.1.3 using your models; specifically, compute wAIC scores and make a plot like Figure 10.5 (posterior check) to see how well your models fits the data. \n",
    "\n",
    "1.4. Following Example 10.1.3, build two models for the classification of an individual's yearly income taking into account education. One of the models should take into account education only the other should take into account gender and education on income.\n",
    "\n",
    "1.5. Replicate the analysis in 10.1.3 using your models; specifically, compute wAIC scores and make a plot like Figure 10.6 (posterior check) to see how well your model fits the data.\n",
    "\n",
    "1.6. Using your analysis from 1.3, discuss the effect gender has on income.\n",
    "\n",
    "1.7. Using your analysis from 1.5, discuss the effect of gender on income taking into account an individual's education.\n",
    "\n",
    "(**Hint: If you haven't seen WAIC, it's because we'll be covering it on Monday November 26, 2018.  In the meantime checkout info about WAIC in this resource on [PyMC3 model selection](https://docs.pymc.io/notebooks/model_comparison.html).**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load data and aggregate it by education and sex\"\"\"\n",
    "\n",
    "    # Read in the full dataframe\n",
    "    df_full = pd.read_csv('census_data.csv', index_col=0)\n",
    "    \n",
    "    # Map from census education categories to new categories\n",
    "    education_map = {\n",
    "        'Preschool' : 'Not-HS-Grad',\n",
    "        '1st-4th' : 'Not-HS-Grad',\n",
    "        '5th-6th' : 'Not-HS-Grad',\n",
    "        '7th-8th' : 'Not-HS-Grad',\n",
    "        '9th' : 'Not-HS-Grad',\n",
    "        '10th' : 'Not-HS-Grad',\n",
    "        '11th' : 'Not-HS-Grad',\n",
    "        '12th' : 'Not-HS-Grad',\n",
    "        'HS-grad': 'HS-Grad',\n",
    "        'Some-college': 'College-Lite',\n",
    "        'Assoc-acdm': 'College-Lite',\n",
    "        'Assoc-voc': 'Prof-Voc',\n",
    "        'Prof-school': 'Prof-Voc',\n",
    "        'Bachelors': 'Bachelors',\n",
    "        'Masters': 'Masters',\n",
    "        'Doctorate': 'Doctorate'\n",
    "        }\n",
    "    \n",
    "    educationID_map = {\n",
    "        'Not-HS-Grad': 0,\n",
    "        'HS-Grad': 1,\n",
    "        'College-Lite': 2,\n",
    "        'Prof-Voc': 3,\n",
    "        'Bachelors': 4,\n",
    "        'Masters': 5,\n",
    "        'Doctorate': 6\n",
    "    }\n",
    "    \n",
    "    sexID_map = {\n",
    "            'Male' : 0,\n",
    "            'Female' : 1\n",
    "    }\n",
    "    \n",
    "    # Map from census income to a float (1.0 for high earning)\n",
    "    earning_map =  {\n",
    "        '<=50K': 0.0,\n",
    "        '>50K': 1.0}\n",
    "    \n",
    "    # New series for education and earnings\n",
    "    education = pd.Series(df_full.edu.map(education_map), name='education')\n",
    "    education_id = pd.Series(education.map(educationID_map), name='education_id', dtype='category')\n",
    "    # We also want the sex\n",
    "    sex = df_full.sex\n",
    "    sex_id = pd.Series(sex.map(sexID_map), name='sex_id', dtype='category')\n",
    "    # The earnings (trying to predict this)\n",
    "    earnings = pd.Series(df_full.earning.map(earning_map), name='earnings')\n",
    "    \n",
    "    # Create a new dataframe\n",
    "    df = pd.concat([education_id, sex_id, education, sex, earnings], axis=1, \n",
    "                   names=['education_id', 'sex_id', 'education', 'sex', 'earning'])\n",
    "    \n",
    "    # Aggregate counts of high and low earners by education for males and females as per hint\n",
    "    # For each category, we suggest that you only keep track of a count of the number of males and females \n",
    "    # who make above (and resp. below) the crazy rich income of $50k (see the dataset in Example 10.1.3).\n",
    "    # https://stackoverflow.com/questions/19384532/how-to-count-number-of-rows-per-group-and-other-statistics-in-pandas-group-by\n",
    "    gb = df.groupby(by=[education_id, sex_id])\n",
    "    counts = gb.size().to_frame(name='count')\n",
    "    df_agg = counts.join(gb.agg({'earnings': 'sum'})).reset_index()\n",
    "    # Extract male and female\n",
    "    mask_male = (df_agg.sex_id == 0)\n",
    "    mask_female = (df_agg.sex_id == 1)\n",
    "    # Frames for male and female earnings by education\n",
    "    df_male = df_agg[mask_male]\n",
    "    df_female = df_agg[mask_female]\n",
    "    # Columns desired in merge\n",
    "    columns_merge = ['education_id', 'count_male', 'earnings_male', 'count_female', 'earnings_female']\n",
    "    df_merged = pd.merge(left=df_male, right=df_female, on='education_id', suffixes=['_male', '_female'])[columns_merge]\n",
    "    df_merged['earn_lo_male'] = df_merged.count_male - df_merged.earnings_male\n",
    "    df_merged['earn_hi_male'] = df_merged.earnings_male\n",
    "    df_merged['earn_lo_female'] = df_merged.count_female - df_merged.earnings_female\n",
    "    df_merged['earn_hi_female'] = df_merged.earnings_female\n",
    "    df_merged = df_merged.drop(columns=['count_male', 'count_female', 'earnings_male', 'earnings_female'])\n",
    "\n",
    "    # Return the aggregated dataframe and \n",
    "    return df_merged, educationID_map, sexID_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>education_id</th>\n",
       "      <th>earn_lo_male</th>\n",
       "      <th>earn_hi_male</th>\n",
       "      <th>earn_lo_female</th>\n",
       "      <th>earn_hi_female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2711.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>1298.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5662.0</td>\n",
       "      <td>1449.0</td>\n",
       "      <td>3164.0</td>\n",
       "      <td>226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3732.0</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>2974.0</td>\n",
       "      <td>253.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>691.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>483.0</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1854.0</td>\n",
       "      <td>1882.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>339.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>407.0</td>\n",
       "      <td>780.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>179.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>71.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  education_id  earn_lo_male  earn_hi_male  earn_lo_female  earn_hi_female\n",
       "0            0        2711.0         221.0          1298.0            23.0\n",
       "1            1        5662.0        1449.0          3164.0           226.0\n",
       "2            2        3732.0        1399.0          2974.0           253.0\n",
       "3            3         691.0         675.0           483.0           109.0\n",
       "4            4        1854.0        1882.0          1280.0           339.0\n",
       "5            5         407.0         780.0           357.0           179.0\n",
       "6            6          71.0         256.0            36.0            50.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, educationID_map, sexID_map = load_data()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gratuitous Titular Reference**:  \n",
    "\n",
    "If you haven't watched [Crazy Rich Asians](http://www.crazyrichasiansmovie.com/) then it might be time.\n",
    "\n",
    "If you haven't listened to [Pink Floyd's](https://en.wikipedia.org/wiki/Pink_Floyd) [The Wall](https://en.wikipedia.org/wiki/The_Wall) then it might be time.\n",
    "\n",
    "Also who are you? :-)\n",
    "\n",
    "Anyway [You don't need no thought control](https://www.youtube.com/watch?v=YR5ApYxkU-U), [You probably want us teaching staff to leave you kids alone](https://www.youtube.com/watch?v=YR5ApYxkU-U), and [Education is overrated, right?  You don't need it!](https://www.youtube.com/watch?v=YR5ApYxkU-U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"answer-separator\">\n",
    "------------------------\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2:  My Sister-In-Law's Baby Cousin Tracy ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**coding required**\n",
    "\n",
    "Wikipedia describes the National Annenberg Election Survey as follows -- \"National Annenberg Election Survey (NAES) is the largest academic public opinion survey conducted during the American presidential elections. It is conducted by the Annenberg Public Policy Center at the University of Pennsylvania.\"  In the file [survey.csv](https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlo4e4ari3r4wd%2Fj9vjyzv62x149%2Fjop9sr9dh4g5%2Fsurvey.csv) we provide the following data from the 2004 National Annenberg Election Survey:  `age` -- the age of the respondents, `numr` -- the number of responses, and `knowlgbtq` -- the number of people at the given age who have at least one LGBTQ acquaintance.  We want you to model how age influences likelihood of interaction with members of the LGBTQ community in three ways. \n",
    "\n",
    "\n",
    "2.1. Using pymc3, create a bayesian linear regression model  with `age` as the quantitative predictor and `knowlgbtq` as the response variable. Plot the mean predictions for ages 0-100, with a 2-sigma envelope.\n",
    "\n",
    "2.2. Using pymc3, create a 1-D Gaussian Process regression model with the same feature and dependent variables.  Use a squared exponential covariance function. Plot the mean predictions for ages 0-100, with a 2-sigma envelope.\n",
    "\n",
    "(**Hint: For an example of GP Regression from class see [this GP Recap](http://am207.info/wiki/gpsalmon.html)**)\n",
    "\n",
    "2.3. How do the models compare? Does age influence likelihood of acquaintance with someone LGBTQ? For Bayesian Linear Regression and GP Regression, how does age affect the variance of the estimates?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gratuitous Titular References**:  \n",
    "\n",
    "Massachusett's own [Joiner Lucas](https://en.wikipedia.org/wiki/Joyner_Lucas) blew up in November 2017 with the release of his single [\"I'm Not Racist\"](https://www.youtube.com/watch?v=43gm3CJePn0) on Youtube.  The video quickly went viral. The title comes from the song's lyrics (and references that degrees of separation that can be involved in individual experience with members of any under-represented group).\n",
    "\n",
    "Given the oncoming cold spell [Winter Blues](https://www.youtube.com/watch?v=I7_ofdl9Wfs) another popular track may be relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"answer-separator\">\n",
    "------------------------\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 - AM207 HWs Out (A OK I MIC DROP)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**coding required**\n",
    "\n",
    "In the dataset [reviews_processed.csv](https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlo4e4ari3r4wd%2Fj9vjyzv62x149%2Fjoyzcmjk5tv8%2Freviews_processed.csv), you'll find a database of Yelp reviews for a number of restaurants. These reviews have already been processed and transformed by someone who has completed the (pre) modeling process described in HW 10 Question 1. That is, imagine the dataset in \"reviews_processed.csv\" is the result of feeding the raw Yelp reviews through the pipeline someone built for that question.\n",
    "\n",
    "The following is a full list of columns in the dataset and their meanings:\n",
    "\n",
    "I. Relevant to 3.1-3.5:\n",
    "\n",
    "  1. \"review_id\" - the unique identifier for each Yelp review\n",
    "  2. \"topic\" - the subject addressed by the review (0 stands for food and 1 stands for service)\n",
    "  3. \"rid\" - the unique identifier for each restaurant\n",
    "  4. \"count\" - the number of sentences in a particular review on a particular topic\n",
    "  5. \"mean\" - the probability of a sentence in a particular review on a particular topic being positive, averaged over total number of sentences in the review related to that topic.\n",
    "  6. \"var\" - the variance of the probability of a sentence in a particular review on a particular topic being positive, taken over all sentences in the review related to that topic.\n",
    "\n",
    "II. Relevant (possibly) to more complex models:\n",
    "\n",
    "  1. \"uavg\" - the average star rating given by a particular reviewer (taken across all their reviews)\n",
    "  2. \"stars\" - the number of stars given in a particular review\n",
    "  3. \"max\" - the max probability of a sentence in a particular review on a particular topic being positive\n",
    "  4. \"min\" - the min probability of a sentence in a particular review on a particular topic being positive\n",
    "\n",
    "The following schema illustrates the model of the raw data that is used to generate \"reviews_processed.csv\":\n",
    "<img src=\"https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlo4e4ari3r4wd%2Fj9vjyzv62x149%2Fjoz064i9yaxb%2Frestuarant_model.png\" width=500/>\n",
    "\n",
    "***Warning:*** *this is a \"real\" data science problem in the sense that the dataset in \"reviews_processed.csv\" is large. We understand that a number of you have limited computing resources, so you are encouraged but not required to use the entire dataset. If you wish you may use 10 restaurants from the dataset, as long as your choice of 10 contains a couple of restaurants with a large number of reviews and a couple with a small number of reviews.*\n",
    "\n",
    "\n",
    "When the value in \"count\" is low, the \"mean\" value can be very skewed.\n",
    "\n",
    "3.1. Following the [SAT prep school example discussed in lab](https://am207.info/wiki/gelmanschoolstheory.html) (and influenced your answers for HW 10 Question #1), set up a Bayesian model (that is, write functions encapsulating the pymc3 code) for a reviewer $j$'s opinion of restaurant $k$'s food and service (considering the food and service separately).  You should have a model for each restaurant and each aspect being reviewed (food and serivce). For restaurant $k$, you will have a model for $\\{\\theta_{jk}^{\\text{food}}\\}$ and one for $\\{\\theta_{jk}^{\\text{service}}\\}$, where $\\theta_{jk}$ is the positivity of the opinion of the $j$-th reviewer regarding the $k$-th restaurant. \n",
    "\n",
    "**Hint:** What quantity in our data naturally corresponds to $\\bar{y}_j$'s in the prep school example? How would you calculate the parameter $\\sigma_j^2$ in the distribution of $\\bar{y}_j$ (note that, contrary to the school example, $\\sigma_j^2$ is not provided explictly in the restaurant data)?\n",
    "\n",
    "3.2. Just to test your that modeling makes sense choose 1 restaurant and run your model from 3.1 on the food and service aspects for that restaurant.  Create 10K samples  each for the food and service model for your chosen restuarant and visualize your samples via a traceplot for each aspect of the restaurant reviews.\n",
    "\n",
    "3.3. Use your model from 3.1 to produce estimates for $\\theta_{jk}$'s for multiple restaurants. Pick a few (try for 5 but if computer power is a problem, choose 2) restaurants and for each aspect (\"food\" and \"service\") of each restaurant, plot your estimates for the $\\theta$'s against the values in the \"mean\" column (corresponding to this restaurant). \n",
    "\n",
    "For the chosen restaurants, for each aspect (\"food\" and \"service\"), generate shrinkage plots and probability shrinkage plots as follows:\n",
    "\n",
    "**Shrinkage plot for a restaurant, topic**:\n",
    "\n",
    "> The aim for this plot is to see the shrinkage from sample means (error bars generated from standard error) to $\\theta_{jk}$'s (error bars generated from theta variance).  \n",
    ">\n",
    "> The sample means of reviews are plotted at $y=0$ and the posterior means ($\\theta_{ik}$) are plotted at $y=1$. For each review connect the sample mean to the posterior mean with a line.  Show error bars on the sample mean points using standard error and on the ($\\theta_{jk}$) points using variance.\n",
    "\n",
    "**Probability Shrinkage plot for a restaurant, topic**:\n",
    "\n",
    "> The aim for this plot is to see the shrinkage from the classification probabilities from the sample means of reviews to the classification probabilities of $\\theta_{jk}$'s.  The classification probabilities are calculated from the gaussian at the given mean and variance. The sample means and standard error are fed into the gaussian to generate one set of classification probabilities.  The $\\theta_{jk}$ estimates and variances are fed into the gaussian to generate the other set of variances.\n",
    ">\n",
    "> The y values are the classification probability (calculated as 1-cdf) using the normal distribution at a given mean and variance.\n",
    ">\n",
    "> The sample means of reviews are plotted with $y$'s obtained by using the sample means as the means in the normal above, with line segments (error bars) representing the standard error. \n",
    ">\n",
    "> The posterior means ($\\theta_{jk}$) are plotted with $y$'s obtained using the posterior means (thetas) in the gaussian above, and variances on the thetas with line segments (error bars) representing the variances on the $\\theta_{jk}$'s.\n",
    ">\n",
    ">We've provided you some code to generate a shrinkage plot and a probability shrinkage plot is included in this notebook, but feel free to implement your own. The code should also help elucidate the text above.\n",
    "\n",
    "Example of a shrinkage plot:\n",
    "<img src=\"https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlo4e4ari3r4wd%2Fj9vjyzv62x149%2Fjop9gx9xfww9%2Fshrinkage.png\" width=\"400\" height=\"400\"/>\n",
    "\n",
    "Example of a probability shrinkage plot:\n",
    "<img src=\"https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlo4e4ari3r4wd%2Fj9vjyzv62x149%2Fjop9fsbbu9bs%2Fshrinkage_prob.png\" width=\"400\" height=\"400\"/>\n",
    "\n",
    "\n",
    "3.4. Based on your shrinkage plots and probability shrinkage plots in 3.3 discuss the statistical benefits of modeling each reviewer's opinion using your hierarchical model rather than approximating the reviewer opinion with the value in \"mean\".\n",
    "\n",
    "3.5. Aggregate, in a simple but reasonable way, the reviewer's opinions given a pair of overall scores for each restaurant -- one for food and one for service. Rank the restaurants by food score and then by service score. \n",
    "\n",
    "(**Hint:**  Think what an average score for each aspect would do here?)\n",
    "\n",
    "3.6. Discuss the statistical weakness of ranking by these scores.\n",
    "\n",
    "(**Hint:** What is statistically problematic about the way you aggregated the reviews of each restaurant to produce an overall food or service score? This is also the same problem with summarizing a reviewer's opinion on a restaurants service and food based on what they write.)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Use 1-cdf at 0.5 to model the probability of having positive sentiment\n",
    "# it basically tells you the area under the gaussian after 0.5 (we'll assume \n",
    "# positive sentiment based on the usual probability > 0.5 criterion)\n",
    "\n",
    "prob = lambda mu, vari: .5 * (1 - erf((0.5- mu) / np.sqrt(2 * vari)))\n",
    "\n",
    "# fix a restaurant and an aspect (food or service)\n",
    "# \"means\" is the array of values in the \"mean\" column for the restaurant and the aspect \n",
    "#         in the dataset\n",
    "# \"thetas\" is the array of values representing your estimate of the opinions of reviewers \n",
    "#          regarding this aspect of this particular restaurant\n",
    "# \"theta_vars\" is the array of values of the varaiances of the thetas\n",
    "# \"counts\" is the array of values in the \"count\" column for the restaurant and the aspect \n",
    "#.         in the dataset\n",
    "# FEEL FREE TO RE-IMPLEMENT THESE\n",
    "\n",
    "def shrinkage_plot(means, thetas, mean_vars, theta_vars, counts, ax):\n",
    "    \"\"\"\n",
    "    a plot that shows how review means (plotted at y=0) shrink to\n",
    "    review $theta$s, plotted at y=1\n",
    "    \"\"\"\n",
    "    data = zip(means, thetas, mean_vars / counts, theta_vars, counts)   \n",
    "    palette = itertools.cycle(sns.color_palette())\n",
    "    with sns.axes_style('white'):\n",
    "        for m,t, me, te, c in data: # mean, theta, mean errir, thetax error, count\n",
    "            color=next(palette)\n",
    "            # add some jitter to y values to separate them\n",
    "            noise=0.04*np.random.randn()\n",
    "            noise2=0.04*np.random.randn()\n",
    "            if me==0:\n",
    "                me = 4\n",
    "            # plot shrinkage line from mean, 0 to\n",
    "            # theta, 1. Also plot error bars\n",
    "            ax.plot([m,t],[noise,1+noise2],'o-', color=color, lw=1)\n",
    "            ax.errorbar([m,t],[noise,1+noise2], xerr=[np.sqrt(me), np.sqrt(te)], color=color,  lw=1)\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlim([0,1])\n",
    "        sns.despine(offset=-2, trim=True, left=True)\n",
    "    return plt.gca()\n",
    "\n",
    "def prob_shrinkage_plot(means, thetas, mean_vars, theta_vars, counts, ax):\n",
    "    \"\"\"\n",
    "    a plot that shows how review means (plotted at y=prob(mean)) shrink to\n",
    "    review $theta$s, plotted at y=prob(theta)\n",
    "    \"\"\"\n",
    "    data = zip(means, thetas, mean_vars / counts, theta_vars, counts)\n",
    "    palette = itertools.cycle(sns.color_palette())\n",
    "    with sns.axes_style('white'):\n",
    "        for m,t, me, te, c in data: # mean, theta, mean errir, theta error, count\n",
    "            color = next(palette)\n",
    "            # add some jitter to y values to separate them\n",
    "            noise = 0.001 * np.random.randn()\n",
    "            noise2 = 0.001 * np.random.randn()\n",
    "            if me == 0: #make mean error super large if estimated as 0 due to count=1\n",
    "                me = 4\n",
    "            p = prob(m, me)\n",
    "            peb = prob(t, te)\n",
    "            # plot shrinkage line from mean, prob-based_on-mean to\n",
    "            # theta, prob-based_on-theta. Also plot error bars\n",
    "            ax.plot([m, t],[p, peb],'o-', color=color, lw=1)\n",
    "            ax.errorbar([m, t],[p + noise, peb + noise2], xerr=[np.sqrt(me), np.sqrt(te)], color=color, lw=1)\n",
    "        ax = plt.gca()\n",
    "        ax.set_xlim([0, 1])\n",
    "        ax.set_ylim([0, 1.05])\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gratuitous Titular Reference**:  \n",
    "\n",
    "Thank you for putting up with us -- No more HWs!  No more gratuitous titular references!\n",
    "\n",
    "We'll leave with a [Steve Aoki](http://www.steveaoki.com/) and [K-Pop](https://en.wikipedia.org/wiki/K-pop) style [Mic Drop](https://www.youtube.com/watch?v=kTlv5_Bs8aw).  Take it away [BTS](https://en.wikipedia.org/wiki/BTS_(band)).  Don't [Burn the Stage](https://www.youtube.com/watch?v=uwgDg8YnU8U) on the way out!\n",
    "\n",
    "[AM207 HW Crew out!](https://www.youtube.com/watch?v=Tg0hLMop200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"answer-separator\">\n",
    "------------------------\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "nteract": {
   "version": "0.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
