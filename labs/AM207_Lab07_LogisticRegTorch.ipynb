{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated Lab 7 Code for Homework\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Fall 2018**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers in the form of a Jupyter notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "## Standard boilerplate to import torch and torch related modules\n",
    "import torch\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Parent Class\n",
    "class Regression(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.params = dict()\n",
    "    \n",
    "    def get_params(self, k):\n",
    "        return self.params.get(k, None)\n",
    "    \n",
    "    def set_params(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            self.params[k] = v\n",
    "        \n",
    "                    \n",
    "    def fit(self, X, y):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        raise NotImplementedError()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Our PyTorch implementation of Logistic Regression\n",
    "class LRPyTorch(nn.Module):\n",
    "\n",
    "    ## the constructor is where we'll define all our layers (input, hidden, and output)\n",
    "    def __init__(self):\n",
    "\n",
    "        ## this line creates an instance of our parent (or base) class which in this case\n",
    "        ## is nn.Module.\n",
    "        super().__init__()\n",
    "\n",
    "        ## in the lines below we'll create instance variables and assign them torch.nn Models\n",
    "        ## in order to create our layers.  You should ordinarily have one variable definition for each layer\n",
    "        ## in your neural network except for the output layer.  The output layer is defined by the number of\n",
    "        ## outputs in your last layer. Since we're dealing with simple Artificial Neural Networks, we should\n",
    "        ## predominantly be using nn.Linear.  \n",
    "        self.l1 = nn.Linear(784, 10)\n",
    "\n",
    " \n",
    "    # forwards takes as a parameter x -- the batch of inputs that we want to feed into our neural network model\n",
    "    # and returns the output of the model ... i.e. the results of the output layer of the model after forward\n",
    "    # propagation through our model. practically this means you should call each layer you defined in the\n",
    "    # constructor in sequence plus any activation functions on each layer.\n",
    "    def forward(self, x):\n",
    "     \n",
    "        # call all our layers on our input (in this case we only need one)\n",
    "        x = self.l1(x)\n",
    "\n",
    "        # Since we're using Cross Entropy Loss\n",
    "        # we can return our output directly\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-0dc4dfc40739>, line 417)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-0dc4dfc40739>\"\u001b[0;36m, line \u001b[0;32m417\u001b[0m\n\u001b[0;31m    self.set_params(training_losses=losses)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Artificial_Neural_Network(Regression):\n",
    "    \n",
    "    def __init__(self, input_model, reg_rate = 0.01, learning_rate=0.1, batch_size=256, epochs=30, hidden=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        ## Load MNIST Data\n",
    "        train_dataset, test_dataset, train_loader, test_loader, validation_loader, train_idx, validation_idx = self.load_data(batch_size=batch_size)\n",
    "        \n",
    "        ## Add Datasets and Data Loaders to our params\n",
    "        self.set_params(train_dataset=train_dataset, \n",
    "                        train_loader=train_loader,\n",
    "                        test_dataset=test_dataset,\n",
    "                        test_loader=test_loader,\n",
    "                        validation_loader=validation_loader,\n",
    "                        train_idx=train_idx,\n",
    "                        validation_idx=validation_idx\n",
    "                       )\n",
    "        \n",
    "        \n",
    "        ## Here we instantiate the PyTorch model that we so nicely defined previously\n",
    "        if hidden == None:\n",
    "            model = input_model()\n",
    "        else:\n",
    "            model = input_model(hidden=hidden)\n",
    "\n",
    "        ## Here we define our loss function.  We're using CrossEntropyLoss but other options include\n",
    "        ## NLLLoss (negative log likelihood loss for when the log_softmax activation is explicitly defined\n",
    "        ## on the output layer), MSELoss for OLS Regression, KLLDivLoss for KL Divergence, BCELoss\n",
    "        ## for binary cross entropy and many others\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        ## Here we define our optimizer.  In class we've been using SGD although in practice one will often\n",
    "        ## use other optimizers like Adam or RMSProp.  The primary parameter the optimizer takes is the\n",
    "        ## set of parameters in your model.  Fortunately those are easily accessible via model.paramters()\n",
    "        ## where model is the instance of the model you defined.  Other useful parameters include lr for the\n",
    "        ## learning rate and weight_decay for the rate of l2 regularization.\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "        \n",
    "        \n",
    "        ## Set the rest of our parameters -- batch_size, learning_rate, epochs, optimizer,\n",
    "        ## model and criterion\n",
    "        \n",
    "        ## Add Datasets and Data Loaders to our params\n",
    "        self.set_params(optimizer=optimizer, \n",
    "                        learning_rate=learning_rate,\n",
    "                        batch_size=batch_size,\n",
    "                        model=model,\n",
    "                        criterion=criterion,\n",
    "                        epochs=epochs)   \n",
    "        \n",
    "    def load_data(self, validation_split=10000, batch_size=256):\n",
    "        \"\"\"load the MNIST training and test sets from MNIST\"\"\"\n",
    "        \n",
    "        \n",
    "        ## We start by defining our training dataset\n",
    "        ## --root-- a string pointing to the relative path of the directory where we'll store our MNIST data\n",
    "        ## --train-- tells us whether to download the training set (True) or the test set (False)\n",
    "        ## MNIST in torchvision only has train (60K) and test (10K) datasets.  Other datasets also have a validation set\n",
    "        ## --transforms-- is a torchvision.transforms object that specifies what transforms to apply to each element\n",
    "        ## in the dataset.  The required transform is transforms.ToTensor() that turns each element into a PyTorch floating\n",
    "        ## point tensor object.  You could also add others like transforms.Normalize if you wished\n",
    "        ## --download-- specifies whether to download the data from the online urls.   If set to false, then you should\n",
    "        ## provide the data locally yourself\n",
    "        train_dataset = datasets.MNIST(root='./hw3_data',\n",
    "                                    train=True,\n",
    "                                    transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                     ]),\n",
    "                                    download=True)\n",
    "\n",
    "        ## similar to the above, the main difference is that we should set train=False since we want the\n",
    "\n",
    "        ## test set data\n",
    "        test_dataset = datasets.MNIST(root='./hw3_data',\n",
    "                                   train=False,\n",
    "                                   transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                     ]),\n",
    "                                   download=True)\n",
    "\n",
    "        ## A DataLoader or (Dataset Loader) turns the specified data set into a sequence of data elements\n",
    "        ## that you can access in your loops for training or evaluating accuracy, etc.\n",
    "\n",
    "        ## First we need to further split our training dataset into training and validation sets.\n",
    "\n",
    "        # Define the indices\n",
    "        indices = list(range(len(train_dataset))) # start with all the indices in training set\n",
    "\n",
    "        # Define your batch_size\n",
    "        batch_size = batch_size\n",
    "\n",
    "        # Random, non-contiguous split\n",
    "        validation_idx = np.random.choice(indices, size=validation_split, replace=False)\n",
    "        train_idx = list(set(indices) - set(validation_idx))\n",
    "\n",
    "        # define our samplers -- we use a SubsetRandomSampler because it will return\n",
    "        # a random subset of the split defined by the given indices without replacement\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        validation_sampler = SubsetRandomSampler(validation_idx)\n",
    "\n",
    "        # Create the train_loader -- use your real batch_size which you\n",
    "        # I hope have defined somewhere above\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                        batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "        # You can use your above batch_size or just set it to 1 here.  Your validation\n",
    "        # operations shouldn't be computationally intensive or require batching.\n",
    "        validation_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                        batch_size=validation_split, sampler=validation_sampler)\n",
    "\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                                  batch_size=len(test_dataset),\n",
    "                                                  shuffle=False)\n",
    "\n",
    "        return (train_dataset, test_dataset, \n",
    "                train_loader, test_loader, \n",
    "                validation_loader,\n",
    "                train_idx, validation_idx\n",
    "               )\n",
    "    \n",
    "    def sample_training_images(self):\n",
    "        \"\"\"Create a set of sample images from the MNIST training images\"\"\"\n",
    "        \n",
    "        training_set = self.get_params('train_dataset')\n",
    "        train_idx = self.get_params('train_idx')\n",
    "        sample_indices = np.random.choice(train_idx, 10)\n",
    "        \n",
    "        sample_images = training_set.train_data[sample_indices,:,:].numpy()\n",
    "        sample_labels = [training_set.train_labels[x] for x in sample_indices]\n",
    "        \n",
    "        self.set_params(sample_training_images=sample_images)\n",
    "        self.set_params(sample_training_labels=sample_labels)\n",
    "        \n",
    "    def save_misclassified(self, predictions, images, labels):\n",
    "        \"\"\"Create and save a set of sample images misclassified images by the model\"\"\"\n",
    "             \n",
    "        sample_indices = np.random.choice(range(len(predictions)), 10)\n",
    "        \n",
    "        sample_images = [images[x].reshape(28,28) for x in sample_indices]\n",
    "        sample_labels = [predictions[x] for x in sample_indices]\n",
    "        true_labels = [labels[x] for x in sample_indices]\n",
    "\n",
    "        ## save the random samples -- images, labels, ground_truth\n",
    "        self.set_params(misclassified_images=sample_images)\n",
    "        self.set_params(misclassified_labels=sample_labels)\n",
    "        self.set_params(misclassified_true_labels=true_labels)\n",
    "        \n",
    "        ## save all the misclassified predictions and labels\n",
    "        self.set_params(all_missed_labels=predictions)\n",
    "        self.set_params(all_missed_true_labels=labels)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def viz_training_images(self):\n",
    "        \"\"\"Visualize/Plot sample training images\"\"\"\n",
    "        \n",
    "        if not self.get_params('training_labels'):\n",
    "            self.sample_training_images()\n",
    "        \n",
    "        # get the images and labels\n",
    "        sample_images = self.get_params(\"sample_training_images\")\n",
    "        sample_labels = self.get_params(\"sample_training_labels\")\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 5, figsize=(20, 10))\n",
    "        plt.suptitle(\"Some Sample Images from MNIST\", fontsize=20, weight='heavy')\n",
    "\n",
    "        for i in range(5):\n",
    "            ax1[i].imshow(sample_images[i])\n",
    "            ax1[i].set_title(\"MNIST Label: {}\".format(sample_labels[i]))\n",
    "            ax2[i].imshow(sample_images[i+5])\n",
    "            ax2[i].set_title(\"MNIST Label: {}\".format(sample_labels[i+5]), weight='bold')\n",
    "            \n",
    "        plt.show()\n",
    "\n",
    "    def viz_misclassified_images(self):\n",
    "        \"\"\"Visualize/Plot misclassified training images\"\"\"\n",
    "\n",
    "        # get the images and labels\n",
    "        sample_images = self.get_params(\"misclassified_images\")\n",
    "        sample_labels = self.get_params(\"misclassified_labels\")\n",
    "        true_labels = self.get_params(\"misclassified_true_labels\")\n",
    "\n",
    "        if not sample_labels:\n",
    "            raise(Exception(\"Please run predict() or score() with save_misclassified=True\"))\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 5, figsize=(20, 10))\n",
    "        plt.suptitle(\"Some Sample Misclassified Images\", fontsize=20, weight='heavy')\n",
    "\n",
    "        for i in range(5):\n",
    "            ax1[i].imshow(sample_images[i])\n",
    "            ax1[i].set_title(\"MNIST Label: {} Classified: {}\".format(true_labels[i], sample_labels[i]), weight='bold')\n",
    "            ax2[i].imshow(sample_images[i+5])\n",
    "            ax2[i].set_title(\"MNIST Label: {} Classified: {}\".format(true_labels[i+5], sample_labels[i+5]), weight='bold')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    ## Stolen from excellent visualization from submission from Madeleine Duran/Sarah Walker\n",
    "    def viz_training_loss(self, epochs=30):\n",
    "        \"\"\"Visualize/Plot our training loss\"\"\"\n",
    "        \n",
    "        losses = self.get_params(\"training_losses\")\n",
    "        \n",
    "        if type(losses) == type(None):\n",
    "            raise(\"Please run fit() to train data\")\n",
    "        \n",
    "        fig, axes = plt.subplots(nrows=1, ncols=epochs, figsize=(20,5), sharex=True, sharey=True)\n",
    "        plt.suptitle(\"Loss Trajectory for MNIST LR Model\", fontsize=20, weight='heavy')\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            axes[i].plot(range(len(losses[i])), losses[i])\n",
    "            axes[i].set_title(\"epoch {}\".format(i))\n",
    "            if i % 2 == 1:\n",
    "                axes[i].axvspan(0, len(losses[i]), facecolor='gray', alpha=0.2)\n",
    "        plt.subplots_adjust(wspace=0)\n",
    "        plt.show()\n",
    "        \n",
    "    def get_loader(self, dataset):\n",
    "        \"\"\"Retrieve dataloader, images, labels based upon dataset name\"\"\"\n",
    "        \n",
    "        if dataset == 'Test':\n",
    "            loader = self.get_params('test_loader')\n",
    "        elif dataset == 'Validation':\n",
    "            loader = self.get_params('validation_loader')\n",
    "        else:\n",
    "            loader = self.get_params('train_loader')\n",
    "            \n",
    "        # Get Loader\n",
    "        return loader\n",
    "    \n",
    "    def predict(self, dataset='Test', save_misclassified=True):\n",
    "        \"\"\"Classify images based on the fitted logistic regression model\"\"\"\n",
    "\n",
    "        loader = self.get_loader(dataset)\n",
    "        \n",
    "        predictions = []\n",
    "        all_labels = []\n",
    "        misclassified = []\n",
    "        misclassified_images = []\n",
    "        misclassified_labels = []\n",
    "        misclassified_preds = np.array([])\n",
    "        correct = 0\n",
    "        model = self.get_params('model')\n",
    "\n",
    "        for inputs, labels in loader:\n",
    "\n",
    "            ## get the inputs from the dataloader and turn into a variable for \n",
    "            ## feeding to the model\n",
    "            inputs = Variable(inputs)\n",
    "\n",
    "            ## Reshape so that batches work properly\n",
    "            inputs = inputs.view(-1, 28*28)\n",
    "\n",
    "            # run our model on the inputs\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # get the class of the max log-probability\n",
    "            pred = outputs.data.max(1)[1]\n",
    "            \n",
    "            # get the correct predictions\n",
    "            correct += (pred == labels).sum()\n",
    "\n",
    "            # save current batch of predictions\n",
    "            predictions += list(pred)\n",
    "            \n",
    "            # save all labels\n",
    "            all_labels += list(labels)\n",
    "            \n",
    "            if save_misclassified:\n",
    "                \n",
    "                # keep track of the misclassified labels, images, and prediction\n",
    "                missed = (pred != labels)\n",
    "                missed_labels = labels[missed]\n",
    "                images = inputs.data.numpy()\n",
    "                missed_images = [images[index] for index,value in enumerate(missed) if value==True]\n",
    "\n",
    "                misclassified_labels = np.append(misclassified_labels, missed_labels)\n",
    "                misclassified_images += missed_images\n",
    "                misclassified_preds = np.append(misclassified_preds, pred[missed])\n",
    "                \n",
    "                            \n",
    "            \n",
    "        self.set_params(predictions=predictions, \n",
    "                        correct_predictions=correct,\n",
    "                        prediction_dataset_length=len(predictions),\n",
    "                        all_labels=all_labels\n",
    "                       )\n",
    "        \n",
    "        # Save misclassified images/predictions/labels for visualizing later\n",
    "        if save_misclassified:\n",
    "            self.save_misclassified(misclassified_preds, misclassified_images, misclassified_labels,)\n",
    "            \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    \n",
    "    def score(self, dataset='Test', print_score=True, save_misclassified=True ):\n",
    "        \"\"\"Calculate accuracy score based upon model classification\"\"\"\n",
    "        \n",
    "        self.predict(dataset=dataset, save_misclassified=save_misclassified)\n",
    "        correct = self.get_params('correct_predictions')\n",
    "        total = self.get_params('prediction_dataset_length')\n",
    "        \n",
    "        if print_score:\n",
    "            print('Dataset: {} \\nAccuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "                dataset, correct, total, 100.0 * correct / total))\n",
    "        \n",
    "        return(correct/total)\n",
    "    \n",
    "    \n",
    "    def missed_number(self, number):\n",
    "        \"\"\"Calculate accuracy for a certain true label\"\"\"\n",
    "        \n",
    "        labels = np.array(self.get_params(\"all_labels\"))\n",
    "        predictions = np.array(self.get_params(\"predictions\"))\n",
    "        missed = np.array(self.get_params(\"all_missed_true_labels\"))\n",
    "        \n",
    "        total_number = len(labels[labels == number])\n",
    "        missed_number = len(missed[missed == number])\n",
    "        \n",
    "        return(1-missed_number/total_number)\n",
    "    \n",
    "    \n",
    "    def generate_missed_percentages(self):    \n",
    "        for i in range(10):\n",
    "            print(\"Label: {} -- Accuracy: {}\".format(i, 100*self.missed_number(i)))\n",
    "               \n",
    "        \n",
    "    def fit(self, do_validation=True, show_validation=True):\n",
    "        \"\"\"Fit our logistic regression model on MNIST training set\"\"\"\n",
    "        \n",
    "        ## We defined a number of variables in our constructor -- let's reclaim them here\n",
    "        optimizer=self.get_params(\"optimizer\")\n",
    "        model=self.get_params(\"model\")\n",
    "        epochs=self.get_params(\"epochs\")\n",
    "        criterion=self.get_params(\"criterion\")\n",
    "        train_loader=self.get_params(\"train_loader\")\n",
    "        \n",
    "        ## Get the Total size of training set\n",
    "        self.get_params('train_dataset')\n",
    "        training_size = len(self.get_params('train_idx'))\n",
    "        \n",
    "        iterations = int(np.ceil(training_size/self.get_params(\"batch_size\")))\n",
    "        \n",
    "        ## We need something to keep track of our losses\n",
    "        losses = np.zeros((epochs, iterations))\n",
    "        \n",
    "        ## We need something ot keep track of our validation scores\n",
    "        validation_scores = np.zeros(epochs)\n",
    "  \n",
    "        \n",
    "        ## Our training loop.  We can loop over a fixed number of epochs or\n",
    "        ## using a sensitivity parameter (i.e. until net change in loss is\n",
    "        ## below a certain tolerance).  Here we iterate over a fixed number of\n",
    "        ## epochs\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            ## We defined our train_loader DataLoader earlier.  The train_loader is a\n",
    "            ## sequence of tuples with the first element of each tuple being\n",
    "            ## the batched training inputs (the batch_size being defined in your DataLoader)\n",
    "            ## and the second second element of each tuple being the corresponding labels\n",
    "            ## more or less all the pytorch classes are built to handle batching transparently\n",
    "\n",
    "            ## loop through the DataLoader.  Each loop is one iteration.  All the loops\n",
    "            ## form one epoch\n",
    "            for batch_index, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "                # Convert the inputs/labels passed from the DataLoader into\n",
    "                # autograd Variables.  The dataloader provides them as PyTorch Tensors\n",
    "                # per the transforms.ToTensor() operation.\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                ## as mentioned above we receive the inputs as tensors of size (batch_size,1, 28, 28)\n",
    "                ## which is effectively (batch_size, 28, 28) basically as a 3 dimensional tensor\n",
    "                ## representing a stack of (28x28) matrices with each matrix element a floating point number\n",
    "                ## representing the value of that pixel in the image.  Unfortunately our Neural Network model\n",
    "                ## can't handle that representation and needs a pixel matrices to be flattened into a row vector\n",
    "                ## of inputs.  The model takes a 2d tensor representing batch of such row vectors each row vector\n",
    "                ## representing one set of inputs corresponding to one image.  In order to accomplish this\n",
    "                ## flattening we use the .view method defined on autograd Variables.\n",
    "                inputs = inputs.view(-1, 28*28)\n",
    "\n",
    "                # we need to zero out our gradients after each pass\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "\n",
    "                ## This is the optimize - forward step - backwards step part of our design pattern\n",
    "\n",
    "                # this is the forward step --> we calculate the new outputs based upon the input data from\n",
    "                # this batch and store the outputs in a variable\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # we compare the outputs to the ground truth labels in the batch to calculate the loss for this step\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                ## count the loss\n",
    "                losses[epoch,batch_index] = loss.data[0]\n",
    "\n",
    "                # we run backpropagation on the loss variable which repopulates the gradients all the way\n",
    "                # back through our model to the input layer\n",
    "                loss.backward()\n",
    "\n",
    "                # Use the gradients calculated in the backprop that took place in .backwards() to do a new\n",
    "                # gradient descent step\n",
    "                optimizer.step()\n",
    "            \n",
    "            ## After each epoch -- we should test validation accuracy\n",
    "            if do_validation:\n",
    "                \n",
    "                ### Do your validation scoring here!!!!\\\n",
    "                pass\n",
    "            \n",
    "                \n",
    "        ## Set Loss Matrix for visualizing\n",
    "        self.set_params(training_losses=losses)\n",
    "        self.set_params(validation_scores=validation_scores)\n",
    "        \n",
    "        return self\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
