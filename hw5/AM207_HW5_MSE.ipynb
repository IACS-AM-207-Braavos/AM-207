{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "##### Data: chall.txt\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Fall 2018**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date: ** Saturday, October 13th, 2018 at 11:59pm\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers in the form of a Jupyter notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborators\n",
    "\n",
    "\n",
    "**Joe Davison**\n",
    "<br>\n",
    "**Anna Davydova**\n",
    "<br>\n",
    "**Michael S. Emanuel**\n",
    "<br>\n",
    "**Dylan Randle**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"answer-separator\">\n",
    "------------------------\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports\n",
    "import os\n",
    "from numpy import sqrt, exp, pi\n",
    "from scipy.integrate import cumtrapz\n",
    "from scipy.interpolate import PchipInterpolator\n",
    "from scipy.stats import norm\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.discrete.discrete_model import Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def arange_inc(x: float, y: float = None, z: float = None) -> np.ndarray:\n",
    "    \"\"\"Return a numpy arange inclusive of the end point, i.e. range(start, stop + 1, step)\"\"\"\n",
    "    if y is None:\n",
    "        (start, stop, step) = (1, x + 1, 1)\n",
    "    elif z is None:\n",
    "        (start, stop, step) = (x, y + 1, 1)\n",
    "    elif z > 0:\n",
    "        (start, stop, step) = (x, y + 1, z)\n",
    "    elif z < 0:\n",
    "        (start, stop, step) = (x, y - 1, z)\n",
    "    return np.arange(start, stop, step)\n",
    "\n",
    "# Set default font size to 20\n",
    "matplotlib.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: We'll Always Have that Night Sampling in Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding required**\n",
    "\n",
    "\n",
    "Let $X$ be a random variable with distribution described by the following pdf:\n",
    "\n",
    "$$\n",
    "f_X(x) = \\begin{cases}\n",
    "\\frac{1}{12}(x-1), &1\\leq x\\leq 3\\\\\n",
    "-\\frac{1}{12}(x-5), &3< x\\leq 5\\\\\n",
    "\\frac{1}{6}(x-5), &5< x\\leq 7\\\\\n",
    "-\\frac{1}{6}(x-9), &7< x\\leq 9\\\\\n",
    "0, &otherwise\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Let $h$ be the following function of $X$:\n",
    "\n",
    "$$h(X) = \\frac{1}{3\\sqrt{2}\\pi}\\mathrm{exp}\\left\\{ -\\frac{1}{18}\\left( X - 5\\right)^2\\right\\}$$\n",
    "\n",
    "\n",
    "Compute $\\mathbb{E}[h(X)]$ via Monte Carlo simulation using the following sampling methods:\n",
    "\n",
    "**1.1.** Inverse Transform Sampling\n",
    "\n",
    "**1.2.** Rejection Sampling with a uniform proposal distribution (rejection sampling in a rectangular box with uniform probability of sampling any x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute  𝔼[h(X)]E[h(X)]  via Monte Carlo simulation using the following sampling methods:\n",
    "# (1.1) Inverse Transform Sampling\n",
    "# (1.2) Rejection Sampling with a uniform proposal distribution\n",
    "\n",
    "# 1. Shared prerequisites for problem 1 (both parts)\n",
    "def f_X(x: float) -> float:\n",
    "    \"\"\"The PDF for problem 1\"\"\"\n",
    "    if x < 1:\n",
    "        return 0\n",
    "    elif x <= 3:\n",
    "        return (1/12) * (x - 1)\n",
    "    elif x <= 5:\n",
    "        return (1/12) * (5 -x)\n",
    "    elif x <= 7:\n",
    "        return (1/6) * (x-5)\n",
    "    elif x <= 9:\n",
    "        return (1/6) * (9-x)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def h(x: float) -> float:\n",
    "    \"\"\"The function h(X) whose expectation we want to take\"\"\"\n",
    "    # The normalizing constant\n",
    "    a: float = 1.0 / (3 * sqrt(2) * pi)\n",
    "    # The term in the exponential\n",
    "    u: float = -(1/18)*(x-5)*(x-5)\n",
    "    return a * exp(u)\n",
    "\n",
    "\n",
    "def expectation_mc(h, x_sample):\n",
    "    \"\"\"Take the expectation of a function given samples\n",
    "    h:          The function whose expectation we want to take\n",
    "    x_sample:   The samples of the function\n",
    "    \"\"\"\n",
    "    # Evaluate h(x) on the samples and return the mean\n",
    "    return np.mean(h(x_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1. Inverse Transform Sampling\n",
    "def cdf_and_inverse(f, a, b, dx):\n",
    "    \"\"\"Generate a numerical inverse CDF to the PDF given by f(x)\n",
    "    f:  The probability density function whose CDF is to be numerically inverted\n",
    "    a:  The start of the support for f(x)\n",
    "    b:  The end of the support for f(x)\n",
    "    dx: The step size to use in sampling on [a, b]    \n",
    "    \"\"\"\n",
    "    # Sample f_X(x) on the interval [a, b] with step size dx\n",
    "    dx = 0.01\n",
    "    sample_x = arange_inc(a, b, dx) \n",
    "    sample_f = np.array([f(x) for x in sample_x])\n",
    "    \n",
    "    # Numerical integral of F using cumtrapz library function\n",
    "    sample_F = np.zeros_like(sample_f)\n",
    "    sample_F[1:] = cumtrapz(sample_f, sample_x)\n",
    "    # Normalize this to guarantee it ranges from [0, 1] notwithstanding any round-off\n",
    "    sample_F = sample_F / sample_F[-1]\n",
    "    \n",
    "    # Use the Pchip interpolator b/c this guarantees that monotonic input is sent to monotonic output\n",
    "    # Numerical CDF using interpolation\n",
    "    F = PchipInterpolator(sample_x, sample_F)\n",
    "    # Numerical inverse CDF using interpolation\n",
    "    # Silence these warnings; it's OK, the splined inverse interpolant is horizontal in places but it works\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        F_inv = PchipInterpolator(sample_F, sample_x)\n",
    "    # Return the splined CDF and inverse CDF function\n",
    "    return F, F_inv\n",
    "\n",
    "# Get the CDF and inverse CDF for the given f\n",
    "F_X, F_X_inv = cdf_and_inverse(f_X, 1.0, 9.0, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Visualize PDF and CDF\n",
    "\n",
    "# Values for plots\n",
    "plot_x = arange_inc(1, 9, 0.05)\n",
    "plot_f = np.array([f_X(x) for x in plot_x])\n",
    "plot_F = np.array([F_X(x) for x in plot_x])\n",
    "\n",
    "# Plot the PDF f_X(x)\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches([16, 8])\n",
    "ax.set_title('PDF $f_X(x)$')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('$f_X(x)$')\n",
    "ax.set_xlim([1,9])\n",
    "ax.plot(plot_x, plot_f, label='PDF')\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot the CDF F_X(x)\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches([16, 8])\n",
    "ax.set_title('CDF $F_X(x)$')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('$F_X(x)$')\n",
    "ax.set_xlim([1,9])\n",
    "ax.set_ylim([0,1])\n",
    "ax.plot(plot_x, plot_F, label='CDF')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Take samples and compute expectation\n",
    "def samples_x_inv_trans(F_X_inv, size):\n",
    "    \"\"\"\n",
    "    Sample random variable X using Inverse Transform Sampling\n",
    "    F_X_inv: inverse of the CDF\n",
    "    size:    size of the array to generate\n",
    "    \"\"\"\n",
    "    # Sample u uniformly on [0, 1]\n",
    "    u = np.random.uniform(size=size)\n",
    "    # Apply the inverse transform\n",
    "    return F_X_inv(u)\n",
    "\n",
    "# Generate 1,000,000 samples for x\n",
    "sample_size: int = 10**6\n",
    "x_samples_its = samples_x_inv_trans(F_X_inv, size=sample_size)\n",
    "# Compute E_f[H] on these samples\n",
    "exp_h_its: float = expectation_mc(h, x_samples_its)\n",
    "# Report the results\n",
    "print(f'Expectation of h(x) using Inverse Transform Sampling: {exp_h_its:0.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_x_reject(f_X, size, a: float, b: float, y_max: float):\n",
    "    \"\"\"Sample random variable X using Rejection Sampling with a uniform proposal distribution\n",
    "    f_X:   probability density function for f(x)\n",
    "    size:  size of the array to generate\n",
    "    a:     start of the support of f(x); left side of rectangular box\n",
    "    b:     end of the support of f(x); right side of rectangular box\n",
    "    y_max: maximum of f(x) on [a, b]; height of rectangular box\n",
    "    \"\"\"\n",
    "    # Preallocate space for the drawn samples\n",
    "    x_samples = np.zeros(size)\n",
    "    # Count the number of samples drawn and attempts\n",
    "    idx: int = 0\n",
    "    attempts: int = 0\n",
    "    # Continue drawing new samples until we've collected size of them\n",
    "    while idx < size:\n",
    "        # Draw a random value of x on [a, b]\n",
    "        x = np.random.uniform(a, b)\n",
    "        # Draw a random value of y on [0, y_max]; if y <= f_X(x), keep this sample\n",
    "        if np.random.uniform(0, y_max) <= f_X(x):\n",
    "            # Save the sample in slot idx, then increment idx\n",
    "            x_samples[idx] = x\n",
    "            idx += 1\n",
    "        # Always increment attempts\n",
    "        attempts += 1\n",
    "    # Return the list of samples as well as the number of attempts\n",
    "    return x_samples, attempts\n",
    "\n",
    "# Generate samples with rejection sampling\n",
    "# Maximum value of f(X) occurs at x=7\n",
    "y_max: float = f_X(7)\n",
    "x_samples_rs, attempts = samples_x_reject(f_X, sample_size, 1.0, 9.0, y_max)\n",
    "# Report number of trials\n",
    "print(f'Drew {sample_size} samples for x ~ f(x) in {attempts} attempts.')\n",
    "\n",
    "# Compute E_f[H] on these samples\n",
    "exp_h_rs: float = expectation_mc(h, x_samples_rs)\n",
    "# Report the results\n",
    "print(f'Expectation of h(x) using Rejection Sampling: {exp_h_rs:0.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"answer-separator\">\n",
    "------------------------\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: The Consequences of O-ring Failure can be Painful and Deadly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding required**\n",
    "\n",
    "In 1986, the space shuttle Challenger exploded during take off, killing the seven astronauts aboard. It is believed that the explosion was caused by the failure of an O-ring (a rubber ring that seals parts of the solid fuel rockets together), and that the failure was caused by the cold weather at the time of launch (31F).\n",
    "\n",
    "In the file chall.txt, you will find temperature (in Fahrenheit) and failure data from 23 shuttle launches, where 1 stands for O-ring failure and 0 no failure. We assume that the observed temperatures are fixed and that, at temperature $t$, an O-ring fails with probability $f(\\theta_{1}+\\theta_{2}t)$ conditionally on $\\Theta = (\\theta_1, \\theta_2)$.\n",
    "\n",
    "$f(\\v{z})$ is defined to be the logistic function -- $f(\\v{ z }) = 1/(1 + \\exp(\\v{ -z }))$ \n",
    "\n",
    "**2.1.** Based on your own knowledge and experience, suggest a prior distribution for the regression parameters ($\\theta_1, \\theta_2$).  Make sure to explain your choice of prior. \n",
    "\n",
    "**2.2.** Produce 5000-10000 samples from the posterior distribution of $\\Theta $ using rejection sampling, and plot them and their marginals. (This may take a while.)\n",
    "\n",
    "**2.3.** Use the logit package in the `statsmodels` library to compute 68% confidence intervals on the $\\theta$ parameters.  Compare those intervals with the 68% credible intervals from the posterior above. Overlay these on the above marginals plots. \n",
    "\n",
    "**2.4.** Use the MLE values from `statsmodels` and the posterior mean from **2.2** at each temperature to plot the probability of failure in the frequentist and bayesian settings as a function of temperature. What do you see? \n",
    "\n",
    "**2.5.** Compute the mean posterior probability for an O-ring failure at $t = 31^\\circ F$. To do this you must calculate the posterior at $31^\\circ F$ and take the mean of the samples obtained.\n",
    "\n",
    "**2.6.** You can instead obtain the probability from the posterior predictive. Use the posterior samples to obtain samples from the posterior predictive at $31^\\circ F$ and calculate the fraction of failures.\n",
    "\n",
    "**2.7.** The day before a new launch, meteorologists predict that the temperature will be $T \\sim N(68, 1)$ during take-off. Estimate the probability for an O-ring failure during this take-off. (You will calculate multiple predictives at different temperatures for this purpose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Visualize the data\n",
    "# Download the file into a DataFrame\n",
    "df = pd.read_csv('chall.txt', names=['temp', 'fail'], delim_whitespace=True, index_col=False)\n",
    "# Alias into local variables\n",
    "temps = df.temp.values\n",
    "fails = df.fail.values\n",
    "temps_pass = df[df.fail==0]['temp'].values\n",
    "temps_fail = df[df.fail==1]['temp'].values\n",
    "\n",
    "# Plot summary of data\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches([16, 8])\n",
    "ax.set_title('Shuttle Launches vs. O-Ring Failure')\n",
    "ax.set_xlabel('Temperature (F)')\n",
    "ax.set_ylabel('Failure? (1 = Failed)')\n",
    "ax.scatter(temps_pass, np.zeros_like(temps_pass), label='Pass', color='b')\n",
    "ax.scatter(temps_fail, np.ones_like(temps_fail), label='Fail', color='r')\n",
    "ax.legend(loc='lower left')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** I rember the Challenger disaster vividly because I was nine years old when it happened and they played it endlessly on CNN.\n",
    "\n",
    "I don't really know much about this phenomenon, so my inclination is to choose an uninformative prior.  A normal distribution seems like a good choice that is familiar and has high entropy.  I'm highly skeptical of making up parameter values for the prior without a good basis for them.  Instead, I went ahead and did the next part of this problem (please see below) and looked at the estimated parameter values for $\\theta_1$ and $\\theta_2$ as well as their standard errors.\n",
    "\n",
    "The ML estimate of $\\theta_1$ was 15.04, and the standard error was 7.4.  The ML estimate of $\\theta_2$ was -0.23 with a standard error of 0.11.  Therefore we set as priors:\n",
    "\n",
    "$$p(\\theta_1) \\sim N(15.04, 7.38)$$\n",
    "$$p(\\theta_2) \\sim N(-0.23, 0.108)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta_prior_generic(theta_1_mean, theta_1_std, theta_2_mean, theta_2_std):\n",
    "    \"\"\"\n",
    "    Make a generic prior functrion that wraps the means and standard deviations\n",
    "    theta_2_mean:       the mean of theta_1 on a normal distribution\n",
    "    theta_1_std:        the standard deviation of theta_1 on a normal distribution\n",
    "    theta_2_mean:       the mean of theta_2 on a normal distribution\n",
    "    theta_2_std:        the standard deviation of theta_2 on a normal distribution\n",
    "    \"\"\"\n",
    "    # Return a function of two variables\n",
    "    def prior_instance(theta_1, theta_2):\n",
    "        # Probability for theta_1\n",
    "        theta_1_prob = norm.pdf(theta_1, loc=theta_1_mean, scale=theta_1_std)\n",
    "        theta_2_prob = norm.pdf(theta_2, loc=theta_2_mean, scale=theta_2_std)\n",
    "        # Return the joint probability\n",
    "        return theta_1_prob * theta_2_prob\n",
    "    \n",
    "    # Return this function with the bound parameter values\n",
    "    return prior_instance\n",
    "\n",
    "\n",
    "def sample_theta_prior_generic(theta_1_mean, theta_1_std, theta_2_mean, theta_2_std):\n",
    "    \"\"\"\n",
    "    Draw samples for theta_1 and theta_2 on the prior distribution\n",
    "    theta_2_mean:       the mean of theta_1 on a normal distribution\n",
    "    theta_1_std:        the standard deviation of theta_1 on a normal distribution\n",
    "    theta_2_mean:       the mean of theta_2 on a normal distribution\n",
    "    theta_2_std:        the standard deviation of theta_2 on a normal distribution\n",
    "    \"\"\"\n",
    "    def sample_instance(size: int = 1):\n",
    "        # Preallocate storage; theta is an array of shape (size, 2)\n",
    "        theta = np.zeros((size, 2))\n",
    "        # Draw samples for theta_1 and theta_2\n",
    "        theta[:, 0] = np.random.normal(loc=theta_1_mean, scale=theta_1_std, size=size)\n",
    "        theta[:, 1] = np.random.normal(loc=theta_2_mean, scale=theta_2_std, size=size)\n",
    "        # Return the combined theta array\n",
    "        return theta\n",
    "\n",
    "    # Return the instance with bound parameter values\n",
    "    return sample_instance\n",
    "\n",
    "# Selected parameters for priors of theta_1 and theta_2 match the output of \n",
    "# the logistic regression estimate in statsmodels\n",
    "theta_1_mean = 15.04\n",
    "theta_1_std = 7.38\n",
    "theta_2_mean = -0.23\n",
    "theta_2_std = 0.108\n",
    "\n",
    "# Create instances of the prior and its sampling function with bound parameter values\n",
    "theta_prior = theta_prior_generic(theta_1_mean, theta_1_std, theta_2_mean, theta_2_std)\n",
    "sample_theta_prior = sample_theta_prior_generic(theta_1_mean, theta_1_std, theta_2_mean, theta_2_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2. Produce 5000-10000 samples from the posterior distribution of Θ using rejection sampling, \n",
    "# and plot them and their marginals. (This may take a while.)\n",
    "\n",
    "# Functions for likelihood, posterior, and grid box for thetas\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid (logistic) function\"\"\"\n",
    "    return 1.0 / (1.0 + exp(-z))\n",
    "\n",
    "def likelihood(temps, fails, theta_1, theta_2):\n",
    "    \"\"\"Compute the likelihood of the data given parameter values theta_1 and theta_2\"\"\"\n",
    "    # Compute the z score for each launch\n",
    "    z = theta_1 + theta_2 * temps\n",
    "    # Compute the sigmoid probabilities of failure at each launch\n",
    "    fail_prob = sigmoid(z)\n",
    "    pass_prob = 1.0 - fail_prob\n",
    "    # The likelihood is the product over every launch of the probability of the predicted events\n",
    "    pred_prob = fails * fail_prob + (1 - fails) * pass_prob\n",
    "    return np.prod(pred_prob)\n",
    "\n",
    "\n",
    "def theta_post(theta_1, theta_2):\n",
    "    \"\"\"Posterior probability for theta; uses theta_prior and likelihood defined above\"\"\"\n",
    "    return theta_prior(theta_1, theta_2) * likelihood(temps, fails, theta_1, theta_2)\n",
    "\n",
    "# Set minimum and maximum for theta_1 and theta_2 using a two standard deviation range\n",
    "# (3 SD would be better, but it's already going to take a long time...)\n",
    "theta_1_min, theta_1_max = theta_1_mean - 2*theta_1_std, theta_1_mean + 2*theta_1_std\n",
    "theta_2_min, theta_2_max = theta_2_mean - 2*theta_2_std, theta_2_mean + 2*theta_2_std\n",
    "# Package theta limits for readability\n",
    "theta_limits = (theta_1_min, theta_1_max, theta_2_min, theta_2_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Estimate L_star (largest value of posterior) by sampling the prior or grid search\n",
    "def L_star_sample(sample_theta_prior, sample_size):\n",
    "    \"\"\"Estimate L_star by sampling\"\"\"\n",
    "    # pre-allocate space for the samples\n",
    "    post_probs = np.zeros(sample_size)\n",
    "    # Sample theta from the prior\n",
    "    thetas = sample_theta_prior(sample_size)\n",
    "    # Maximum posterior seen so far and accompanying theta    \n",
    "    max_post_seen = 0.0\n",
    "    argmax_theta = None\n",
    "    # Evaluate the posterior probability on sample_size samples\n",
    "    for i, theta in enumerate(thetas):        \n",
    "        post_prob = theta_post(theta[0], theta[1])\n",
    "        post_probs[i] = post_prob\n",
    "        # Keep running track of the maximum posterior and the associated theta\n",
    "        if post_prob > max_post_seen:\n",
    "            max_post_seen = post_prob\n",
    "            argmax_theta = theta\n",
    "    # Take three summary statistics over this sample: max, mean, std\n",
    "    L_max = np.max(post_probs)\n",
    "    L_mean = np.mean(post_probs)\n",
    "    L_std = np.std(post_probs)\n",
    "    # Return summary statistics of posterior probability on this sample\n",
    "    return L_max, L_mean, L_std, argmax_theta\n",
    "\n",
    "\n",
    "def L_star_grid(theta_limits, grid_size):\n",
    "    \"\"\"Estimate L_star by building a grid\"\"\"\n",
    "    # Unpack theta_limits\n",
    "    theta_1_min, theta_1_max, theta_2_min, theta_2_max = theta_limits\n",
    "    # Create a grid of both parameters\n",
    "    theta_1_samples = np.linspace(theta_1_min, theta_1_max, num=grid_size)\n",
    "    theta_2_samples = np.linspace(theta_2_min, theta_2_max, num=grid_size)\n",
    "    # Maximum posterior seen so far and accompanying theta    \n",
    "    max_post_seen = 0.0\n",
    "    argmax_theta = None    \n",
    "    # Evaluate posterior probability on the grid\n",
    "    post_grid = np.zeros((grid_size, grid_size))\n",
    "    for i in range(grid_size):\n",
    "        theta_1 = theta_1_samples[i]\n",
    "        for j in range(grid_size):\n",
    "            theta_2 = theta_2_samples[j]\n",
    "            # Save posterior probability on post_grid\n",
    "            post_prob = theta_post(theta_1, theta_2)\n",
    "            post_grid[i,j] = post_prob\n",
    "            # Keep running track of the maximum posterior and the associated theta\n",
    "            if post_prob > max_post_seen:\n",
    "                max_post_seen = post_prob\n",
    "                argmax_theta = np.array([theta_1, theta_2])\n",
    "    # Take three summary statistics over this sample: max, mean, std\n",
    "    L_max = np.max(post_grid)\n",
    "    L_mean = np.mean(post_grid)\n",
    "    L_std = np.std(post_grid)\n",
    "    return L_max, L_mean, L_std, argmax_theta\n",
    "\n",
    " \n",
    "# *************************************************************************************************\n",
    "# Estimate the maximum posterior, L_star, and report it\n",
    "if 'L_max' not in locals():\n",
    "    sample_size: int = 10**5\n",
    "    L_max, L_mean, L_std, argmax_theta = L_star_sample(sample_theta_prior, sample_size)\n",
    "    work_per_sample = L_max / L_mean\n",
    "    L_star = L_max\n",
    "    msg = f'Using {sample_size} samples form the prior, maximum posterior L_max = {L_max:0.3e}, '\n",
    "    msg += f'L_mean = {L_mean:0.3e}, iterations per sample = {work_per_sample:0.2f}.'\n",
    "    print(msg)\n",
    "\n",
    "if 'L_max_grid' not in locals():\n",
    "    grid_size = 200\n",
    "    L_max_grid, L_mean_grid, L_std_grid, argmax_theta_grid = L_star_grid(theta_limits, grid_size)  \n",
    "    work_per_sample_grid = L_max_grid / L_mean_grid\n",
    "    msg = f'Using a grid of size {grid_size} on the prior, maximum posterior L_max = {L_max_grid:0.3e}, '\n",
    "    msg += f'L_mean = {L_mean_grid:0.3e}, iterations per sample = {work_per_sample_grid:0.3f}.'\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 perform the sampling of theta\n",
    "def sample_theta_post(theta_limits, theta_post, L_star, num_reps):\n",
    "    \"\"\"Sample theta_1 and theta_2 on the posterior by rejection sampling.  \n",
    "    Persist samples in a CSV due to long run time.\"\"\"\n",
    "    # Unpack theta_limits\n",
    "    theta_1_min, theta_1_max, theta_2_min, theta_2_max = theta_limits\n",
    "    # Create array to store parameters\n",
    "    thetas = np.zeros((num_reps, 2))    \n",
    "    # Count both successes and attempts\n",
    "    idx: int = 0\n",
    "    attempts: int = 0\n",
    "    # Status update\n",
    "    bucket_size: int = 10**4\n",
    "    buckets: int = 0\n",
    "    # Start timer\n",
    "    t0 = time.time()\n",
    "    # Continue drawing theta until we have num_reps\n",
    "    while idx < num_reps:\n",
    "        # Draw two candidates uniformly\n",
    "        theta_1 = np.random.uniform(theta_1_min, theta_1_max)\n",
    "        theta_2 = np.random.uniform(theta_2_min, theta_2_max)\n",
    "        # Compute the posterior\n",
    "        post = theta_post(theta_1, theta_2)\n",
    "        # Update L_star\n",
    "        L_star = max(L_star, post)\n",
    "        # Rejection sample\n",
    "        if np.random.uniform(low=0, high=L_star) <= post:\n",
    "            # Save this sample\n",
    "            thetas[idx] = (theta_1, theta_2)\n",
    "            idx += 1\n",
    "        # Count the attempt\n",
    "        attempts += 1\n",
    "        # Status update\n",
    "        if attempts >= (buckets+1) * bucket_size:\n",
    "            buckets = attempts // bucket_size\n",
    "            t1 = time.time()\n",
    "            # Compute time used\n",
    "            time_used = (t1 - t0)\n",
    "            # Estimate remaining time\n",
    "            time_proj = num_reps / idx * time_used if idx > 0 else np.NAN\n",
    "            print(f'Completed {buckets} buckets and {idx} samples.', end=' ')\n",
    "            print(f'Elapsed time {time_used:0.0f}, projected {time_proj:0.0f} seconds.')\n",
    "    # Return the parameters and the number of attempts\n",
    "    return (thetas, attempts)\n",
    "\n",
    "# Test whether the posterior sample of theta was saved to file on a previous run\n",
    "fname_theta_sample = 'challenger_theta_sample.csv'\n",
    "if fname_theta_sample in os.listdir():\n",
    "    # If it was saved, just load it\n",
    "    df_theta_sample = pd.read_csv(fname_theta_sample, index_col=0)\n",
    "    theta_sample = df_theta_sample.values\n",
    "    num_reps = len(df_theta_sample)\n",
    "    print(f'Loaded {num_reps} samples of theta drawn from posterior distribution.')\n",
    "else:\n",
    "    # It it wasn't save, draw samples from the posterior distribution and save them\n",
    "    num_reps = 10000\n",
    "    theta_sample, attempts = sample_theta_post(theta_limits, theta_post, L_star, num_reps)\n",
    "    print(f'Generated {num_reps} samples from posterior distribution of theta in {attempts} attempts.')\n",
    "    # Save the sampled thetas into a DataFrame\n",
    "    df_theta = pd.DataFrame(data=theta_sample, columns=['theta_1', 'theta_2'])\n",
    "    # Save it if not already present\n",
    "    df_theta.to_csv(fname_theta_sample)\n",
    "    \n",
    "# Alias samples for theta_1 and theta_2 for legibility\n",
    "theta_1_sample = theta_sample[:, 0]\n",
    "theta_2_sample = theta_sample[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sampled thetas and their marginals\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches([16, 8])\n",
    "ax.set_title(f'{num_reps} Samples from Posterior Distribution')\n",
    "ax.set_xlabel(r'$\\theta_1$')\n",
    "ax.set_ylabel(r'$\\theta_2$')\n",
    "ax.grid()\n",
    "ax.scatter(theta_1_sample, theta_2_sample, s=3, alpha=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the marginal distribution of theta_1\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches([16, 8])\n",
    "ax.set_title(r'Marginal Distribution of $\\theta_1$')\n",
    "ax.set_xlabel(r'$\\theta_1$')\n",
    "ax.hist(theta_1_sample, bins=60)\n",
    "# Overlay the 68% confidence interval (hard code these here)\n",
    "ax.axvline(x=7.705, color='r')\n",
    "ax.axvline(x=22.381, color='r')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the marginal distribution of theta_2\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches([16, 8])\n",
    "ax.set_title(r'Marginal Distribution of $\\theta_2$')\n",
    "ax.set_xlabel(r'$\\theta_2$')\n",
    "ax.hist(theta_2_sample, bins=60)\n",
    "# Overlay the 68% confidence interval (hard code these here)\n",
    "ax.axvline(x=-0.340, color='r')\n",
    "ax.axvline(x=-0.125, color='r')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3. Use the logit package in the statsmodels library to compute 68% confidence intervals on the θ parameters. \n",
    "# Compare those intervals with the 68% credible intervals from the posterior above. \n",
    "# Overlay these on the above marginals plots.\n",
    "\n",
    "# Create an array X of predictors for statsmodel including a constant\n",
    "X = sm.add_constant(df.temp)\n",
    "# Fit logit model\n",
    "logit_model = Logit(endog=df.fail, exog=X).fit()\n",
    "# Report the model summary\n",
    "logit_model.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parameter values from the logit model\n",
    "logit_thetas = logit_model.params\n",
    "# Extract the 68% confidence intervals\n",
    "conf_intervals = logit_model.conf_int(alpha=0.32)\n",
    "print(f'Parameter estiamtes from Logistic Regression Model (68% confidence intervals)')\n",
    "print(f'68% confidence interval for theta_1 = {conf_intervals.values[0, 0]:0.3f} - {conf_intervals.values[0, 1]:0.3f}')\n",
    "print(f'68% confidence interval for theta_2 = {conf_intervals.values[1, 0]:0.3f} - {conf_intervals.values[1, 1]:0.3f}')\n",
    "# Get the 16th and 84th percentiles for theta_1 and theta_2\n",
    "theta_1_cred = np.percentile(theta_sample[:, 0], [16, 84])\n",
    "theta_2_cred = np.percentile(theta_sample[:, 1], [16, 84])\n",
    "print(f'\\nParameter estimates from Posterior Sampling (68% credible intervals)')\n",
    "print(f'68% credible interval for theta_1   = {theta_1_cred[0]:0.3f} - {theta_1_cred[1]:0.3f}')\n",
    "print(f'68% credible interval for theta_2   = {theta_2_cred[1]:0.3f} - {theta_2_cred[1]:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting.  The confidence intervals for the Bayesian estimation are meaningfully tighter than they are for the Logistic Regression model.  With that said, in broad strokes the intervals have a lot of overlap and are largely consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4. Use the MLE values from statsmodels and the posterior mean from 2.2 at each temperature to plot the probability\n",
    "#  of failure in the frequentist and bayesian settings as a function of temperature. What do you see?\n",
    "\n",
    "def fail_prob(theta_1, theta_2, temp):\n",
    "    \"\"\"The probability of failure at this temperature given theta_1 and theta_2\"\"\"\n",
    "    z = theta_1 + theta_2 * temp\n",
    "    return sigmoid(z)\n",
    "\n",
    "# Vector of temperatures to test\n",
    "plot_temp = arange_inc(20, 100)\n",
    "# Alias the MLE estimates for legibility\n",
    "theta_1_mle, theta_2_mle = logit_thetas\n",
    "# Compute the posterior means for theta_1 and theta_2\n",
    "theta_1_pm = np.mean(theta_1_sample)\n",
    "theta_2_pm = np.mean(theta_2_sample)\n",
    "# Compute the failure probability vs. temperature in both models\n",
    "plot_fail_mle = fail_prob(theta_1_mle, theta_2_mle, plot_temp)\n",
    "plot_fail_pm = fail_prob(theta_1_pm, theta_2_pm, plot_temp)\n",
    "\n",
    "# Plot the two curves of failure probability vs. temperature\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches([16, 8])\n",
    "ax.set_title('Failure Probability vs. Temperature - 2 Models')\n",
    "ax.set_xlabel('Temperature, Degrees Fahrenheit')\n",
    "ax.set_ylabel('Failure Probability')\n",
    "ax.plot(plot_temp, plot_fail_mle, label='MLE Params')\n",
    "ax.plot(plot_temp, plot_fail_pm, label='Post. Mean')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we collapse the Bayesian approach down to a point estimate for the parameters (i.e. the posterior mean), the two modeling approaches produce charts that look essentially identical.  This isn't surprising.  I would expect that the Bayesian methodology might be more different if we incorporated the full distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5. Compute the mean posterior probability for an O-ring failure at t=31∘F. \n",
    "# To do this you must calculate the posterior at 31∘F and take the mean of the samples obtained.\n",
    "\n",
    "# Set the temperature\n",
    "temp_cold = 31\n",
    "# Compute the failure probability using the posterior mean parameter estimates\n",
    "fail_prob_pm = fail_prob(theta_1_pm, theta_2_pm, temp_cold)\n",
    "print(f'The posterior mean probability of failure for a cold launch at {temp_cold} Fahrenheit '\n",
    "      f'is {fail_prob_pm*100:0.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6. You can instead obtain the probability from the posterior predictive. \n",
    "# Use the posterior samples to obtain samples from the posterior predictive at 31∘F \n",
    "# and calculate the fraction of failures.\n",
    "\n",
    "# For each pair (theta_1, theta_2) compute the failure probability at the cold temperature\n",
    "# Then take the mean of these posterior predictives.\n",
    "fail_prob_pp = np.mean([fail_prob(theta_1, theta_2, temp_cold) for theta_1, theta_2 in theta_sample])\n",
    "print(f'The posterior predictive probability of failure for a cold launch at {temp_cold} Fahrenheit '\n",
    "      f'is {fail_prob_pp*100:0.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 10000 samples of the temperature\n",
    "temp_forecast_samples = np.random.normal(loc=68, scale=1,size=num_reps)\n",
    "# Compute 10000 failure probabilities, each one a draw of theta from the posterior against a temperature draw\n",
    "fail_probs_fc = np.zeros(num_reps)\n",
    "for i in range(num_reps):\n",
    "    # Get paired theta values and temperature in this simulation\n",
    "    theta_1, theta_2 = theta_sample[i]\n",
    "    temp = temp_forecast_samples[i]\n",
    "    fail = fail_prob(theta_1, theta_2, temp)\n",
    "    fail_probs_fc[i] = fail\n",
    "# The estimated probability is the mean of these sampled probabilities\n",
    "fail_prob_fc = np.mean(fail_probs_fc)\n",
    "# Report results\n",
    "print(f'Estimated mean failure probability for a launch with temperature forecast to be '\\\n",
    "      f'normal with mean=68, std dev = 1 is {fail_prob_fc*100:0.1f}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"answer-separator\">\n",
    "------------------------\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Maximum Uniformity -- Frequentist Bootstraps and the Bayesian Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding required**\n",
    "\n",
    "Recall in HW 3 Question 1 we attempted to explore an edge case in using non-parametric bootstrap to construct confidence intervals.  Let's revisit the setup of that problem.\n",
    "Suppose you have $\\{X_1, X_2, ... X_n\\}$ datapoints such that $X_i$ are independently and identically drawn from a $Unif(0, \\theta)$.  Consider the extreme order statistic Y = $X_{(n)}$ = max($X_1, X_2, ... X_n$).\n",
    "\n",
    "**3.1.** Derive (or possibly re-write from HW3) expressions for $F_Y(y\\ \\vert\\ n, \\theta)$ the CDF of Y and $f_Y(y\\ |\\ n, \\theta)$ the pdf of Y.\n",
    "\n",
    "**3.2.** In HW3 we had difficulty constructing confidence intervals to estimate $\\theta$ using our normal percentiles methods so instead we introduced pivot confidence intervals.  Let's reframe the problem so that we can use percentiles to construct our confidence intervals.  Define $Z \\equiv n \\cdot (\\theta - Y)$ and use elementary calculation to write an expression for $F_Z(z\\ \\vert\\ n, \\theta)$ the CDF of $Z$ and $f_Z(z\\ \\vert\\ n, \\theta)$ the pdf of Z.\n",
    "\n",
    "**3.3.** What is the limiting distribution of Z (as $n \\rightarrow \\infty$)?  Plot that limiting distribution.\n",
    "\n",
    "**3.4.** Use scipy/numpy to generate 100000 samples {$X_i$} from Unif(0,100) (i.e. let $\\theta$ = 100).  Store them in Based on your data sample, what's $\\hat{\\theta}$ the empirical estimate for $\\theta$.\n",
    "\n",
    "**3.5.** Use non-parametric bootstrap to generate a sampling distribution of 10000 estimates for $Z$ by substituting $\\hat{\\theta}$ for $\\theta$.  Plot a histogram of your sampling distribution.  Make sure to title and label the plot.  Use percentiles to construct the 10% and 68% bootstrap confidence intervals.  Plot them in your graph.\n",
    "\n",
    "**Hint:  Should the confidence intervals be symmetric around the estimate $\\hat{\\theta}$**?\n",
    "\n",
    "**3.6.** Make an argument that we can construct a bootstrap confidence interval that always mismatches the limiting distribution.\n",
    "\n",
    "**3.7.** Let's switch to being Bayesian.  In 3.1 we came up with an expression for the likelihood $f_Y(y\\ |\\ n, \\theta)$.  Use the [Pareto distribution](https://en.wikipedia.org/wiki/Pareto_distribution) to construct a prior for $\\theta$.  What are some reasonable values to use for the scale and shape?\n",
    "\n",
    "**3.8.** Write down an expression for the posterior distribution $f_Y(\\theta\\ |\\ n, y)$ \n",
    "\n",
    "**3.9.** Draw 10000 posterior samples and plot a histogram of the posterior distribution.  Use percentiles to construct the 68% HPD.  Plot the posterior distribution and mark the HPD on your plot.\n",
    "\n",
    "**3.10.** How does the 68% HPD compare with the confidence interval generated from bootstrapping?  Why doesn't the bayesian interval construction suffer the same concerns you noted in 3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"answer-separator\">\n",
    "------------------------\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
