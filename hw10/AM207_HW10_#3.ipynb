{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 10\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Fall 2018**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date:** Sunday, November 18th, 2018 at 11:59pm\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers in the form of a Jupyter notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborators\n",
    "\n",
    "** Place the name of everyone who's submitting this assignment here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"answer-separator\">\n",
    "------------------------\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from matplotlib import cm\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Metropoflix and Chill (What's your Net Worth)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**coding required**\n",
    "\n",
    "Suppose we ask you to memorize the order of the top five movies on IMDB. When we quiz you on the order afterwards, you may not recall the correct order, but the mistakes you make in your recall can be modeled by simple probabilistic models.\n",
    "  \n",
    "Let's say that the top five movies are:  \n",
    "1. *The Shawshank Redemption*\n",
    "2. *The Godfather*\n",
    "3. *The Godfather: Part II*\n",
    "4. *Black Panther*\n",
    "5. *Pulp Fiction*\n",
    "\n",
    "Let's represent this ordering by the vector $\\omega = (1,2,3,4,5)$. \n",
    "\n",
    "If you were to mistakenly recall the top five movies as:\n",
    "2. *The Godfather*\n",
    "3. *The Godfather: Part II*\n",
    "5. *Pulp Fiction*\n",
    "4. *Black Panther*\n",
    "1. *The Shawshank Redemption*\n",
    "\n",
    "We'd represent your answer by the vector $\\theta = (2,3,5,4,1)$.\n",
    "\n",
    "Unfortunately, your answer is wrong.  Fortunately (for our purposes) we have a way of quantifying just how wrong. Define the Hamming distance between two top five rankings, $\\theta, \\omega$, as follows:\n",
    "$$d(\\theta, \\omega) = \\sum_{i=1}^5 \\mathbb{I}_{\\theta_i\\neq \\omega_i},$$ \n",
    "where $\\mathbb{I}_{\\theta_i\\neq \\omega_i}$ is an indicator function that returns 1 if $\\theta_i\\neq \\omega_i$, and 0 otherwise.\n",
    "\n",
    "For example, the Hamming distance between your answer and the correct answer is $d(\\theta, \\omega)=4$, because you only ranked *Black Panther* correctly. \n",
    "\n",
    "Finally, let's suppose that the probability of giving a particular answer (expressed as $\\theta$) is modeled as\n",
    "$$ p(\\theta \\,|\\, \\omega, \\lambda) \\propto  e^{-\\lambda\\, d(\\theta,\\, \\omega)}$$\n",
    "where $\\lambda$ can be thought of as an inverse temperature\n",
    "\n",
    "1.1. Implement a Metropolis sampler to produce sample guesses from 500 individuals, with the $\\lambda$ values, $\\lambda=0.2, 0.5, 1.0$. What are the top five possible guesses?\n",
    "\n",
    "1.2. Compute the probability that *The Shawshank Redemption* is ranked as the top movie (ranked number 1) by the Metropolis algorithm sampler. Compare the resulting probabilities for the various $\\lambda$ values. \n",
    "\n",
    "1.3. How does $\\lambda$ affect the probability that *The Shawshank Redemption* is ranked as the top movie?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gratuitous Titular Reference**:  \n",
    "\n",
    "It's 2018 -- Even Wikipedia knows what [Netflix and Chill](https://en.wikipedia.org/wiki/Netflix_and_chill) is about. (mixtape by Grime MC Merky ACE].  \n",
    "\n",
    "[Drake's the type of dude](https://knowyourmeme.com/memes/drake-the-type-of) to not care about [netflix and chill but about that net net net worth](https://youtu.be/DRS_PpOrUZ4?t=224) \n",
    "\n",
    "Drake may wanna know if [Kiki/KB](https://www.thefader.com/2018/10/24/real-kiki-drake-in-my-feelings-interview-kyanna-barber) is feeling him, but the [NTSB](https://www.ntsb.gov)  [definitely isn't](https://www.cnn.com/2018/07/25/entertainment/ntsb-in-my-feelings/index.html)\n",
    "\n",
    "Shout out [Nawlins](https://riverbeats.life/neworleans/drake-shares-his-in-my-feelings) and [Atlanta](http://www.thefader.com/2018/06/29/drake-sampled-atlanta-scorpion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"answer-separator\">\n",
    "------------------------\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: In a Flash the Iris devient un Fleur-de-Lis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**coding required**\n",
    "\n",
    "We've done classification before, but the goal of this problem is to introduce you to the idea of classification using Bayesian inference. \n",
    "\n",
    "Consider the famous *Fisher flower Iris data set* a  multivariate data set introduced by Sir Ronald Fisher (1936) as an example of discriminant analysis. The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. Based on the combination of these four features, you will build a model to predict the species. \n",
    "\n",
    "For this problem only consider two classes: **virginica** and **not-virginica**. \n",
    "\n",
    "The iris data can be obtained [here](https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlo4e4ari3r4wd%2Fj9vjyzv62x149%2Fjoe90cwt3dla%2Firis.csv).\n",
    "\n",
    "Let $(X, Y )$ be our dataset, where $X=\\{\\vec{x}_1, \\ldots \\vec{x}_n\\}$ and $\\vec{x}_i$ is the standard feature vector corresponding to an offset 1 and the four components explained above. $Y \\in \\{0,1\\}$ are the scalar labels of a class. In other words the species labels are your $Y$ data (virginica = 0 and virginica=1), and the four features -- petal length, petal width, sepal length and sepal width -- along with the offset make up your $X$ data. \n",
    "\n",
    "The goal is to train a classifier, that will predict an unknown class label $\\hat{y}$ from a new data point $x$. \n",
    "\n",
    "Consider the following glm (logistic model) for the probability of a class:\n",
    "\n",
    "$$ p(y) = \\frac{1}{1+e^{-x^T \\beta}} $$\n",
    "\n",
    "(or $logit(p) = x^T \\beta$ in more traditional glm form)\n",
    "\n",
    "where $\\beta$ is a 5D parameter to learn. \n",
    "\n",
    "Then given $p$ at a particular data point $x$, we can use a bernoulli likelihood to get 1's and 0's. This should be enough for you to set up your model in pymc3. (Note: You might want to set up $p$ as a deterministic explicitly so that pymc3 does the work of giving you the trace).\n",
    "\n",
    "\n",
    "2.1. Use a 60-40 stratified (preserving class membership) split of the dataset into a training set and a test set. (Feel free to take advantage of scikit-learn's `train_test_split`).\n",
    "\n",
    "2.2. Choose a prior for $\\beta \\sim N(0, \\sigma^2 I) $ and write down the formula for the posterior $p(\\beta| Y,X)$. Since we dont care about regularization here, just use the mostly uninformative value $\\sigma = 10$.\n",
    "\n",
    "2.3. Find the MAP for the posterior on the training set.\n",
    "\n",
    "2.4. Implement a PyMC3 model to sample from this posterior of $\\beta$.  \n",
    "\n",
    "2.5. Generate 5000 samples of $\\beta$.  Visualize the betas and generate a traceplot and autocorrelation plots for each beta component.\n",
    "\n",
    "2.6. Based on your samples construct an estimate for the posterior mean.\n",
    "\n",
    "2.7. Select at least 2 datapoints and visualize a histogram of the posterior probabilities.  Denote the posterior mean and MAP on your plot for each datapoint\n",
    "\n",
    "\n",
    "Although having the posterior probabilities is nice, they are not enough.  We need to think about how to make predictions based on our machinery.  If we define the following:\n",
    "\n",
    " - $p_{MEAN}$: using the posterior mean betas to generate probabilities for each data point\n",
    " - $p_{MAP}$: using the posterior MAP betas to generate probabilities for each data point\n",
    " - $p_{CDF}$: using the fraction of your posterior samples have values above 0.5 for each data point\n",
    " - $p_{PP}$:  using the fraction of 1s out of the samples drawn from the posterior predictive distribution for each data point\n",
    "\n",
    "2.8. Plot the distributions of $p_{MEAN}$, $p_{CDF}$, $p_{MAP}$ and $p_{PP}$ over all the data points in the training set. How are these different?\n",
    "\n",
    "\n",
    "How do we turn these probabilities into predictions?  *There are two ways to make these predictions, given an estimate of $p(y=1\\ \\vert\\ x)$:* \n",
    "\n",
    "- Sample from the Bernoulli likelihood at the data point $x$ to decide if that particular data points classification $y(x)$ should be a 1 or a 0.\n",
    "\n",
    "- Do the intuitive \"machine-learning-decision-theoretic\" (MLDT) thing and you assign a data  point $x$ a classification 1 if $p(y=1 \\vert x) > 0.5$.\n",
    "\n",
    "2.9. Plot the posterior-predictive distribution of the misclassification rate with respect to the true class identities $y(x)$ of the data points $x$ (in other words you are plotting a histogram with the misclassification rate for the $n_{trace}$ posterior-predictive samples) on the training set.\n",
    "\n",
    "2.10. For every posterior sample, consider whether the data point ought to be classified as a 1 or 0 from the $p>0.5 \\implies y=1$ decision theoretic prespective. Using the MLDT defined above, overlay a plot of the histogram of the misclassification rate for the posterior on the corresponding plot for the posterior-predictive you constructed in 2.9.  Which case (from posterior-predictive or from-posterior) has a wider mis-classification distribution? \n",
    "\n",
    "2.11. Repeat 2.9 and 2.10 for the test set (i.e. make predictions).  Describe and interpret the widths of the resulting distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gratuitous Titular References**:  \n",
    "\n",
    "[The Iris Dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) was introduced by Ronald Fisher as part of a [famous article](https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1469-1809.1936.tb02137.x) introducing [LDA](https://en.wikipedia.org/wiki/Linear_discriminant_analysis).\n",
    "\n",
    "The three iris variants in the dataset were at the time [difficult to tell apart morphologically](https://www.jstor.org/stable/2394164?seq=1#page_scan_tab_contents)\n",
    "\n",
    "While the origin of the [Fleur-de-Lis is debated](https://www.heraldica.org/topics/fdl.htm), it is most likely an [Iris florentina](https://en.wikipedia.org/wiki/Iris_florentina) or [Iris pseudacorus](https://en.wikipedia.org/wiki/Iris_pseudacorus) but not a [lily flower](https://www.collinsdictionary.com/dictionary/english-french/lily).\n",
    "\n",
    "[Iris West](https://en.wikipedia.org/wiki/Iris_West) is a love interest of [Barry Allen](https://en.wikipedia.org/wiki/Flash_(Barry_Allen) one of the main incarnations of [The Flash](https://en.wikipedia.org/wiki/Flash_(comics).  Coming from [Central City](https://en.wikipedia.org/wiki/Central_City_(DC_Comics) she is most likely classified as **not-virginica**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"answer-separator\">\n",
    "------------------------\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 - Our Yelp Restaurant Review is in and the Fish is So Raw!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**no coding required**\n",
    "\n",
    "In this course, we've spent a lot of time learning algorithms for performing inference on complex models. We've also spent time using these models to make decisions regarding our data. But in nearly every assignment, the model for the data is specified in the problem statement. In real life, the creative and, arguably, much more difficult task is to start with a broadly defined goal and then to customize or create a model which will meet this goal in some way. \n",
    "\n",
    "\n",
    "This homework problem is atypical in that it does not involve any programming or (necessarily) difficult mathematics/statistics. The process of answering these questions *seriously* will however give you an idea of how one might create or select a model for a particular application and your answers will help you with formalizing the model if and when you're called upon to do so.\n",
    "\n",
    "***Grading:*** *We want you to make a genuine effort to mold an ambiguous and broad real-life question into a concrete data science or machine learning problem without the pressure of getting the \"right answer\". As such, we will grade your answer to this homework question on a pass/fail basis. Any reasonable answer that demonstrates actual effort will be given a full grade.*\n",
    "\n",
    "We've compiled for you a fairly representative selection of [Yelp reviews](https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlo4e4ari3r4wd%2Fj9vjyzv62x149%2Fjoe92vh7ni6e%2Fyelp_reviews.zip for a (now closed) sushi restaurant called Ino's Sushi in San Francisco. Read the reviews and form an opinion regarding the various qualities of Ino's Sushi. Answer the following:\n",
    "\n",
    "3.1. If the task is to summarize the quality of a restaurant in a simple and intuitive way, what might be problematic with simply classifying this restaurant as simply \"good\" or \"bad\"? Justify your answers with specific examples from the dataset.\n",
    "\n",
    "3.2. For Ino's Sushi, categorize the food and the service, separately, as \"good\" or \"bad\" based on all the reviews in the dataset. Be as systematic as you can when you do this.\n",
    "\n",
    "  (**Hint:** Begin by summarizing each review. For each review, summarize the reviewer's opinion on two aspects of the restaurant: food and service. That is, generate a classification (\"good\" or \"bad\") for each aspect based on what the reviewer writes.) \n",
    "  \n",
    "3.3. Identify statistical weaknesses in breaking each review down into an opinion on the food and an opinion on the service. That is, identify types of reviews that make your method of summarizing the reviewer's optinion on the quality of food and service problemmatic, if not impossible. Use examples from your dataset to support your argument. \n",
    "\n",
    "3.4. Identify all the ways in which the task in 3.2 might be difficult for a machine to accomplish. That is, break down the classification task into simple self-contained subtasks and identify how each subtask can be accomplished by a machine (i.e. which area of machine learning, e.g. topic modeling, sentiment analysis etc, addressess this type of task).\n",
    "\n",
    "3.5. Now let us think of a different problem, a regression problem in which our aim is to predict and do inference on what rating a given user of Yelp might give a particular restaurant. How might you estimate the across-user quality of a restaurant from data? And how might you estimate the across-restaurant curmudgeonlyness of a user?\n",
    "\n",
    "3.6 Additionally, consider a \"space of latent factors\" where aspects of the user's taste interact with aspects of the restaurant. An example of such a factor might be the user's propensity to get emotional after having the perfect filet-mignon. How might you combine this information with that in 3.5 to improve your prediction and inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1.** If the task is to summarize the quality of a restaurant in a simple and intuitive way, what might be problematic with simply classifying this restaurant as simply \"good\" or \"bad\"? Justify your answers with specific examples from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A review of the customer feedback for Ino, reveals a barbell/bimodal distribution of opinions (as one of the reviewers called an \"inverse Gausssian\"). About 60% the customers love this restaurant with five star ratings (mainly driven by food quality) and the other 40% hate the restaurant with one star rating (mainly due to chef's abrassive/aggressive personality and steep price point).  I would be hard-pressed to find another restaurant where there is such a dichotomy of opinions among its customers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this backdrop, classifying this restaurant as 'good' vs. 'bad' based solely on the number of Yelp stars would be a mistake, since the ratings are driven by different factors and are not necessarily comparable. What makes a restaurant 'good' or 'bad' is very subjective and dependent on several factors such as: 1) service, 2) food quality, 3) decor, 4) pricing, 5) location, and more often than not 6) your friends' opinion and 7) the mood you are in when you enter the restaurant (i.e. if you are grumpy to begin with the review will skew to the downside). \n",
    "<br>\n",
    "<br>\n",
    "For Ino, I also noticed that those customers who ordered omakase had a better experience than those who ordered inidividual dishes.  In this particular case it appears that there are two primary and often conflicting factors being considered by the reviewers that determine whether they gave a 'good' or a 'bad' review.  Specifically, the two competing/conflicting review metrics are food quality and service. Price also plays a role but to a lesser extent (i.e. if people like their experience they don't seem to mind the price but if they are unhappy, then price comes into play as well). It appears that all of the customers had experienced varrying degrees unfriendly service but some chose to overlook in favor of positive gastronomical experience, while other's got stuck on bad service that quite literally gave them a bad aftertaste for the food. For example, reviewer Karen L., who gave Ino 5 stars, notes that \"Service can make you feel uncomfortable...pop (chef) won't be nice to you....I am not taking any stars off for service because the sushi was amaaaazing!!!\". However, reviewer Surya G. counters with \"...the liver was tasty and succulent....tuna...was very good. Ino needs to shut down. There is no space for shouty chefs....shouting at their wives and guests.\" In a way, it appears that both reviewers had a similar experience (decent food, bad serivce) but their ratings are polar opposites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2**. For Ino's Sushi, categorize the food and the service, separately, as \"good\" or \"bad\" based on all the reviews in the dataset. Be as systematic as you can when you do this.\n",
    "\n",
    "(Hint: Begin by summarizing each review. For each review, summarize the reviewer's opinion on two aspects of the restaurant: food and service. That is, generate a classification (\"good\" or \"bad\") for each aspect based on what the reviewer writes.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to create a data frame with 2 categorial label columns ('good'/'bad') based on servicce and food quality.  I also think we might want to consider price and whether or not omakase was ordered but will go with just 2 aspects that the question requires - specifically service and food. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer</th>\n",
       "      <th>food</th>\n",
       "      <th>service</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Surya G</td>\n",
       "      <td>good</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Karen L.</td>\n",
       "      <td>good</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tony L.</td>\n",
       "      <td></td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kristen B.</td>\n",
       "      <td>good</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sylvia L.</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Youna K.</td>\n",
       "      <td>good</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alison C.</td>\n",
       "      <td>good</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Michael L.</td>\n",
       "      <td>good</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ling C.</td>\n",
       "      <td></td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Maile N.</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     customer  food service\n",
       "0     Surya G  good     bad\n",
       "1    Karen L.  good     bad\n",
       "2     Tony L.           bad\n",
       "3  Kristen B.  good     bad\n",
       "4   Sylvia L.  good    good\n",
       "5    Youna K.  good     bad\n",
       "6   Alison C.  good        \n",
       "7  Michael L.  good        \n",
       "8     Ling C.           bad\n",
       "9    Maile N.  good    good"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings=pd.DataFrame()\n",
    "ratings['customer']=['Surya G','Karen L.','Tony L.','Kristen B.','Sylvia L.','Youna K.','Alison C.','Michael L.','Ling C.','Maile N.']\n",
    "ratings['food']=['good','good','','good','good','good','good','good','','good']\n",
    "ratings['service']=['bad','bad','bad','bad','good', 'bad','','','bad','good']\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.** Identify statistical weaknesses in breaking each review down into an opinion on the food and an opinion on the service. That is, identify types of reviews that make your method of summarizing the reviewer's optinion on the quality of food and service problemmatic, if not impossible. Use examples from your dataset to support your argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right off the bat, we notice a lot of missing data in the table above that aims to summarize reviews on food and service quality. This is due to the reviewer either not mentioning one of the categories in their review or not even getting to sit down to eat (being turned off or kicked out by bossy owners).  We saw that with Ling C., whose review claims she was kicked out before she even got to sit down and eat (although it sounds like they simply did not have any openings that night).  Likewise, the review from Tony L does not have any information about their take on food quality because they never sat down to eat, being turned off by the price point and they way they were treated at the door. On the other hand, we have several reviewers that loved the food but do not mention serivce. For example, Alisson C. and Michael L. praise the food quality but say nothing about about their interaction with Ino's staff.  The big issue with our missing data from the frequentist perspective is that we cannot simply impute those values. We have so few data points that making an imputaiton (mean or model), could skew our model significantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4.** Identify all the ways in which the task in 3.2 might be difficult for a machine to accomplish. That is, break down the classification task into simple self-contained subtasks and identify how each subtask can be accomplished by a machine (i.e. which area of machine learning, e.g. topic modeling, sentiment analysis etc, addressess this type of task)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we would apply webscraping (specifically BeautifulSoup package) to extract Yelp review data.  I would then deploy Latent Dirichlet Allocation (Natural Language Processing algorithm) to my data to indentify topics within each review (i.e. topic modeling). Specifically, we could use the Gensim package to do this. The Latent Dirichlet Allocation (LDA) approach would likely do a decent job picking out food and price topics. However, it might struggle to nail down service as one single topic since the way that the customers refer to service is differs widely from each review (i.e. some like or hate the chef's personality, some complain about his wife, some complain about not being able to get a table, others just hint that service is slow by saying you won't get your water refille like Youna K.). Thus, while LDA is helpful in the general sorting of the words we have in our data into topic subgroups, we would expect to have to manually narrow down the topics into those that make sense (i.e food and service). If we were unable to deploy this tool and were looking for a more simple approach, we could have the machine simply sort sentences based on mentions of food related subjects ('food','sushi', 'omakase', 'rolls' etc.) vs. mentions of service ('service', 'hostess', 'chef','seating', 'waiter', etc.).  When it comes to sentiment analysis, I would feed the separated topic subgroups (i.e. food and service) and screen for positive words like ('great', 'ammmaaaaazing', 'delicious', 'friendly', etc.) vs. negative words( 'horrible', 'scold', 'wait','impatient', 'dirty' etc.) and calculate the score of positive words within each topic subgroup vs. negative. We can deploy VaderSentiment pacakge in python to do this for us. Again, I expect the machine to struggle with the service subgroup when calculating setiment since there are many ways in which the customers expressed their displeasure with the restuarant staff( e.g. being laughed at like Kristen B., getting a dirty look like Ling C., being haggled about the price like Tony L.) while some like Alison C. didn't say anything about the service in their review.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5**. Now let us think of a different problem, a regression problem in which our aim is to predict and do inference on what rating a given user of Yelp might give a particular restaurant. How might you estimate the across-user quality of a restaurant from data? And how might you estimate the across-restaurant curmudgeonlyness of a user?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will apply what we learned from the Gelman school's theory in class to model a given user's rating of a given restaurant. We will follow the discussion from the lab on this topic http://am207.info/wiki/gelmanschoolstheory.html. We will build a model for each restaurant x and each user's sentiment y (let's chose positive) on food and service. Thus we will have $\\theta_{xy}^{food}$ and $\\theta_{xy}^{service}$ for each user y and each restaurant x. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we focus on just one restaurant (say Ino for example) and just one criteria (e.g. service) our problem starts to resemble Gelman Schools example we worked on in class, where we have 10 reviews of service (quantified by positivity from sentiment analysis in 3.4) for a given restaurant (say Ino for example). In this case the mean sentiment from sentences about service from each user will represent their sentiment. We can also calculate mean stentiment and its variance for a given aspect of each restaurant. What that means is that we can now look at each restaurant and calculate mean sentiment for each aspect (food or service) and its variance but pooling the sentiment from each user's review. Our treament for a given restaurant is then the mean sentiment for each aspect (food and service). Let's say we now want to estimate across-user quality of a given restaurant from the data. Let's call it restaurant x. Then we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\bar{rating}_{xy}^{food} \\vert \\theta_{x}^{food}\\sim N(\\theta_{x}^{food},\\sigma_{x}^{food 2}).$$\n",
    "<br>\n",
    "<br>\n",
    "$$\\bar{rating}_{xy}^{service} \\vert \\theta_{x}^{service}\\sim N(\\theta_{x}^{service},\\sigma_{x}^{service 2}).$$\n",
    "<br>\n",
    "Where y=1,.....n is each individual Yelp user and x is the restaurant under consideration.\n",
    "<br>\n",
    "We would estimate mean by taking average of our sentiment score discussed above across all reviewers for this particular restuarant x as follows for food and service separately:\n",
    "<br>\n",
    "$$\\bar{rating_x} = \\frac{1}{n_y} \\sum_{i=1}^{n_y} rating_{xy}$$\n",
    "<br>\n",
    "\n",
    "We can estimate sampling $\\sigma_x^2$ by looknig at the variance from the mean in each reviewers food and service sentiment for that particular restaurant x.\n",
    "\n",
    "<br>\n",
    "Conversly, if we are looking at an indididual user across many restaurants, our likelihood would look as follows:\n",
    "<br>\n",
    "<br>\n",
    "$$\\bar{rating}_{xy}^{food} \\vert \\theta_{y}^{food}\\sim N(\\theta_{y}^{food},\\sigma_{y}^{food 2}).$$\n",
    "<br>\n",
    "<br>\n",
    "$$\\bar{rating}_{xy}^{service} \\vert \\theta_{y}^{service}\\sim N(\\theta_{y}^{service},\\sigma_{y}^{service 2}).$$\n",
    "<br>\n",
    "Where x=1......n is each indididual restaurant with user y under consideration. \n",
    "<br>\n",
    "We would estimate mean by taking average of our sentiment score discussed above across restaurants x for that particular reviewer y  for food and service separately:\n",
    "<br>\n",
    "$$\\bar{rating_y} = \\frac{1}{n_x} \\sum_{i=1}^{n_x} rating_{xy}$$\n",
    "<br>\n",
    "We can estimate sampling $\\sigma_y^2$ by looknig at the variance from the mean in each restaurant for food and service sentiment for that particular Yelp user y.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this framework as our set up, we can then proceed with setting up hierchical sampling in the following form for each restaurant across users and separately for each user 'curmudgeonlyness' across restaurants. Keep in mind that we are working within the confines of rating range from 1 star to 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu \\sim \\mathcal{N}(2, 3)\\\\\n",
    "\\tau \\sim \\text{Half-Cauchy}(0, 3)\\\\\n",
    "\\theta_{x} \\sim \\mathcal{N}(\\mu, \\tau)\\\\\n",
    "\\bar{rating_{x}} \\sim \\mathcal{N}(\\theta_{x}, \\sigma_{x})\n",
    "$$\n",
    "\n",
    "where $\\{ rating_{x}, \\sigma_{x} \\}$ are  calculated as per our discussion above.\n",
    "\n",
    "\n",
    "$$\n",
    "\\mu \\sim \\mathcal{N}(2, 3)\\\\\n",
    "\\tau \\sim \\text{Half-Cauchy}(0, 3)\\\\\n",
    "\\theta_{y} \\sim \\mathcal{N}(\\mu, \\tau)\\\\\n",
    "\\bar{rating_{y}} \\sim \\mathcal{N}(\\theta_{y}, \\sigma_{y})\n",
    "$$\n",
    "\n",
    "where $\\{ rating_{y}, \\sigma_{y} \\}$ are caculcated as per our discussion above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.6** Additionally, consider a \"space of latent factors\" where aspects of the user's taste interact with aspects of the restaurant. An example of such a factor might be the user's propensity to get emotional after having the perfect filet-mignon. How might you combine this information with that in 3.5 to improve your prediction and inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here too, if we could deploy Latent Dirichlet Allocation tool it would be helpful in being able to pick-up latent/hidden variables in our data.  If we had to do this manually, instead of breaking our data into two obvious groups (i.e. $\\theta^{food}$ and $\\theta^{service}$) we could come up with categories that incorporate the latent variables. For example, we could have a category that combines customer's price-sensitivity with how they view food and service. For example, in Ino's case, if the customer is price sensitive AND recieves unfriendly service, they are much more likely to give the restaurant a horrible review. In general, a price sensitive customer would likely to be more demanding on the restaurant food quality and service. Conversely, a customer who is not price sensitive, would not care about being told that there is a 20 dollar minimum (i.e Ino again) and would not be turned off by small portions of sushi (something that some of Ino's customers complained about).  Thus, in our sampling above we will have  $\\theta_{xy}^{frugal*food}$ and $\\theta_{xy}^{spendthrift*food}$  and  $\\theta_{xy}^{frugal*service}$ $\\theta_{xy}^{spendthrift*service}$ for each y customer and each restaurant x. We would then repeat/expand the process above using these aspects instead of just food and service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gratuitous Titular Reference**:  \n",
    "\n",
    "[Sushi is not raw fish](http://www.todayifoundout.com/index.php/2011/12/sushi-is-not-raw-fish)\n",
    "\n",
    "![](https://i.imgflip.com/pnawi.jpg)\n",
    "\n",
    "[More Gordon Ramsey memes](https://knowyourmeme.com/memes/people/gordon-ramsay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"answer-separator\">\n",
    "------------------------\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nteract": {
   "version": "0.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
