{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El Fin (Final Exam) for AMPTH-2017/APMA E-207\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Fall 2018**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date:** Monday, December 17th, 2018 at 11:59pm\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers in the form of a Jupyter notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborators\n",
    "\n",
    "#### Group 44 - Braavos\n",
    "\n",
    "**Joe Davison<br>\n",
    "Anna Davydova<br>\n",
    "Michael S. Emanuel<br>\n",
    "Dylan Randle<br>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy.stats import logistic\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn.apionly as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities for serializing variables\n",
    "def load_vartbl(fname: str) -> Dict:\n",
    "    \"\"\"Load a dictionary of variables from a pickled file\"\"\"\n",
    "    try:\n",
    "        with open(fname, 'rb') as fh:\n",
    "            vartbl = pickle.load(fh)\n",
    "    except:\n",
    "        vartbl = dict()\n",
    "    return vartbl\n",
    "\n",
    "\n",
    "def save_vartbl(vartbl: Dict, fname: str) -> None:\n",
    "    \"\"\"Save a dictionary of variables to the given file with pickle\"\"\"\n",
    "    with open(fname, 'wb') as fh:\n",
    "        pickle.dump(vartbl, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off deprecation warning (too noisy)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=mpl.MatplotlibDeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: GLMs with correlation\n",
    "\n",
    "### The dataset: A Bangladesh Contraception use census\n",
    "\n",
    "This problem is based on one-two (12H1 and continuations ) from your textbook. The data is in the file `bangladesh.csv`. These data are from the 1988 Bangladesh Fertility Survey. Each row is one of 1934 women. There are six variables:\n",
    "\n",
    "- (1) `district`: ID number of administrative district each woman resided in\n",
    "- (2) `use.contraception`: An indicator (0/1) of whether the woman was using contraception\n",
    "- (3) `urban`: An indicator (0/1) of whether the woman lived in a city, as opposed to living in a rural area\n",
    "- (4) `woman`: a number indexing a single woman in this survey\n",
    "- (5) `living.chidren`: the number of children living with a woman\n",
    "- (6) `age.centered`: a continuous variable representing the age of the woman with the sample mean subtracted\n",
    "\n",
    "We need to make sure that the cluster variable, district, is a contiguous set of integers, so that we can use the index to differentiate the districts easily while sampling ((look at the Chimpanzee models we did in lab to understand the indexing). So create a new contiguous integer index to represent the districts. Give it a new column in the dataframe, such as `district.id`.\n",
    "\n",
    "You will be investigating the dependence of contraception use on the district in which the survey was done. Specifically, we will want to regularize estimates from those districts where very few women were surveyed. We will further want to investigate whether the areas of residence (urban or rural) within a district impacts a woman's use of contraception.\n",
    "\n",
    "Feel free to indulge in any exploratory visualization which helps you understand the dataset better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize variables for GLM quesion\n",
    "fname: str = 'glm.pickle'\n",
    "vartbl: Dict = load_vartbl(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('bangladesh.csv', sep=';')\n",
    "# Generate map of distinct districts\n",
    "district_id_map = {d:i for i, d in enumerate(sorted(set(df.district)))}\n",
    "# Add new column district_id to the dataset; these are contiguous integers 0-59\n",
    "df['district_id'] = df.district.map(district_id_map)\n",
    "\n",
    "# Rename the columns to avoid the confusing '.' in column names\n",
    "df.rename(axis='columns', inplace=True, mapper=\n",
    "    {'use.contraception':'use_contraception',\n",
    "     'living.children':'living_children',\n",
    "     'age.centered':'age_centered',\n",
    "     })\n",
    "\n",
    "# The number of districts\n",
    "num_districts: int = len(district_id_map)\n",
    "\n",
    "# Create a dataset aggregated by district (sufficient statistic for the by district model)\n",
    "agg_tbl = {\n",
    "        'woman': ['count'],\n",
    "        'use_contraception': ['sum']\n",
    "        } \n",
    "df_district = df.groupby(by=df.district_id).agg(agg_tbl)\n",
    "df_district.columns = [\"_\".join(x) for x in df_district.columns.ravel()]\n",
    "# The number of women sampled in each district\n",
    "district_count = df_district.woman_count\n",
    "\n",
    "# Set the number of samples for this problem (used in multiple parts)\n",
    "num_samples: int = 11000\n",
    "num_tune: int = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the dataset\n",
    "display(df.sample(25).sort_values(by=['woman']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "We will use `use.contraception` as a Bernoulli response variable. \n",
    "\n",
    "When we say \"fit\" below, we mean, specify the model, plot its graph, sample from it, do some tests, and forest-plot and summarize the posteriors, at the very least.\n",
    "\n",
    "**A1** Fit a traditional \"fixed-effects\" model which sets up district-specific intercepts, each with its own Normal(0, 10) prior. That is, the intercept is modeled something like \n",
    "\n",
    "```python\n",
    "alpha_district = pm.Normal('alpha_district', 0, 10, shape=num_districts)\n",
    "p=pm.math.invlogit(alpha_district[df.district_id])\n",
    "```\n",
    "\n",
    "Why should there not be any overall intercept in this model? \n",
    "\n",
    "**A2** Fit a multi-level \"varying-effects\" model with an overall intercept `alpha`, and district-specific intercepts `alpha_district`. Assume that the overall intercept has a Normal(0, 10) prior, while the district specific intercepts are all drawn from the **same** normal distribution with mean 0 and standard deviation $\\sigma$. Let $\\sigma$ be drawn from HalfCauchy(2). The setup of this model is similar to the per-chimanzee models in the prosocial chimanzee labs.\n",
    "\n",
    "**A3** What does a posterior-predictive sample in this model look like? What is the difference between district specific posterior predictives and woman specific posterior predictives. In other words, how might you model the posterior predictive for a new woman being from a particular district vs that os a new woman in the entire sample? This is a word answer; no programming required.\n",
    "\n",
    "**A4** Plot the predicted proportions of women in each district using contraception against the id of the district, in both models. How do these models disagree? Look at the extreme values of predicted contraceptive use in the fixed effects model. How is the disagreement in these cases?\n",
    "\n",
    "**A5** Plot the absolute value of the difference in probability of contraceptive use against the number of women sampled in each district. What do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A1** Fit a traditional \"fixed-effects\" model which sets up district-specific intercepts, each with its own Normal(0, 10) prior. That is, the intercept is modeled something like\n",
    "\n",
    "alpha_district = pm.Normal('alpha_district', 0, 10, shape=num_districts)\n",
    "p=pm.math.invlogit(alpha_district[df.district_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**View Data Aggregeated by District**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[12,8])\n",
    "ax.set_title('Histogram of Count of Women in Each District')\n",
    "ax.set_xlabel('Count of Women')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.hist(df_district.woman_count, bins=20)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[12,8])\n",
    "ax.set_title('Count of Women in Each District')\n",
    "ax.set_xlabel('District ID')\n",
    "ax.set_ylabel('Count of Women')\n",
    "ax.bar(df_district.index.values, df_district.woman_count)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are three districts with small sample sizes.  These will be hard to estimate in this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify the Fixed Effects Model and Draw Samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fixed-effects model\n",
    "with pm.Model() as model_fe:\n",
    "    # Set the prior for the intercept in each district\n",
    "    alpha_district = pm.Normal(name='alpha_district', mu=0.0, sd=10.0, shape=num_districts)\n",
    "    # Set the probability that each woman uses contraception in this model\n",
    "    # It depends only on the district she lives in\n",
    "    p = pm.math.invlogit(alpha_district[df.district_id])\n",
    "    # The response variable - whether this woman used contraception; modeled as Bernoulli\n",
    "    # Bind this to the observed values\n",
    "    use_contraception = pm.Bernoulli('use_contraception', p=p, observed=df['use_contraception'])\n",
    "\n",
    "# Sample from the fixed-effects model\n",
    "try:\n",
    "    trace_fe = vartbl['trace_fe']\n",
    "    print(f'Loaded samples for the Fixed Effects model in trace_fe.')\n",
    "except:\n",
    "    with model_fe:\n",
    "        trace_fe = pm.sample(draws=num_samples, tune=num_tune, chains=2, cores=1)\n",
    "    vartbl['trace_fe'] = trace_fe\n",
    "    save_vartbl(vartbl, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tabular Summary of the 60 alpha Parameters for Fixed Effects Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_fe = pm.summary(trace_fe)\n",
    "display(summary_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Graphs for Fixed Effects Model: Traceplot, Autocorrplot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pm.traceplot(trace_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pm.autocorrplot(trace_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 60 variables (one for each district) it can be a bit difficult to visualize what's going on.  The autocorr plots look OK with very low correlations after the first handful of steps.  Most of the alpha parameters for each district are clustered together.  A few of them have very wide bases; these are the districts with small number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Whisker Plot of Posterior Intercepts for Each District**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples of alpha as a an Nx60 array\n",
    "alpha_samples_fe = trace_fe.get_values('alpha_district')\n",
    "# Arrange the alpha samples into a dataframe for plotting\n",
    "col_names_fe = [f'alpha_{i}' for i in range(num_districts)]\n",
    "df_alpha_samples_fe = pd.DataFrame(data=alpha_samples_fe, columns = col_names_fe)\n",
    "\n",
    "# Generate a whisker plot\n",
    "mpl.rcParams.update({'font.size': 16})\n",
    "fig, ax = plt.subplots(figsize=[12,24])\n",
    "ax.set_title('Fixed Effects Model: Whisker Plot for alpha by District')\n",
    "ax.set_xlabel('Alpha')\n",
    "ax.set_ylabel('District ID')\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_xticks(np.arange(-5,6))\n",
    "ax.grid()\n",
    "sns.boxplot(data=alpha_samples_fe, orient='h', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the earlier plots, this one is very readable.  It shows the alpha parameter (intercept) for each district.  A high alpha means women in that district are more likely to use contraception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forest Plot for Fixed Effects Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,20))\n",
    "gs = pm.forestplot(trace_fe)\n",
    "gs.figure = fig\n",
    "ax1, ax2 = fig.axes\n",
    "ax1 = ax1.set_xlim(0.8, 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests for Fixed Effects Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check n_eff, the number of effective draws\n",
    "n_eff = summary_fe['n_eff']\n",
    "n_eff_mean = np.mean(n_eff)\n",
    "n_eff_min = np.min(n_eff)\n",
    "print(f'Mean of n_eff for {num_districts} alpha parameters is {n_eff_mean:0.0f}')\n",
    "print(f'Min of n_eff is {n_eff_min:0.0f}')\n",
    "\n",
    "# Check Rhat, the correlation between of parameter estimates between chains\n",
    "rhat = summary_fe['Rhat']\n",
    "rhat_mean = np.mean(rhat)\n",
    "rhat_min = np.min(rhat)\n",
    "print(f'\\nMean of RHat for {num_districts} alpha parameters is {rhat_mean:0.3f}')\n",
    "print(f'Min of Rhat is {rhat_min:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of effective draws is high and the correlations between chains are good.  These results seem fine, and don't indicate any sampling problems.  Also there were no warnings from the samplers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Posterior for Fixed Effects Model: alpha for Each District**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pm.plot_posterior(trace_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summarize the Posteriors for Fixed Effects Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start summarizing the posteriors with the table displayed well above with summary statistics for each parameter.  It's a lot to take in because there are 60 districts.  Let's do some higher level summaries below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the weighted average of the mean and sd of alpha \n",
    "# weighting is by the number of women in each district\n",
    "alpha_mean_wtd = np.average(a=summary_fe['mean'], weights=district_count)\n",
    "alpha_sd_wtd = np.average(a=summary_fe['sd'], weights=district_count)\n",
    "\n",
    "print(f'Weighted average of mean and sd alpha, weighted by number of women in each district:')\n",
    "print(f'alpha_mean_wtd: {alpha_mean_wtd:0.4f}')\n",
    "print(f'alpha_sd_wtd:    {alpha_sd_wtd:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is telling us that overall, women in this survey are more likely not to use contraception (mean alpha weighted by size is negative).  The size of the negative alpha is somewhat larger than the weighted average standard deviation, so the effect has a reasonable size.  This lines up with the overall mean rate of contraception use in the surve, 39.25%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why should there not be any overall intercept in this model?**\n",
    "\n",
    "This model fits a parameter $\\alpha$ to each district the determines the probability $p$ that women in the district use contraception according to a logistic distribution.  Adding an overall intercept would just create a new parameter that was colinear with the 60 alphas, one for each district.  It would be redundant.  It could create sampling problems due to misindetification, since a model where the overall intercept was 0.1 higher and all the $alpha_i$ were 0.1 lower would be the same.  It's worth pointing out here that if the priors on the individual districts were grouped more tightly around zero, while the prior for the overall rate was wider, then this model specification might have some benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A2** Fit a multi-level \"varying-effects\" model with an overall intercept `alpha`, and district-specific intercepts `alpha_district`. Assume that the overall intercept has a Normal(0, 10) prior, while the district specific intercepts are all drawn from the **same** normal distribution with mean 0 and standard deviation $\\sigma$. Let $\\sigma$ be drawn from HalfCauchy(2). The setup of this model is similar to the per-chimanzee models in the prosocial chimanzee labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify the Variable Effects Model and Draw Samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the varying-effects model\n",
    "with pm.Model() as model_ve:\n",
    "    # Set the prior for the overall intercept\n",
    "    alpha = pm.Normal(name='alpha', mu=0.0, sd=10.0)\n",
    "    # Set the width sigma for the variability among districts\n",
    "    sigma = pm.HalfCauchy(name='sigma', beta=2.0)\n",
    "    # Set the district-specific alphas to have mean 0 and standard deviation sigma\n",
    "    alpha_district = pm.Normal(name='alpha_district', mu=0.0, sd=sigma, shape=num_districts)    \n",
    "    # Set the probability that each woman uses contraception in this model\n",
    "    # It depends only on the district she lives in\n",
    "    p = pm.math.invlogit(alpha + alpha_district[df.district_id])\n",
    "    # The response variable - whether this woman used contraception; modeled as Bernoulli\n",
    "    # Bind this to the observed values\n",
    "    use_contraception = pm.Bernoulli('use_contraception', p=p, observed=df['use_contraception'])\n",
    "\n",
    "# Sample from the variable-effects model\n",
    "try:\n",
    "    trace_ve = vartbl['trace_ve']\n",
    "    print(f'Loaded samples for the Variable Effects model in trace_ve.')\n",
    "except:\n",
    "    with model_ve:\n",
    "        trace_ve = pm.sample(draws=num_samples, tune=num_tune, chains=2, cores=1)\n",
    "    vartbl['trace_ve'] = trace_ve\n",
    "    save_vartbl(vartbl, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tabular Summary of the 60 alpha Parameters for Variable Effects Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ve = pm.summary(trace_ve)\n",
    "display(summary_ve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Graphs for Fixed Effects Model: Traceplot, Autocorrplot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pm.traceplot(trace_ve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pm.autocorrplot(trace_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additional structure of the variable effects model makes it easier to visualize.  We can see that the alpha and sigma parameters have reasonable looking distributions that are consistent on the two chains.  The traceplots look like pure white noise.  We can see more of a pattern in the individual districts, where the center of each district reflects overall contraception usage in the district and the width reflects the number of women polled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Whisker Plot of Posterior Intercepts for Each District in Variable Effects Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples of alpha as an Nx60 array\n",
    "alpha_overall_ve = trace_ve.get_values('alpha')\n",
    "sigma_ve = trace_ve.get_values('sigma')\n",
    "alpha_district_samples_ve = trace_ve.get_values('alpha_district')\n",
    "alpha_samples_ve = np.hstack([alpha_overall_ve.reshape(-1,1),\n",
    "                              sigma_ve.reshape(-1,1),\n",
    "                              alpha_district_samples_ve])\n",
    "y_labels = ['intercept', 'sigma'] + [f'district {i}' for i in range(num_districts)]\n",
    "\n",
    "# Arrange the alpha samples into a dataframe for plotting\n",
    "col_names_ve = ['alpha', 'sigma'] + [f'alpha_{i}' for i in range(num_districts)]\n",
    "df_alpha_samples_ve = pd.DataFrame(data=alpha_samples_ve, columns = col_names_ve)\n",
    "\n",
    "# Generate a whisker plot\n",
    "mpl.rcParams.update({'font.size': 16})\n",
    "fig, ax = plt.subplots(figsize=[12,24])\n",
    "ax.set_title('Variable Effects Model: Whisker Plot for alpha by District')\n",
    "ax.set_xlabel('Parameter Value')\n",
    "ax.set_ylabel('Parameter Name')\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_xticks(np.arange(-5,6))\n",
    "ax.grid()\n",
    "sns.boxplot(data=alpha_samples_ve, orient='h', ax=ax)\n",
    "ax.set_yticklabels(y_labels)\n",
    "display(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that jumps out here is that the pooled alpha (overall intercept of contraceptive usage) is quite tight compared to most of the districts.  That's good! It shows that the information pooling is working and will help estimate the districts with small sample sizes.  The estimate on $\\sigma$, the standard deviation of the global intercept $\\alpha$, is also quite tight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forest Plot for Variable Effects Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,20))\n",
    "gs = pm.forestplot(trace_ve)\n",
    "gs.figure = fig\n",
    "ax1, ax2 = fig.axes\n",
    "ax1.set_xlim(0.8, 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests for Variable Effects Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check n_eff, the number of effective draws\n",
    "n_eff = summary_ve['n_eff']\n",
    "n_eff_alpha = n_eff[0]\n",
    "n_eff_sigma = n_eff[-1]\n",
    "n_eff_alphas_mean = np.mean(n_eff[1:-1])\n",
    "n_eff_alphas_min = np.min(n_eff[1:-1])\n",
    "print(f'n_eff for overall intercept alpha: {n_eff_alpha:0.0f}')\n",
    "print(f'n_eff for sigma (stdev of alpha):  {n_eff_sigma:0.0f}')\n",
    "print(f'Mean of n_eff for {num_districts} alpha_district parameters is {n_eff_mean:0.0f}')\n",
    "print(f'Min of n_eff is {n_eff_alphas_min:0.0f}')\n",
    "\n",
    "# Check Rhat, the correlation between of parameter estimates between chains\n",
    "rhat = summary_ve['Rhat']\n",
    "rhat_alpha = rhat[0]\n",
    "rhat_sigma = rhat[-1]\n",
    "rhat_alphas_mean = np.mean(rhat[1:-1])\n",
    "rhat_alphas_min = np.min(rhat[1:-1])\n",
    "print(f'\\nRHat for overall intercept alpha: {rhat_alpha:0.3f}')\n",
    "print(f'RHat for sigma (stdev ov alpha):  {rhat_sigma:0.3f}')\n",
    "print(f'Mean of RHat for {num_districts} alpha_district parameters is {rhat_mean:0.3f}')\n",
    "print(f'Min of Rhat is {rhat_alphas_min:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of effective parameters for $\\alpha$ and $\\sigma$, describing the overall mean, are a bit smaller than for the district level parameters.  But they are still quite high and completely adequate.  The minimum for the districts is also OK.\n",
    "The RHat parameters are all very close to 1.0.  These tests are all consistent with good sampling behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Posterior for Variable Effects Model: alpha for Each District**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pm.plot_posterior(trace_ve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summarize the Posteriors for Variable Effects Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the weighted average of the mean and sd of alpha \n",
    "# weighting is by the number of women in each district\n",
    "alpha_mean = summary_ve['mean']['alpha']\n",
    "alpha_std = summary_ve['sd']['alpha']\n",
    "sigma_mean = summary_ve['mean']['sigma']\n",
    "sigma_std = summary_ve['sd']['sigma']\n",
    "\n",
    "print(f'alpha (overall intercept)')\n",
    "print(f'mean: {alpha_mean:0.3f}')\n",
    "print(f'std : {sigma_std:0.3f}')\n",
    "print()\n",
    "print(f'sigma (variability of districts)')\n",
    "print(f'mean: {sigma_mean:0.3f}')\n",
    "print(f'std : {sigma_std:0.3f}')\n",
    "\n",
    "alphas_mean_wtd = np.average(a=summary_ve['mean'][1:-1], weights=district_count)\n",
    "alphas_sd_wtd = np.average(a=summary_ve['sd'][1:-1], weights=district_count)\n",
    "\n",
    "print(f'\\nWeighted average of mean and std alpha by district, weighted by number of women in each district:')\n",
    "print(f'mean: {alphas_mean_wtd:0.4f}')\n",
    "print(f'std:  {alphas_sd_wtd:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new model is easier to interpret.  The global intercept $\\alpha$ has a mean of -0.54 and standard deviation of -0.085. It reflects the overall prevalence of contraceptive use across districts.  \n",
    "\n",
    "$\\sigma$ has a mean of 0.523 and a standard deviation of 0.085.  It reflects how much individual district vary from the global intercept $\\alpha$.\n",
    "\n",
    "The estimates on both $\\alpha$ and $\\sigma$ are fairly tight, reflecting that the model has successfully pooled information across the 60 districts.\n",
    "\n",
    "The mean of $\\alpha_i$ over the districts is 0.073.  It is close to zero because most of the overall departure from 50/50 contraceptive use is being absorbed in the global intercept $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A3** What does a posterior-predictive sample in this model look like? What is the difference between district specific posterior predictives and woman specific posterior predictives. In other words, how might you model the posterior predictive for a new woman being from a particular district vs that of a new woman in the entire sample? This is a word answer; no programming required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One posterior sample in this model assigns a global intercept $\\alpha$, a global variability among districts $\\sigma$, and draws $\\alpha_i$ for the 60 districts.  (This is the posterior over parameter space, not the posterior-predictive over y-space).  The probability $p_i$ that women in a given district use contraceptive is the inverse logistic function of $\\alpha + \\alpha_i$.\n",
    "\n",
    "We can then generate a posterior predictive sample by specifying the number of women in each of the 60 districts.  The probability that one woman who lives in district $i$ uses contraception is $p_i$ as described above.  The probability that $k_i$ of the $n_i$ women living in district $i$ will use contraception will be a binomial distribution with parameters $n_i$, $k_i$ and $p_i$, since the sum of Bernoulli trials is distributed binomially.\n",
    "\n",
    "On the other hand, suppose we don't know anything about a woman and we want to predict the probability she uses contraception.  We can follow a simple ancestral sampling strategy.  First we model the probability that she lives in each of the districts, which we can do based on the overall population breakdown.  Then we take a weighted average over the probabilities for women living in each district."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A4** Plot the predicted proportions of women in each district using contraception against the id of the district, in both models. How do these models disagree? Look at the extreme values of predicted contraceptive use in the fixed effects model. How is the disagreement in these cases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate posterior predictive samples in both models\n",
    "num_samples_ppc: int = 10000\n",
    "try:\n",
    "    post_pred_fe = vartbl['post_pred_fe']\n",
    "    post_pred_ve = vartbl['post_pred_ve']\n",
    "    print(f'Loaded posterior predictive samples for Fixed Effects and Varying Effects models.')\n",
    "except:\n",
    "    post_pred_fe = pm.sample_ppc(trace_fe, samples=num_samples_ppc, model=model_fe)\n",
    "    post_pred_ve = pm.sample_ppc(trace_ve, samples=num_samples_ppc, model=model_ve)\n",
    "    vartbl['post_pred_fe'] = post_pred_fe\n",
    "    vartbl['post_pred_ve'] = post_pred_ve\n",
    "    save_vartbl(vartbl, fname)\n",
    "\n",
    "# Compute the mean contraception use in posterior samples of each model\n",
    "df['use_contraception_fe'] = np.mean(post_pred_fe['use_contraception'], axis=0)\n",
    "df['use_contraception_ve'] = np.mean(post_pred_ve['use_contraception'], axis=0)\n",
    "\n",
    "# Update the aggregated contraception use in each district\n",
    "agg_tbl = {\n",
    "        'woman': ['count'],\n",
    "        'use_contraception': ['mean'],\n",
    "        'use_contraception_fe': ['mean'],\n",
    "        'use_contraception_ve': ['mean'],        \n",
    "        } \n",
    "df_district = df.groupby(by=df.district_id).agg(agg_tbl)\n",
    "df_district.columns = [\"_\".join(x) for x in df_district.columns.ravel()]\n",
    "# Change column names to make model suffix at the end of the name\n",
    "df_district.rename(axis='columns', inplace=True, mapper=\n",
    "    {'use_contraception_fe_mean':'use_contraception_mean_fe',\n",
    "     'use_contraception_ve_mean':'use_contraception_mean_ve',\n",
    "     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_district)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up horizontal bar chart\n",
    "district_id_agg = df_district.index.values\n",
    "# Spacing between models in each district\n",
    "space = 0.3\n",
    "plot_y1 = district_id_agg - space\n",
    "plot_y2 = district_id_agg\n",
    "plot_y3 = district_id_agg + space\n",
    "# Height of each horizontal bar\n",
    "height = 0.2\n",
    "\n",
    "# Plot contraception use for each district\n",
    "fig, ax = plt.subplots(figsize=[16,20])\n",
    "ax.set_title('Contraception Use vs. DistrictID by Model')\n",
    "ax.set_xlabel('Contraception Use in District')\n",
    "ax.set_ylabel('District ID')\n",
    "ax.set_ylim(0, 60)\n",
    "ax.barh(y=plot_y1, width=df_district.use_contraception_mean, height=height, label='Data', color='r')\n",
    "ax.barh(y=plot_y2, width=df_district.use_contraception_mean_fe, height=height, label='FE', color='g')\n",
    "ax.barh(y=plot_y3, width=df_district.use_contraception_mean_ve, height=height, label='VE', color='b')\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do these models disagree?<br>\n",
    "Look at the extreme values of predicted contraceptive use in the fixed effects model. \n",
    "How is the disagreement in these cases?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get districts with min and max contraception use in the FE model\n",
    "district_min_fe = np.argmin(df_district.use_contraception_mean_fe)\n",
    "district_max_fe = np.argmax(df_district.use_contraception_mean_fe)\n",
    "# Display results\n",
    "print(f'District ID {district_min_fe} has Minimum Contraception in Fixed Effect Model')\n",
    "display(df_district.loc[district_min_fe])\n",
    "print(f'District ID {district_max_fe} has Maximum Contraception in Fixed Effect Model')\n",
    "display(df_district.loc[district_max_fe])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 60 districts, it can hard to read off the district numbers, so we can cross reference the chart to to the table above.  It's easiest to spot the districts where contraception use is high.  District ID 2 has 100\\% contraceptive use in the data, but based on a tiny sample of 2 responedents.  The fixed effects model has a much weaker prior and predicts that women in this district will use contraceptives 95.7\\% of the time.  The varying effects model benefits from information pooling across all the districts and does far more shrinkage to the population mean for a district with such a small sample.  It predicts 45.7\\% contraceptive use in this district.\n",
    "\n",
    "District ID 10 has the lowest predicted contraception use among all 60 districts in the fixed effect model.  The FE model predicts 0.5\\% contraception use, whereas the varying effect model predicts 18.2\\% contraception use.  This case is more interesting than the last one.  With 21 women in the sample count, and 0 of them reporting contraception use, it appears at first glance that in this case, the FE model is more accurate, and the VE model may be overcorrecting for the population mean.\n",
    "\n",
    "In general, these two extreme cases give us a good idea for how the two models disagree.  The FE model comes quite close to simply predicting that each district has a contraception use rate close to the observed rate in the data.  This makes it vulnerable to making extreme predictions in districts that have a small number of responses.  The VE model is pooling across all the districts, and performing shrikage to the population mean.  It does more shrinkage in districts with a small response rate.  The two models with vary the most in districts with a small response count that is far from the population mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A5** Plot the absolute value of the difference in probability of contraceptive use against the number of women sampled in each district. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble series for this plot\n",
    "# the x-axis is the number of women in the district\n",
    "plot_x = df_district.woman_count.values\n",
    "# the y-axis is the absolute value of the difference between the FE and VE models\n",
    "plot_y = np.abs(df_district.use_contraception_mean_fe - df_district.use_contraception_mean_ve).values\n",
    "\n",
    "# Generate the plot\n",
    "fig, ax = plt.subplots(figsize=[12,8])\n",
    "ax.set_title('Difference Between FE and VE Models vs. Sample Size')\n",
    "ax.set_xlabel('Sample Size (# Women Polled)')\n",
    "ax.set_ylabel('Abs(pred_FE - pred_VE)')\n",
    "ax.plot(plot_x, plot_y, color='b', marker='o', markersize=8, linewidth=0)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the smaller the sample size, the larger the difference between the FE and VE models.  This is because the VE model is performing more shrinkage on districts with a smaller sample size.  We could have gotten a stronger regression fit if we added a second explanatory variable, the absolute value of the difference between the sample contraception use rate in each district and the overall population contraception use rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B.\n",
    "\n",
    "Let us now fit a model with both varying intercepts by `district_id` (like we did in the varying effects model above) and varying slopes of `urban` by `district_id`. To do this, we will \n",
    "\n",
    "- (a) have an overall intercept, call it `alpha`\n",
    "- (b) have an overall slope of `urban`, call it `beta`.\n",
    "- (c) have district specific intercepts `alpha_district`\n",
    "- (d) district specific slopes for `urban`, `beta_district`\n",
    "- (e) model the co-relation between these slopes and intercepts. \n",
    "\n",
    "We have not modelled covariance and correlation before, so look at http://am207.info/wiki/corr.html for notes on how this is done.\n",
    "\n",
    "To see the ideas behind this, see section 13.2.2 on the income data  from your textbook (included as a pdf in this zip). Feel free to use [code with attribution from Osvaldo Martin](https://github.com/aloctavodia/Statistical-Rethinking-with-Python-and-PyMC3/blob/master/Chp_13.ipynb)..with attribution and understanding...there is some sweet pymc3 technical wrangling in there.\n",
    "\n",
    "**B1** Write down the model as a pymc3 specification and look at its graph. Note that this model builds a 60 by 2 matrix with `alpha_district` values in the first column and `beta_district` values in the second. By assumption, the first column and the second column have correlation structure given by an LKJ prior, but there is no explicit correlation among the rows. In other words, the correlation matrix is 2x2 (not 60x60). Make sure to obtain the value of the off-diagonal correlation as a `pm.Deterministic`. (See Osvaldo Martin's code above)\n",
    "\n",
    "**B2**: Sample from the posterior of the model above *with a target acceptance rate of .9 or more*. (Sampling takes me 7 minutes 30 seconds on my 2013 Macbook Air). Comment on the quality of the samples obtained.\n",
    "\n",
    "**B3** Propose a method based on the reparametrization trick for multi-variate gaussians) of improving the quality of the samples obtained and implement it. (A hint can be obtained from here: https://docs.pymc.io/api/distributions/multivariate.html#pymc3.distributions.multivariate.MvNormal . Using that hint lowered the sampling time to 2.5 minutes on my laptop).\n",
    "\n",
    "**B4** Inspect the trace of the correlation between the intercepts and slopes, plotting the correlation marginal. What does this correlation tell you about the pattern of contraceptive use in the sample? It might help to plot the mean (or median) varying effect estimates for both the intercepts and slopes, by district. Then you can visualize the correlation and maybe more easily think through what it means to have a particular correlation. Also plot the predicted proportion of women using contraception, with urban women on one axis and rural on the other.  Finally, also plot the difference between urban and rural probabilities against rural probabilities. All of these will help you interpret your findings. (Hint: think in terms of low or high rural contraceptive use)\n",
    "\n",
    "**B5** Add additional \"slope\" terms (one-by-one) into the model for \n",
    "\n",
    "- (a) the centered-age of the women and \n",
    "- (b) an indicator for whether the women have a small number or large number of existing kids in the house (you can treat 1-2 kids as low, 3-4 as high, but you might want to experiment with this split). \n",
    "\n",
    "Are any of these effects significant? Are any significant effects similar over the urban/rural divide?\n",
    "\n",
    "**B6** Use WAIC to compare your models. What are your conclusions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B1** Write down the model as a pymc3 specification and look at its graph. Note that this model builds a 60 by 2 matrix with `alpha_district` values in the first column and `beta_district` values in the second. By assumption, the first column and the second column have correlation structure given by an LKJ prior, but there is no explicit correlation among the rows. In other words, the correlation matrix is 2x2 (not 60x60). Make sure to obtain the value of the off-diagonal correlation as a `pm.Deterministic`. (See Osvaldo Martin's code above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pm_make_cov(sigma_priors, corr_coeffs, ndim):\n",
    "    \"\"\"Assemble a covariance matrix single variable standard deviations and correlation coefficients\"\"\"\n",
    "    # Citation: AM 207 lecture notes: http://am207.info/wiki/corr.html\n",
    "    # Diagonal matrix of standard deviation for each varialbes\n",
    "    sigma_matrix = tt.nlinalg.diag(sigma_priors)\n",
    "    # A symmetric nxn matrix has n choose 2 = n(n-1)/2 distinct elements\n",
    "    n_elem = int(ndim * (ndim - 1) / 2)\n",
    "    # Convert between array indexing and [i, j) indexing\n",
    "    tri_index = np.zeros([ndim, ndim], dtype=int)\n",
    "    tri_index[np.triu_indices(ndim, k=1)] = np.arange(n_elem)\n",
    "    tri_index[np.triu_indices(ndim, k=1)[::-1]] = np.arange(n_elem)\n",
    "    # Assemble the covariance matrix using the equation\n",
    "    # CovMat = DiagMat * CorrMat * DiagMat\n",
    "    corr_matrix = corr_coeffs[tri_index]\n",
    "    corr_matrix = tt.fill_diagonal(corr_matrix, 1)\n",
    "    return tt.nlinalg.matrix_dot(sigma_matrix, corr_matrix, sigma_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a varying slopes model incorporating a beta_urban term\n",
    "with pm.Model() as model_vs:  \n",
    "    # Set the prior for the overall intercept\n",
    "    alpha = pm.Normal(name='alpha', mu=0.0, sd=10.0)\n",
    "    # Set the prior for the overall intercept on urban, beta\n",
    "    beta = pm.Normal(name='beta', mu=0.0, sd=10.0)\n",
    "    \n",
    "    # Citation: http://am207.info/wiki/corr.html for code controlling correlation structure\n",
    "    # The parameter nu is the prior on correlation; 0 is uniform, infinity is no corelation\n",
    "    nu = pm.Uniform('nu', 1.0, 5.0)\n",
    "    # The number of dimensions here is 2: correlation structure is bewteen alpha and beta by district\n",
    "    num_factors: int = 2\n",
    "    # Sample the correlation coefficients using the LKJ distribution\n",
    "    corr_coeffs = pm.LKJCorr('corr_coeffs', nu, num_factors)\n",
    "\n",
    "    # Sample the variances of the single factors\n",
    "    sigma_priors = tt.stack([pm.Lognormal('sigma_prior_alpha', mu=0.0, tau=1.0),\n",
    "                             pm.Lognormal('sigma_prior_beta', mu=0.0, tau=1.0)])\n",
    "\n",
    "    # Make the covariance matrix as a Theano tensor\n",
    "    cov = pm.Deterministic('cov', pm_make_cov(sigma_priors, corr_coeffs, num_factors))\n",
    "    # The multivariate Gaussian of (alpha, beta) by district\n",
    "    theta_district = pm.MvNormal('theta_district', mu=[0.0, 0.0], cov=cov, shape=(num_districts, num_factors))   \n",
    "\n",
    "    # The vector of standard deviations for each variable; size num_factors x num_factors\n",
    "    # Citation: efficient generation of sigmas and rhos from cov\n",
    "    # https://github.com/aloctavodia/Statistical-Rethinking-with-Python-and-PyMC3/blob/master/Chp_13.ipynb\n",
    "    sigmas = pm.Deterministic('sigmas', tt.sqrt(tt.diag(cov)))\n",
    "    # correlation matrix (num_factors x num_factors)\n",
    "    rhos = pm.Deterministic('rhos', tt.diag(sigmas**-1).dot(cov.dot(tt.diag(sigmas**-1))))\n",
    "\n",
    "    # Extract the standard deviations of alpha and beta, and the correlation coefficient rho\n",
    "    sigma_alpha = pm.Deterministic('sigma_alpha', sigmas[0])\n",
    "    sigma_beta = pm.Deterministic('sigma_beta', sigmas[1])\n",
    "    rho = pm.Deterministic('rho', rhos[0, 0])\n",
    "\n",
    "    # Extract alpha_district and beta_district from theta_district\n",
    "    alpha_district = pm.Deterministic('alpha_district', theta_district[:,0])\n",
    "    beta_district = pm.Deterministic('beta_district', theta_district[:, 1])\n",
    "\n",
    "    # Set the probability that each woman uses contraception in this model\n",
    "    # It depends on the district she lives in and whether the district is urban\n",
    "    # p = pm.math.invlogit(alpha + alpha_district[df.district_id] + \n",
    "    #                      (beta + beta_district[df.district_id]) * df.urban)\n",
    "    p = pm.math.invlogit(alpha + theta_district[df.district_id, 0] + \n",
    "                         (beta + theta_district[df.district_id, 1]) * df.urban)\n",
    "\n",
    "    # The response variable - whether this woman used contraception; modeled as Bernoulli\n",
    "    # Bind this to the observed values\n",
    "    use_contraception = pm.Bernoulli('use_contraception', p=p, observed=df['use_contraception'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate graphs for each model\n",
    "graph_fe = pm.model_to_graphviz(model_fe)\n",
    "graph_ve = pm.model_to_graphviz(model_ve)\n",
    "graph_vs = pm.model_to_graphviz(model_vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Fixed Effect model\n",
    "print('Fixed Effect Model Graph')\n",
    "display(graph_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Variable Effect model\n",
    "print('Variable Effect Model Graph')\n",
    "display(graph_ve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Variable Slope model\n",
    "print('Variable Slope Model Graph')\n",
    "display(graph_vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B2**: Sample from the posterior of the model above *with a target acceptance rate of .9 or more*. (Sampling takes me 7 minutes 30 seconds on my 2013 Macbook Air). Comment on the quality of the samples obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the varying-slope model\n",
    "try:\n",
    "    trace_vs = vartbl['trace_vs']\n",
    "    print(f'Loaded samples for the Variable Slopes model in trace_vs.')\n",
    "except:\n",
    "    with model_vs:\n",
    "        nuts_kwargs = {'target_accept': 0.90}\n",
    "        trace_vs = pm.sample(draws=num_samples, tune=num_tune, nuts_kwargs=nuts_kwargs, chains=2, cores=1)\n",
    "    vartbl['trace_vs'] = trace_vs\n",
    "    save_vartbl(vartbl, fname)\n",
    "\n",
    "# Summary of the varying-slope model\n",
    "summary_vs = pm.summary(trace_vs)\n",
    "# List of parameters to report\n",
    "params_all = summary_vs.index.values\n",
    "num_pairs = num_factors * (num_factors-1) // 2\n",
    "# The \"key\" parameters\n",
    "params_key = \\\n",
    "    ['alpha', 'beta', 'nu', 'sigma_prior_alpha', 'sigma_prior_beta'] + \\\n",
    "    [f'sigmas__{i}' for i in range(num_factors)] + \\\n",
    "    [f'corr_coeffs__{i}' for i in range(num_pairs)] + \\\n",
    "    [f'cov__{i}_{j}' for i in range(num_factors) for j in range(num_factors)]\n",
    "# The parameters pertaining to different districts\n",
    "params_district = \\\n",
    "        [f'alpha_district__{i}' for i in range(num_districts)] + \\\n",
    "        [f'beta_district__{i}' for i in range(num_districts)]\n",
    "# The parameters to display\n",
    "params = params_key + params_district\n",
    "display(summary_vs.loc[params])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment on the quality of the samples obtained above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable name for the traceplot\n",
    "varnames = ['alpha', 'beta', 'nu', 'sigma_prior_alpha', 'sigma_prior_beta', 'sigmas', 'rho', 'cov']\n",
    "# Generate traceplot\n",
    "fig = pm.traceplot(trace_vs, varnames=varnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate autocorr plot\n",
    "fig = pm.autocorrplot(trace_vs, varnames=varnames[0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of these samples is perhaps adequate, but not great.  The first warning sign came as, well, a warning from the sampler.  Each chain warned of 1 divergence during sampling.  (And this was with a 90% acceptance rate).  It also warned that the number of effective parameters was below 25% for some parameters.\n",
    "\n",
    "The table above puts the parameters into a more \"strategic\" order, with the long vectors of alpha and beta by district at the bottom so they don't overwhelm the first few.  We can focus on these parameters for the plots as well, because the district level parameters seem OK.\n",
    "\n",
    "The traceplots above don't present any obvious problems.  The autocorr plots are more informative.  We can see that there is quite a bit of correlation for the priors on the sigma of alpha and beta.  The covariance entries couldn't be plotted because pymc3 complained that the the array was too deep.\n",
    "\n",
    "The number of effective parameters for sigma_prior_alpha, sigma_prior_beta, the sigmas, the correlation coefficients, and the covariances are all too small relative to the number of samples drawn.  Indeed this deficiency triggered a runtime warning from the sampler, complaining that n_eff was below 25% for some of the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B3** Propose a method based on the reparametrization trick for multi-variate gaussians) of improving the quality of the samples obtained and implement it. (A hint can be obtained from here: https://docs.pymc.io/api/distributions/multivariate.html#pymc3.distributions.multivariate.MvNormal . Using that hint lowered the sampling time to 2.5 minutes on my laptop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two key ideas and they are both included in the hint cited above.  The first idea is that instead of sampling correlation coefficients and standard deviations directly, it is more efficient to sample the entries of a Cholesky factor matrix.  The second idea, which is more important for sampling performance, is the core of the reparameterization trick.  In one dimension, the reparameterization trick tells us that if we want to draw samples for a variable x with mean mu and standard deviation sigma, it is often faster and numerically easier on our sampling back end to instead draw samples for a variable z that is distributed as the standard normal with mean = 0 and standard deviation = 1.  Then we can construct the samples for x by applying the derministic rule \n",
    "$$x_i = \\sigma \\cdot z_i + mu$$\n",
    "\n",
    "The reparameterization trick for multivariate Guassians is completely analogous.  The one difference is that we need to use the \"matrix square root\" of the covariance matrix, which is its Cholesky factor L.  So the analogous equation in the multidimensional case is\n",
    "$$x_i = L \\cdot z_i \\cdot L^T + \\mu$$\n",
    "\n",
    "This is exactly the strategy carried out in the reparameterized model below.  The strategy subtantially sped up sampling.  On a Windows platform unfortunately parallel processing doesn't work.  Runtime on the first sampler for 2 chains with 12,000 samples (2,000 tuning, 10,000 retained) was 10:06 minutes per chain or 22 minutes total.  With the improved version, sampling one chain took about 3:41, a speed-up of 2.74 times.  Exact hardware configuration is a custom built PC with an AMD Ryzen Threadripper 2990WX 32 Core processor running at 3.00 GHz.  Sadly having 32 cores isn't much help when pymc3 on Windows is stuck running on one core at a time...  This notebook is set up to load the samples from a persisted data file if possible, so samples are not run redundantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a varying slopes model incorporating a beta_urban term\n",
    "with pm.Model() as model_vsr:\n",
    "    # Citation: ideas to efficiently reparameterize samples from a MV Gaussian\n",
    "    # https://docs.pymc.io/api/distributions/multivariate.html#pymc3.distributions.multivariate.MvNormal\n",
    "    # Set the prior for the overall intercept\n",
    "    alpha = pm.Normal(name='alpha', mu=0.0, sd=10.0)\n",
    "    # Set the prior for the overall intercept on urban, beta\n",
    "    beta = pm.Normal(name='beta', mu=0.0, sd=10.0)\n",
    "    \n",
    "    # Sample the variances of the single factors\n",
    "    # sd_dist = pm.HalfCauchy.dist(beta=2.5, shape=num_factors)\n",
    "    sd_dist = pm.Lognormal.dist(mu=0.0, tau=1.0, shape=num_factors)\n",
    "    # The parameter nu is the prior on correlation; 0 is uniform, infinity is no corelation\n",
    "    eta = pm.Uniform('nu', 1.0, 5.0)\n",
    "    # The number of dimensions here is 2: correlation structure is bewteen alpha and beta by district\n",
    "    num_factors: int = 2\n",
    "    # Sample the correlation coefficients using the LKJ distribution\n",
    "    chol_packed = pm.LKJCholeskyCov('chol_packed', n=num_factors, eta=eta, sd_dist = sd_dist)\n",
    "    # Expand the packed Cholesky matrix to full size\n",
    "    chol = pm.Deterministic('chol', pm.expand_packed_triangular(num_factors, chol_packed))\n",
    "    # Make the covariance matrix by multiplying out the cholesky factor by its transpose\n",
    "    cov = pm.Deterministic('cov', tt.dot(chol, chol.T))\n",
    "    # The multivariate Gaussian of (alpha, beta) by district\n",
    "    # Decompose this into a \"raw\" part and then scale it\n",
    "    theta_raw = pm.Normal(name='theta_raw', mu=0.0, sd=1.0, shape=(num_districts, num_factors))   \n",
    "    # Now scale these to have the desired covariance structure\n",
    "    theta_district = pm.Deterministic(name='theta_district', var=tt.dot(chol, theta_raw.T).T)\n",
    "    \n",
    "    # The vector of standard deviations for each variable; size num_factors x num_factors\n",
    "    # Citation: efficient generation of sigmas and rhos from cov\n",
    "    # https://github.com/aloctavodia/Statistical-Rethinking-with-Python-and-PyMC3/blob/master/Chp_13.ipynb\n",
    "    sigmas = pm.Deterministic('sigmas', tt.sqrt(tt.diag(cov)))\n",
    "    # correlation matrix (num_factors x num_factors)\n",
    "    rhos = pm.Deterministic('rhos', tt.diag(sigmas**-1).dot(cov.dot(tt.diag(sigmas**-1))))\n",
    "\n",
    "    # Extract the standard deviations of alpha and beta, and the correlation coefficient rho\n",
    "    sigma_alpha = pm.Deterministic('sigma_alpha', sigmas[0])\n",
    "    sigma_beta = pm.Deterministic('sigma_beta', sigmas[1])\n",
    "    rho = pm.Deterministic('rho', rhos[0, 0])\n",
    "\n",
    "    # Extract alpha_district and beta_district from theta_district\n",
    "    alpha_district = pm.Deterministic('alpha_district', theta_district[:,0])\n",
    "    beta_district = pm.Deterministic('beta_district', theta_district[:, 1])\n",
    "\n",
    "    # Set the probability that each woman uses contraception in this model\n",
    "    # It depends on the district she lives in and whether the district is urban\n",
    "    # p = pm.math.invlogit(alpha + alpha_district[df.district_id] + \n",
    "    #                      (beta + beta_district[df.district_id]) * df.urban)\n",
    "    p = pm.math.invlogit(alpha + theta_district[df.district_id, 0] + \n",
    "                         (beta + theta_district[df.district_id, 1]) * df.urban)\n",
    "\n",
    "    # The response variable - whether this woman used contraception; modeled as Bernoulli\n",
    "    # Bind this to the observed values\n",
    "    use_contraception = pm.Bernoulli('use_contraception', p=p, observed=df['use_contraception'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the reparameterized varying-slope model\n",
    "try:\n",
    "    trace_vsr = vartbl['trace_vsr']\n",
    "    print(f'Loaded samples for the Variable Slopes Reparameterized model in trace_vsr.')\n",
    "except:\n",
    "    with model_vsr:\n",
    "        nuts_kwargs = {'target_accept': 0.90}\n",
    "        trace_vsr = pm.sample(draws=num_samples, tune=num_tune, nuts_kwargs=nuts_kwargs, chains=2, cores=1)\n",
    "    vartbl['trace_vsr'] = trace_vsr\n",
    "    save_vartbl(vartbl, fname)\n",
    "\n",
    "# Summary of the variable-effects model\n",
    "summary_vsr = pm.summary(trace_vsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B4** Inspect the trace of the correlation between the intercepts and slopes, plotting the correlation marginal. What does this correlation tell you about the pattern of contraceptive use in the sample? It might help to plot the mean (or median) varying effect estimates for both the intercepts and slopes, by district. Then you can visualize the correlation and maybe more easily think through what it means to have a particular correlation. Also plot the predicted proportion of women using contraception, with urban women on one axis and rural on the other.  Finally, also plot the difference between urban and rural probabilities against rural probabilities. All of these will help you interpret your findings. (Hint: think in terms of low or high rural contraceptive use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the traceplot of the correlation rho\n",
    "pm.traceplot(trace_vsr, varnames=['rho'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summarize Contraception Use by District for Urban vs. Rural, and Mean Parameter Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns for urban and rural contraceptive users\n",
    "df['use_contraception_urban'] = df.use_contraception * df.urban\n",
    "df['use_contraception_rural'] = df.use_contraception * (1-df.urban)\n",
    "\n",
    "# Update the aggregated contraception use in each district\n",
    "agg_tbl = {\n",
    "        'woman': ['count'],\n",
    "        'urban': ['sum', 'mean'],\n",
    "        'use_contraception': ['mean'],\n",
    "        'use_contraception_urban':['sum'],\n",
    "        'use_contraception_rural':['sum'],    \n",
    "        }\n",
    "df_district = df.groupby(by=df.district_id).agg(agg_tbl)\n",
    "df_district.columns = [\"_\".join(x) for x in df_district.columns.ravel()]\n",
    "# Compute number of rural women\n",
    "df_district['rural_sum'] = df_district['woman_count'] - df_district['urban_sum']\n",
    "\n",
    "# Change column names to make model suffix at the end of the name\n",
    "df_district.rename(axis='columns', inplace=True, mapper=\n",
    "    {'use_contraception_mean': 'contraception',\n",
    "     'urban_sum':'urban_count',\n",
    "     'rural_sum':'rural_count',\n",
    "     'use_contraception_urban_sum': 'contraception_urban',\n",
    "     'use_contraception_rural_sum': 'contraception_rural',\n",
    "     })\n",
    "\n",
    "# Change use_contraception_urban and use_contraception_rural to rates\n",
    "df_district.contraception_urban = df_district.contraception_urban / df_district.urban_count\n",
    "df_district.contraception_rural = df_district.contraception_rural / df_district.rural_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of parameter names for alpha_district and beta_district for each district\n",
    "district_suffix = [f'district__{i}' for i in range(num_districts)]\n",
    "params_alpha_district = [f'alpha_{suffix}' for suffix in district_suffix]\n",
    "params_beta_district = [f'beta_{suffix}' for suffix in district_suffix]\n",
    "\n",
    "# Get the mean of alpha_district and beta_district for all the districts; mean taken over samples\n",
    "alpha_district_mean = summary_vsr.loc[params_alpha_district]['mean'].values\n",
    "beta_district_mean = summary_vsr.loc[params_beta_district]['mean'].values\n",
    "\n",
    "# Mean of \"global\" parameters alpha, beta, and rho\n",
    "alpha_mean = summary_vsr.loc['alpha']['mean']\n",
    "beta_mean = summary_vsr.loc['beta']['mean']\n",
    "rho_mean = summary_vsr.loc['rho']['mean']\n",
    "print(f'Mean values of the global parameters alpha, beta and rho:')\n",
    "print(f'alpha: {alpha_mean:0.4f}')\n",
    "print(f'beta:  {beta_mean:0.4f}')\n",
    "print(f'rno:   {rho_mean:0.4f}')\n",
    "\n",
    "# Add alpha_district, beta_district, alpha_eff, beta_eff\n",
    "df_district['alpha_district'] = alpha_district_mean\n",
    "df_district['beta_district'] = beta_district_mean\n",
    "df_district['alpha_eff'] = alpha_mean + alpha_district_mean\n",
    "df_district['beta_eff'] = beta_mean + beta_district_mean\n",
    "\n",
    "# Compute predicted probability for urban and rural contraceptive use\n",
    "df_district['pred_urban'] = logistic.cdf(df_district.alpha_eff + df_district.beta_eff)\n",
    "df_district['pred_rural'] = logistic.cdf(df_district.alpha_eff)\n",
    "\n",
    "display_cols = ['woman_count', 'urban_count', 'rural_count', 'contraception_urban', 'contraception_rural', 'contraception',\n",
    "               'alpha_eff', 'beta_eff', 'pred_urban', 'pred_rural']\n",
    "display(df_district[display_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the mean varying effect estimates for both the incercepts and slopes, by district**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot beta vs. alpha\n",
    "fig, ax = plt.subplots(figsize=[12,8])\n",
    "ax.set_title('Mean Beta vs. Mean Alpha By District')\n",
    "ax.set_xlabel('Mean alpha_district for Each District over Samples')\n",
    "ax.set_ylabel('Mean beta_district for Each District over Samples')\n",
    "ax.plot(alpha_district_mean, beta_district_mean, label='data', color='b', linewidth=0, marker='o', markersize=8)\n",
    "ax.plot(alpha_district_mean, alpha_district_mean * rho_mean, label=r'$\\rho \\alpha$', linewidth=4, color='r')\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the predicted proportion of women using contraception, with urban women on one axis and rural on the other**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=[16,8])\n",
    "ax2 = ax1.twinx()\n",
    "ax1.set_title('Predicted Proportion of Women Using Contraception: Urban and Rural')\n",
    "ax1.set_xlabel('District ID')\n",
    "ax1.set_ylabel('Predicted % Urban Women Using Contraception')\n",
    "ax2.set_ylabel('Predicted % Rural Women Using Contraception')\n",
    "district_ids = df_district.index.values\n",
    "h1 = ax1.plot(district_ids, df_district.pred_urban, label='urban', color='b', linewidth=0, marker='o', markersize=8)\n",
    "h2 = ax2.plot(district_ids, df_district.pred_rural, label='rural', color='g', linewidth=0, marker='o', markersize=8)\n",
    "ax1.legend(handles=[h1[0], h2[0]], labels=['urban', 'rural'])\n",
    "ax1.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[16,8])\n",
    "ax.set_title('Predicted Proportion of Women Using Contraception: Urban and Rural')\n",
    "ax.set_xlabel('District ID')\n",
    "ax.set_ylabel('Predicted % Women Using Contraception')\n",
    "\n",
    "ax.plot(district_ids, df_district.pred_urban, label='urban', color='b', linewidth=0, marker='o', markersize=8)\n",
    "ax.plot(district_ids, df_district.pred_rural, label='rural', color='g', linewidth=0, marker='o', markersize=8)\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the difference between urban and rural probabilities against rural probabilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[16,8])\n",
    "ax.set_title('Predicted Difference in Urban & Rural Probs vs. Rural Prob')\n",
    "ax.set_xlabel('Predicted Rural Contraceptive Prob.')\n",
    "ax.set_ylabel('Predicted Urban - Rural Contraceptive Prob')\n",
    "plot_x = df_district.pred_rural\n",
    "plot_y = df_district.pred_urban - df_district.pred_rural\n",
    "ax.plot(plot_x, plot_y, color='b', linewidth=0, marker='o', markersize=8)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first chart above is a scatter plot with data points for each of the 60 districts.  One dot represents model parameters for one district.  The x-axis is the mean value of alpha for that district (i.e. the intercept for that district).  The y-axis is the mean value of beta for that district (i.e. the change in predicted z-score for contraceptive use for urban women).\n",
    "\n",
    "Let's put the two corners of this plot into words to develop an intuition for what's going on.  At the top left of the graph, we see one district (district 10) where alpha_district is close to -1 and beta_district is close to +1.  Let's also remember that the overall values are alpha = -0.72 and beta = 0.72.  So, in this district, alpha = -0.72 -1.0 = -1.72 and beta = 0.72 + 1.0 = +1.72.  This is saying that rural women who live in this district have a z-score for contraceptive use of -1.72, i.e. they have a 14% probability to use contraception.  The model thinks urban women have a z-score of -1.72 + 1.72 = 0.0, i.e. about half of them use contraception.  It turns out that this district has 21 women, all of whom live in rural areas and none of whom use contraception.  So the parameters describing urban women in this district are essentially noise.\n",
    "\n",
    "At the bottom right of the chart, we see a district with an alpha_district of +1.20 and a beta_district of -1.42 (district id 33).  Adding these district level offsets to the global means, alpha in this district is -0.72 + 1.20 = 0.48 and beta is +0.72 - 1.42 = -0.70.  In this district, rural women use contraceptives with a 62% probatility while urban women use contraceptives with a 45% probability.\n",
    "\n",
    "The next charts allow us to see that \n",
    "\n",
    "* Overall, urban contraception use is higher than rural contraception use\n",
    "* Rural contraceptive use is quite a bit more variable than urban contraception use, which is clustered more tightly\n",
    "* Therefore, in a district that has high overall contraceptive use (large alpha_district), the urban contraceptive use needs to be dialed back with a lower setting on beta_district\n",
    "\n",
    "This is exactly the pattern that we see in the first scatter plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B5** Add additional \"slope\" terms (one-by-one) into the model for \n",
    "\n",
    "- (a) the centered-age of the women and \n",
    "- (b) an indicator for whether the women have a small number or large number of existing kids in the house (you can treat 1-2 kids as low, 3-4 as high, but you might want to experiment with this split). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with slope for the age of the woman;\n",
    "# model_DUA stands for District, Urban, Age\n",
    "with pm.Model() as model_DUA:\n",
    "    # Set the prior for the overall intercept\n",
    "    alpha = pm.Normal(name='alpha', mu=0.0, sd=10.0)\n",
    "    # Set the prior for the overall intercept on urban, beta_age\n",
    "    beta_urban = pm.Normal(name='beta_urban', mu=0.0, sd=10.0)\n",
    "\n",
    "    # Set the prior for the intercept on age, beta_age; this is fixed only, no by district version\n",
    "    # Set the sd of beta_age to 1.0 not 10.0, because the standard deviation of age_centered is 9.0\n",
    "    beta_age = pm.Normal(name='beta_age', mu=0.0, sd=1.0)    \n",
    "    \n",
    "    # Sample the variances of the single factors by district\n",
    "    sd_dist = pm.Lognormal.dist(mu=0.0, tau=1.0, shape=num_factors)\n",
    "    # The parameter nu is the prior on correlation; 0 is uniform, infinity is no corelation\n",
    "    eta = pm.Uniform('nu', 1.0, 5.0)\n",
    "    # The number of dimensions here is 2: correlation structure is bewteen alpha and beta by district\n",
    "    num_factors: int = 2\n",
    "    # Sample the correlation coefficients using the LKJ distribution\n",
    "    chol_packed = pm.LKJCholeskyCov('chol_packed', n=num_factors, eta=eta, sd_dist = sd_dist)\n",
    "    # Expand the packed Cholesky matrix to full size\n",
    "    chol = pm.Deterministic('chol', pm.expand_packed_triangular(num_factors, chol_packed))\n",
    "    # Make the covariance matrix by multiplying out the cholesky factor by its transpose\n",
    "    cov = pm.Deterministic('cov', tt.dot(chol, chol.T))\n",
    "    # The multivariate Gaussian of (alpha, beta) by district\n",
    "    # Decompose this into a \"raw\" part and then scale it\n",
    "    theta_raw = pm.Normal(name='theta_raw', mu=0.0, sd=1.0, shape=(num_districts, num_factors))   \n",
    "    # Now scale these to have the desired covariance structure\n",
    "    theta_district = pm.Deterministic(name='theta_district', var=tt.dot(chol, theta_raw.T).T)\n",
    "    \n",
    "    # Set the probability that each woman uses contraception in this model\n",
    "    # It depends on the district she lives in and whether the district is urban\n",
    "    p = pm.math.invlogit(alpha + theta_district[df.district_id, 0] + \n",
    "                         (beta_urban + theta_district[df.district_id, 1]) * df.urban + \n",
    "                         beta_age * df.age_centered)\n",
    "\n",
    "    # The response variable - whether this woman used contraception; modeled as Bernoulli\n",
    "    # Bind this to the observed values\n",
    "    use_contraception = pm.Bernoulli('use_contraception', p=p, observed=df['use_contraception'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B6** Use WAIC to compare your models. What are your conclusions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Mixture of experts and mixture density networks to solve inverse problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you had to predict a one-to-many function? The data provided below comes from a dataset generated by Chris Bishop (yes that Bishop) to explain the models mentioned in the title above. We have included pdfs from his book which describe these models in some detail. We saw this model earlier in HW where we did an EM like algorithm to obtain a mixture of regressions.\n",
    "\n",
    "The data is in `one-to-many.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we plot the data it looks like this. Notice both the uneven sampling (more towards the center), and the \"more than one y\" for a given x.\n",
    "\n",
    "![](images/inverse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal regression approaches to modeling such a function wont work, as they expect the function to be a proper mathematical function, that is, single valued.\n",
    "\n",
    "These kind of problems are called **inverse problems**, where more than one input state leads to an output state, and we have to try and model these multiple input states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mixture of gaussians (or other distributions) might is a sensible way to do this.\n",
    "\n",
    "You choose one of the gaussians with some probability. The nean of the gaussian is then given by some regression function, say for example a straight line. We could additionally fix the standard deviation or model it as well. \n",
    "\n",
    "Thus, for each component Gaussian, we choose a functional form for the mean and standard deviation. So our model looks something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x)  = \\sum_i \\lambda_i g_i (x) $$\n",
    "\n",
    "Say we fit a model with 3 gaussians to this data. Such a model cannot fit the function above. Notice for example that at $x=0.2$ only one of the gaussians will dominate, different from the situation at $x=0.5$. This means that the probabilities of \"belonging\" to one or the other gaussians is also changing with $x$.\n",
    "\n",
    "If we allow the mixing probabilities to depend on $x$, we can model this situation.\n",
    "\n",
    "$$f(x)  = \\sum_i \\lambda_i (x) g_i (x) $$\n",
    "\n",
    "Such a model is called a \"mixture of experts\" model. The idea is that one \"expert\" gaussian is responsible in one sector of the feature space, while another expert is responsible in another sector.\n",
    "\n",
    "You can think of this model as implementing a \"standard\" gaussian mixture at each \"point\" x, with the added complexity that all of the means, standard deviations, and mixture probabilities change from one x to another.\n",
    "\n",
    "See https://www.cs.toronto.edu/~hinton/absps/hme.pdf and http://www.ee.hacettepe.edu.tr/~eyuksel/Publications/2012_TwentyYearsofMixtureofExperts.pdf for more details. I found the latter clearer and easier to understand.\n",
    "\n",
    "For this entire question you might find diagram code from [here](https://github.com/hardmaru/pytorch_notebooks/blob/master/mixture_density_networks.ipynb) useful. Take with attribution.\n",
    "\n",
    "We will assume we have **3 gaussians**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Variational Mixture of experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll construct a gaussian mixture model of 3 \"expert\" linear regressions. The idea is to create a fit which looks like this:\n",
    "\n",
    "![](images/mixreg.png)\n",
    "\n",
    "Here the three regression lines work in different regions of $f$. We want a pricipled way to sample from this model and to be able to produce posteriors and posterior-predictives.\n",
    "\n",
    "There are 3 parts to this model. First the means of the gaussians in the mixture are modeled with linear regression as shown in the picture above. We will also model $log(\\sigma)$ for each gaussian in the mixture as a linear regression as well ($\\sigma$ needs to be positive).\n",
    "\n",
    "We now need to model the mixture probabilities, i.e., the probabilities required to choose one or the other gaussian. These mixing probabilities, the $\\lambda$s will be modeled as a softmax regression (ie do a linear regression and softmax it to get 3 probabilities).\n",
    "\n",
    "**A1** Write a pymc3 model for this problem. For all biases and weights in your regressions, assume N(0,5) priors. Add noise 0.01 to each of the three $\\sigma$s to make sure you dont have a collapsed 0 width gaussian, ie we want some data in every cluster. (Thus to get the final $\\sigma$, you will exponentiate your regression for $log(\\sigma)$ and add 0.01.)\n",
    "\n",
    "**A2** Fit this model variationally for about 50,000 iterations using the adam optimizer. (`obj_optimizer=pm.adam()`) Plot the ELBO to make sure you have converged. Print summaries and traceplots for the means, $\\sigma$s and probabilities.\n",
    "\n",
    "**A3** Plot the mean posteriors with standard deviations against x. Also produce a diagram like the one above to show the mean\"s with standard deviations showing their uncertainty overlaid on the data.\n",
    "\n",
    "**A4** Plot the posterior predictive (mean and variance) as a function of x) for this model (using `sample_ppc` for example). Why does the posterior predictive look nothing like the data?\n",
    "\n",
    "**A5** Make a \"correct\" posterior predictive diagram by taking into account which \"cluster\" or \"regression line\" the data is coming from. To do this you will need to sample using the softmax probabilities. A nice way to do this is \"Gumbel softmax sampling\". See http://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/ for details. Color-code the predictive samples with the gaussian they came from. Superimpose the predictive on the original data. You may want to contrast a prediction from a point estimate at the mean values of the $\\mu$ and $\\sigma$ traces at a given x (given the picked gaussian) to the \"full\" posterior predictive obtained from sampling from the entire trace of $\\mu$ and $\\sigma$ and $\\lambda$. The former diagram may look something like this:\n",
    "\n",
    "![](images/mixpred.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B. Mixture Density Network\n",
    "\n",
    "A mixture density network (see the enclosed Chapter 5 excerpt from Bishop or https://publications.aston.ac.uk/373/1/NCRG_94_004.pdf) is very closely related to the mixture of experts model. The difference is that we fit the regressions using a neural network where hidden layers are shared amongst the mean, sigma, and mixing probability regressions. (We could have fit 3 separate neural networks in Part A but opted to fit linear regressions for simplicity)\n",
    "\n",
    "(More explanation [here](https://github.com/hardmaru/pytorch_notebooks/blob/master/mixture_density_networks.ipynb). You are welcome to take code from here with attribution.)\n",
    "\n",
    "You job here is to construct a multi-layer perceptron model with a linear hidden layer with 20 units followed by a `Tanh` activation. After the activation layer, 3 separate linear layers with `n_hidden` inputs and `n_gaussian=3` outputs will complete the network. The probabilities part of the network is then passed through a softmax. The means part is left as is. The sigma part is exponentiated and 0.01 added, as in part A\n",
    "\n",
    "Thus the structure looks like:\n",
    "\n",
    "```\n",
    "input:1 --linear-> n_hidden -> Tanh --linear-->n_gaussians      ...mu\n",
    "                            --linear-->n_gaussians->softmax     ...lambda\n",
    "                            --linear-->n_gaussians->exp + 0.01  ...sigma\n",
    "```\n",
    "\n",
    "We then need to use a loss function for the last layer of the network. \n",
    "\n",
    "Using the mean-squared-error loss is not appropriate as the expected value of samples drawn from the sampling distribution of the network will not reflect the 3-gaussian structure (this is the essence of the difference between A4 and A5 above). Thus we'll use the negative loss likelihood of the gaussian mixture model explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B1**: Write the network as a class `MixtureDensityNetwork` which inherits from pytorch `nn.Module`. Implement a constructor which allows at-least the number of hidden layers to be varied. Also implement the `forward` method.\n",
    "\n",
    "**B2**: Train the network using the Adam or similiar optimizer and gradient descent/SGD. Make sure your loss converges and plot this convergence.\n",
    "\n",
    "**B3**: Plot the MLE parameters against x. Make a plot similar to A3 above where you overlay the \"means\" of the gaussians against the data.  Plot traces of the mu/sigma/lambda as an aid in debugging.\n",
    "\n",
    "**B4**: Sample from the sampling distributions at the estimated point values of $\\mu$ and $\\sigma$ (given cluster) to make a plot similar to A5 above\n",
    "\n",
    "**To think but not to hand in** What are the differences between a mixture density network and the mixture of experts. How do these differences translate to feature space? What would happen if we took the shared hidden layer nonlinearity (Tanh) out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C Variational Mixture Density Network\n",
    "\n",
    "We want to implement the Mixture Density Metwork model that we constructed in Part B  directly in pymc3 and use variational inference to sample from it. We  may need more iterations in order to get convergence as this model will likely not converge as fast as the pytorch equivalent.\n",
    "\n",
    "**C1**: Write out the equivalent pymc3 version of the MDN and generate posterior samples with ADVI.\n",
    "\n",
    "**C2**: Sample from the posterior predictive and produce a diagram like B4 and A5 for this model. Plot traces of the mu/sigma/lambda as an aid in debugging your sampler.\n",
    "\n",
    "**C3**: Plot the \"mean\" regression curves (similar to B3 and A3). Do the \"mean\" regression curves in this model look the same from those in Part B?  If they differ why so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3:  Exploring Temperature in Sampling and Optimiztion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At various times in class we've discussed in very vague terms the relation between \"temperature\" and sampling from or finding optima of distributions.  Promises would invariably be made that at some later point we'd discuss the concept of temperature and sampling/optima finding in more detail.  Let's take this problem as an opportunity to keep our promise.\n",
    "\n",
    "Let's start by considering the function $f(x, y)$ defined in the following code cell. $f(x, y)$ is a mixture of three well separated Gaussian probability densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_cov = lambda  theta: np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
    "\n",
    "theta_vec = (5.847707364986893, 5.696776968254305, 1.908095937315489)\n",
    "theta1, theta2, theta3 = theta_vec\n",
    "\n",
    "# define gaussian mixture 1 \n",
    "cov1 = make_cov(theta1)\n",
    "sigma1 = np.array([[2, 0],[0, 1]])\n",
    "mvn1 = scipy.stats.multivariate_normal([12, 7], cov=cov1@sigma1@cov1.T)\n",
    "\n",
    "# define gaussian mixture 2\n",
    "cov2 = make_cov(theta2)\n",
    "sigma2 = np.array([[1, 0],[0, 3]])\n",
    "mvn2 = scipy.stats.multivariate_normal([-1, 6], cov=cov2@sigma2@cov2.T)\n",
    "\n",
    "cov3 = make_cov(theta3)\n",
    "sigma3 = np.array([[.4, 0],[0, 1.3]])\n",
    "mvn3 = scipy.stats.multivariate_normal([3,-2], cov=cov3@sigma3@cov3.T)\n",
    "\n",
    "f = lambda xvec: mvn1.pdf(xvec) + mvn2.pdf(xvec) + .5*mvn3.pdf(xvec)\n",
    "\n",
    "p = lambda x, y: f([x,y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A Visualization and Metropolis\n",
    "\n",
    "**A1**. Visualize $p(x, y)$ with a contour or surface plot.  Make sure to title your plot and label all axes.  What do you notice about $p(x, y)$?  Do you think it will be an easy function to sample?\n",
    "\n",
    "**A2**. Generate 20000 samples from $p(x, y)$ using the Metropolis algorithm.  Pick individual gaussian proposals in $x$ and $y$ with $\\sigma=1$, initial values, burnin parameters, and thinning parameter.  Plot traceplots of the $x$ and $y$ marginals as well as autocorrelation plots.  Plot a pathplot of your samples.  Based on your visualizations, has your Metropolis sampler generated an appropriate representation of the distribution $p(x, y)$?\n",
    "\n",
    "A pathplot is just your samples trace overlaid on your pdf, so that you can see how the sampler traversed. It looks something like this:\n",
    "\n",
    "![](images/pathplot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Changing pdfs using temperature\n",
    "\n",
    "Given a function $p(x)$ we can rewrite that function in following way:\n",
    "\n",
    "$$p(x) = e^{-(-\\log(p(x))}$$\n",
    "\n",
    "So if define the energy density for a function as $E(x)\\equiv-\\log{p(x)}$\n",
    "\n",
    "We can now aim to sample from the function parameratized by a Temperature $T$.\n",
    "\n",
    "$$p(x\\vert T) = e^{-\\frac{1}{T} E(x)} = p(x)^{\\frac{1}{T}}$$\n",
    "\n",
    "If we set T=1 we're sampling from our original function $p(x)$. \n",
    "\n",
    "**B1** In line with A1, visualize modified pdfs (dont worry about normalization) by setting the temperatures to $T=10$ and $T=0.1$. \n",
    "\n",
    "**B2**. Modify your Metropolis algorithm above to take a temperature parameter `T` as well as to keep track of the number of rejected proposals.  Generate 20000 samples from $p(x, y)$ at for each of the following temperatures: {0.1, 1, 3, 7, 10}. Construct  histograms of the marginals, traceplots, autocorrelation plots, and a pathplot for your samples at each temperature.  What happens to the number of rejections as temperature increases? In the limits $T \\rightarrow 0$ and $T \\rightarrow \\infty$ what do you think your samplers will do?\n",
    "\n",
    "**B3**. Approximate the $f(X)$ by the appropriate mixture of Gaussians as a way of generating samples from $f(X)$ to compare with other sampling methods.  Use scipy.stats.multivariate_normal to generate 20000 samples.  How do the histograms compare with the histograms for the samples from $f(X)$ at each temperature.  At what temperature do the samples best represent the function? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Parallel Tempering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen some of the pproperties of sampling at higher temperatures, let's explore a way to incorporate the improved exploration of the *entire pdf* from sampling at higher temperatures while still getting samples that match our distribution.  We'll use a technique called *parallel tempering*.  \n",
    "\n",
    "The general idea of parallel tempering is to simulate $N$ replicas of the original system of interest (in our case, a single Metropolis Hastings chain), each replica at a different temperature. The temperature of a Metropolis Hastings Markov Chain defines how likely it is to sample from a low-density part of the target distribution. The high temperature systems are generally able to sample large volumes of parameter space, whereas low temperature systems, while having precise sampling in a local region of parameter space, may become trapped around local energy minima/probability maxima. Parallel tempering achieves good sampling by allowing the chains at different temperatures to exchange complete configurations. Thus, the inclusion of higher temperature chains ensures that the lower temperature chains can access *all* the low-temperature regions of phase space: the higher temperatures help these chains make the jump-over.\n",
    "\n",
    "Darren Wilkinson's blog post has a [good description](https://darrenjw.wordpress.com/2013/09/29/parallel-tempering-and-metropolis-coupled-mcmc/) of whats going on.\n",
    "\n",
    "Here is the idea that you must implement.\n",
    "\n",
    "There are $N$ replicas each at different temperatures $T_i$ that produce $n$ samples each before possibly swapping states.\n",
    "\n",
    "We simplify matters by only swapping states at adjacent temperatures.  The probability of swapping any two instances of the replicas is given by\n",
    "\n",
    "$$A = min\\left(1, \\frac{p_k(x_{k+1})p_{k+1}(x_k)}{p_k(x_k) p_{k+1}(x_{k+1})}\\right)$$\n",
    "\n",
    "One of the $T_i$'s in our set will always be 1 and this is the only replica that we use as output of the Parallel tempering algorithm.\n",
    "\n",
    "An algorithm for Parallel Tempering is as follows:\n",
    "\n",
    "1. Initialize the parameters $\\{(x_{init}, y_{init})_i\\}, \\{T_i\\}, L$ where \n",
    "    * $L$ is the number of iterations between temperature swap proposals.\n",
    "    * $\\{T_i\\}$ is a list of temperatures.  You'll run one chain at each temperature.\n",
    "    * $\\{(x_{init}, y_{init})_i\\}$ is a list of starting points, one for each chain \n",
    "2. For each chain (one per temperature) use the simple Metropolis code you wrote earlier. Perform $L$ transitions on each chain.\n",
    "3. Set the $\\{(x_{init}, y_{init})_i\\}$ for the next Metropolis run on each chain to the last sample for each chain i.\n",
    "4. Randomly choose 2 chains at adjacent temperatures.\n",
    "    1. Use the above formula to calculate the Acceptance probability $A$.\n",
    "    2. With probability $A$, swap the positions between the 2 chains (that is swap the $x$s of the two chains, and separately swap the $y$s of the chains .\n",
    "5. Go back to 2 above, and start the next L-step epoch \n",
    "6. Continue until you finish $Num. Samples//L$ epochs.\n",
    "\n",
    "\n",
    "**C1**. Explain why swapping states with the given acceptance probability is in keeping with detailed balance. The linked blog post might help.\n",
    "\n",
    "**C2**. Create a parallel tempering sampler that uses 5 chains at  the temperatures {0.1, 1, 3, 7, 10} to sample from $f(x, y)$.  Choose a value of L around 10-20.  Generate 10000 samples from $f(x, y)$.  Construct  histograms of the marginals, traceplots, autocorrelation plots, and a pathplot for your samples.\n",
    "\n",
    "**C3**. How do your samples in **C2** compare to those of the Metropolis sampler?  How do they compare to the samples generated from the Gaussian Mixture approximation of $f(x, y)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D. Global Optima using Simulated Annealing\n",
    "\n",
    "We have new-found intuition about how to use temperature to improve our sampling. Lets now tackle the inverse idea: what happens if you sample at a lower temperature than 1. Our visualizations from Part B should indicate to us that the distributions become extremely tightly peaked arounnd their maxima.\n",
    "\n",
    "If we initialized a metropolis-hastings sampler around an optimum at a really low temperature, it would find us a local minimum. But if we had a higher temperature at the beginning, we can use Metropolis-Hastings sampling at high temperatures to travel around the distribution and find all the peaks (valleys).  Then we will slowly cool down the temperature (which will allow us to escape local optima at higher temperatures) and finally focus us into a particular optimum region and allow you to find the optimum. It can be shown that for ceratin *temperture schedules* this method is guaranteed to find us a global minimum in the limit of infinite iterations.\n",
    "\n",
    "We'll use this methd to find the global minimum of our distribution. The algorithm is as follows. Now we have only one chain, but we very slowly dial down its temperature to below T=1.\n",
    "\n",
    "1. Initialize $(x, y)_i,T, L(T)$ where $L$ is the number of iterations at a particular temperature. \n",
    "2. Perform $L$ transitions thus(we will call this an epoch):\n",
    "    1. Generate a new proposed position $(x, y)_{\\ast}$ using 2 independent gaussians with $\\sigma=1$.\n",
    "    2. If $(x, y)_{\\ast}$ is accepted (according to probability $P = e^{(-\\Delta E/T)}$, set $(x, y)_{i+1} = (x, y)_{\\ast}$, else set $(x, y)_{i+1} = x_{i}$  \n",
    "3. Update T and L \n",
    "4. Until some fixed number of epochs, or until some stop criterion is fulfilled, goto 2.\n",
    "\n",
    "$\\Delta E$ is the change in enery, or the change in the negative log of the probability function. That is, $E = -log p(x,y)$. For a given T and L, this is just Metropolis!\n",
    "\n",
    "This algorithm is called *simulated annealing* and we'll use it to find the global maximum for $f(X)$\n",
    "\n",
    "**D1**. Use simulated annealing with a cooling schedule of $T_{k+1}=0.98T_{k}$ and a L(T) defined initially at 100 with $L_{k+1} = 1.2 L_k$ to find the global optima for $p(x, y)$.  Plot $E(x, y)$ vs iterations.  Given how we constructed $p(x, y)$ it should be fairly straight-forward to observe the  true optima by inspection.  How does the optima found by SA compare to the true optima?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
