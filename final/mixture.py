"""
Michael S. Emanuel
Sun Dec  9 23:25:22 2018
"""

# core
import numpy as np
import pandas as pd
# pymc3 and theano
import pymc3 as pm
from pymc3.variational.callbacks import CheckParametersConvergence
import theano.tensor as tt
from theano.tensor.nnet import softmax
# torch
import torch
import torch.nn as nn
from torch.autograd import Variable 
# plotting
import matplotlib as mpl
import matplotlib.pyplot as plt
# ipython
from IPython.core.display import display
# from IPython import get_ipython
# miscellaneous
from am207_utils import load_vartbl, save_vartbl
from typing import Dict

# *************************************************************************************************
# Q2: Mixture of experts and mixture density networks to solve inverse problems
# *************************************************************************************************
# What if you had to predict a one-to-many function? 
# The data provided below comes from a dataset generated by Chris Bishop (yes that Bishop) 
# to explain the models mentioned in the title above. 
# We have included pdfs from his book which describe these models in some detail. 
# We saw this model earlier in HW where we did an EM like algorithm to obtain a mixture of regressions.

# The data is in one-to-many.csv

# When we plot the data it looks like this. 
# Notice both the uneven sampling (more towards the center), and the "more than one y" for a given x.

# Here the three regression lines work in different regions of f . 
# We want a principled way to sample from this model and to be able to produce posteriors and posterior-predictives.

# There are 3 parts to this model. First the means of the gaussians in the mixture are modeled with linear regression 
# as shown in the picture above. 
# We will also model log(σ) for each gaussian in the mixture as a linear regression as well (σ needs to be positive).

# We now need to model the mixture probabilities, i.e., the probabilities required to choose one or the other gaussian. 
# These mixing probabilities, the λ s will be modeled as a softmax regression 
# (ie do a linear regression and softmax it to get 3 probabilities).

# *************************************************************************************************
# Load persisted table of variables
fname: str = 'mixture.pickle'
vartbl: Dict = load_vartbl(fname)

# Set plotting mode to inline
# https://code.i-harness.com/en/q/1bff0ed
# get_ipython().run_line_magic('matplotlib', 'inline')
# get_ipython().run_line_magic('matplotlib', 'qt')

# Set plot style
mpl.rcParams.update({'font.size': 20})

# Set random seed for reproducibility
np.random.seed(42)

# *************************************************************************************************
# Load the data
df = pd.read_csv('one-to-many.csv')
x = df.x.values
y = df.target.values

# Exploratory plot of the data
fig, ax = plt.subplots(figsize=[12,8])
ax.set_title('One to Many Training Data')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.plot(x, y, color='b', linewidth=0, marker='o', markersize=4)
ax.grid()
plt.close(fig)

# *************************************************************************************************
# A1 Write a pymc3 model for this problem. 
# For all biases and weights in your regressions, assume N(0,5) priors. 
# Add noise 0.01 to each of the three σ s to make sure you dont have a collapsed 0 width gaussian, 
# ie we want some data in every cluster. 
# (Thus to get the final σ, you will exponentiate your regression for log(σ) and add 0.01.)

# Shift applied to sigmas
sigma_shift = 0.01

with pm.Model() as model:
    # The number of data points
    N: int = len(x)
    # The number of functions
    K: int = 3

    # Setting for mu and sigma in normal priors
    normal_prior_mu = 0.0
    normal_prior_sd = 5.0

    # Reshape x to an Nx1 vector (makes matrix multiplications easier to follow)
    xt = tt.reshape(x, (N,1))

    # The parameters for the three regression lines; mean(x) = alpha + x*beta; shape (K)
    alpha = pm.Normal(name='alpha', mu=normal_prior_mu, sd=normal_prior_sd, shape=(K,))
    beta = pm.Normal(name='beta', mu=normal_prior_mu, sd=normal_prior_sd, shape=(K,))

    # Reshape into row vectors
    alpha_row = tt.reshape(alpha, (1,K))
    beta_row = tt.reshape(beta, (1,K))
    
    # The mean of the regression line, mu, is a deterministic function of x = alpha + x * beta; shape (N,K)
    mu = pm.Deterministic('mu', tt.add(alpha_row, tt.dot(xt, beta_row)))
    
    # The parameters for the regression line of log-sigma for each gaussian
    log_sigma_alpha = pm.Normal(name='log_sigma_alpha', mu=normal_prior_mu, sd=normal_prior_sd, shape=(K,))
    log_sigma_beta = pm.Normal(name='log_sigma_beta', mu=normal_prior_mu, sd=normal_prior_sd, shape=(K,))
    # Reshape into row vectors
    log_sigma_alpha_row = tt.reshape(log_sigma_alpha, (1,K))
    log_sigma_beta_row = tt.reshape(log_sigma_beta, (1,K))
    # log_sigma for each gaussian is deterministic
    log_sigma = tt.add(log_sigma_alpha_row, tt.dot(xt, log_sigma_beta_row))
    
    # Sigma for each gaussian is deterministic; include shift of 0.01; shape (N,K)    
    sigma = pm.Deterministic('sigma', tt.exp(log_sigma) + sigma_shift)
    
    # Weighting factors weight_i are modeled as linear regressions also
    weight_alpha = pm.Normal(name='weight_alpha', mu=normal_prior_mu, sd=normal_prior_sd, shape=(K,))
    weight_beta = pm.Normal(name='weight_beta', mu=normal_prior_mu, sd=normal_prior_sd, shape=(K,))
    # Reshape into row vectors
    weight_alpha_row = tt.reshape(weight_alpha, (1,K))
    weight_beta_row = tt.reshape(weight_beta, (1,K))
    
    # The weighting factors are a softmax of weight_alpha + x*weight_beta; shape (N,K)
    weight = pm.Deterministic('weight', softmax(weight_alpha_row + tt.dot(xt, weight_beta_row)))
    
    # Sample points using a pymc3 NormalMixture
    # See lecture notes 25, p. 44
    y_obs = pm.NormalMixture('y_obs', w=weight, mu=mu, sd=sigma, observed=y)
    

# *************************************************************************************************
# A2 Fit this model variationally for about 50,000 iterations using the adam optimizer. 
# (obj_optimizer=pm.adam()) 
# Plot the ELBO to make sure you have converged. 
# Print summaries and traceplots for the means, σ's and probabilities.

# Number of iterations for ADVI fit
num_iters: int = 50000

# Fit the model using ADVI
# Tried to fit using FullRankADVI as well; results were horrible
try:
    advi = vartbl['advi']
    print(f'Loaded ADVI fit for Gaussian Mixture Model.')
except:
    print(f'Running ADVI fit for Gaussian Mixture Model...')
    advi = pm.ADVI(model=model)
    advi.fit(n=num_iters, obj_optimizer=pm.adam(), 
             callbacks=[CheckParametersConvergence()])
    vartbl['advi'] = advi
    save_vartbl(vartbl, fname)


def plot_elbo(elbo, plot_step, title):
    """Generate the ELBO plot"""
    fig, ax = plt.subplots(figsize=[12,8])
    ax.set_title(title)
    ax.set_xlabel('Iteration')
    ax.set_ylabel('ELBO')
    n = len(elbo)
    plot_x = np.arange(0,n,plot_step)
    plot_y = elbo[::plot_step]
    ax.plot(plot_x, plot_y, color='b')
    ax.grid()
    return fig

# Plot the ELBO
fig = plot_elbo(-advi.hist, 10, 'ELBO for ADVI Fit of Gaussian Mixture Model')
plt.close(fig)


# *************************************************************************************************
# Number of samples to draw
num_samples: int = 2000

# Draw parameter samples (trace)
# See lecture 24, p. 33 for example
try:
    trace = vartbl['trace']
    print(f'Loaded trace from ADVI fit of Gaussian Mixture Model.')
except:
    print(f'Drawing posterior samples (parameters) from ADVI fit of Gaussian Mixture Model...')
    trace = advi.approx.sample(num_samples)
    vartbl['trace'] = trace
    save_vartbl(vartbl, fname)

# Print summaries and traceplots for the means, σ's and probabilities.
# figs = pm.traceplot(trace)


# *************************************************************************************************
# A3 Plot the mean posteriors with standard deviations against x. 
# Also produce a diagram like the one above to show the means with standard deviations 
# showing their uncertainty overlaid on the data.

def standardize_gaussians(mu, sigma, weight):
    """Standardize the identities of the three gaussians"""
    # Get index to sort the three series for consistency
    mu_med = np.median(mu, axis=0)
    idx = np.argsort(mu_med, axis=0)[::-1]
    
    # Permute columns of all three arrays
    mu = mu.copy()[:, idx]
    sigma = sigma.copy()[:, idx]
    weight = weight.copy()[:, idx]
    
    return (mu, sigma, weight, idx)


def plot_gaussians(xx, mu, sigma, x, y, title):
    """Generate a plot with the envelope around each of the three gaussians in the mixture"""
    # Compute mu_lo and mu_hi for the envelopes
    mu_lo = mu - 2.0 * sigma
    mu_hi = mu + 2.0 * sigma

    # Frame for the plot
    fig, ax = plt.subplots(figsize=[12,12])
    ax.set_title(title)
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_xlim(0.0, 1.0)
    ax.set_ylim(0.0, 1.0)
    ax.grid()
    
    # Plot the data in black
    ax.plot(x, y, color='k', linewidth=0, marker='o', markersize=3, alpha=0.5)
    # Plot the three lines; match colors to problem
    ax.plot(xx, mu[:,0], color='r')
    ax.plot(xx, mu[:,1], color='b')
    ax.plot(xx, mu[:,2], color='g')
    # Fill in the color between the lower and upper bounds of each line
    ax.fill_between(xx, mu_lo[:,0], mu_hi[:,0], color='r', alpha=0.05)
    ax.fill_between(xx, mu_lo[:,1], mu_hi[:,1], color='b', alpha=0.05)
    ax.fill_between(xx, mu_lo[:,2], mu_hi[:,2], color='g', alpha=0.05)
    return fig


def plot_weights(xx, weight, title):
    """Generate plot with the weights on the three gaussians"""
    fig, ax = plt.subplots(figsize=[12,12])
    ax.set_title(title)
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_xlim(0.0, 1.0)
    ax.set_ylim(0.0, 1.0)
    ax.grid()
    # Plot the weights
    ax.plot(xx, weight[:,0], color='r')
    ax.plot(xx, weight[:,1], color='b')
    ax.plot(xx, weight[:,2], color='g')
    return fig

# Posterior means for the six parameters
post_means = dict()
post_means['alpha'] = np.mean(trace['alpha'], axis=0)
post_means['beta'] = np.mean(trace['beta'], axis=0)
post_means['log_sigma_alpha'] = np.mean(trace['log_sigma_alpha'], axis=0)
post_means['log_sigma_beta'] = np.mean(trace['log_sigma_beta'], axis=0)
post_means['weight_alpha'] = np.mean(trace['weight_alpha'], axis=0)
post_means['weight_beta'] = np.mean(trace['weight_beta'], axis=0)

# Compute posterior means from the trace, which includes the intermediate results for mu, sigma, and weight
mu = np.mean(trace['mu'], axis=0)
sigma = np.mean(trace['sigma'], axis=0)
weight = np.mean(trace['weight'], axis=0)

# Standardize the identities of the three Guassians for consistent colors
mu, sigma, weight, idx = standardize_gaussians(mu, sigma, weight)

# Generate plot similar to one in the problem
fig = plot_gaussians(x, mu, sigma, x, y, 'Mixture Model: 3 Gaussians')
# display(fig)
plt.close(fig)

# Plot the weights
fig = plot_weights(x, weight, 'Mixture Model: Weights')
plt.close(fig)

# *************************************************************************************************
# A4 Plot the posterior predictive (mean and variance) as a function of x) for this model 
# (using sample_ppc for example). Why does the posterior predictive look nothing like the data?

# Draw posterior predictive
# See lecture 24, p. 33 for example
try:
    pred = vartbl['pred']
    print(f'Loaded posterior predictive from ADVI fit of Gaussian Mixture Model.')
except:
    print(f'Drawing posterior predictive from ADVI fit of Gaussian Mixture Model...')
    pred = pm.sample_ppc(trace, model=model)
    vartbl['pred'] = pred
    save_vartbl(vartbl, fname)

# Extract y_pred as an array; shape (num_samples, N)
y_pred = pred['y_obs']
# Mean and standard deviation of y
y_mean = np.mean(y_pred, axis=0)
y_std = np.std(y_pred, axis=0)

def plot_post_mean_std(x, y_mean, y_std, title):
    """Plot posterior mean and std for a set of samples"""
    fig, ax = plt.subplots(figsize=[12,12])
    ax.set_title(title)
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_xlim(0.0, 1.0)
    ax.set_ylim(0.0, 1.0)
    ax.grid()
    
    # Plot posterior mean of y, Shifted +/- 1 SD
    ax.plot(x, y_mean, color='k', linewidth=0, marker='o', label='mean')
    ax.plot(x, y_mean-y_std, color='b', linewidth=0, marker='o', label='mean-std')
    ax.plot(x, y_mean+y_std, color='r', linewidth=0, marker='o', label='mean+std')
    # The standard deviation
    ax.plot(x, y_std, color='magenta', linewidth=0, marker='o', label='std', markersize=3, alpha=0.2)
    ax.legend()
    return fig
  
# Plot the posterior means and stds
fig = plot_post_mean_std(x, y_mean, y_std, 'Mixture Model: Posterior Means +/- 1 SD')
plt.close(fig)

def plot_post_all(x, y_pred, title):
    """Plot all the posterior points in one big cloud"""
    fig, ax = plt.subplots(figsize=[12,12])
    ax.set_title(title)
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_xlim(0.0, 1.0)
    ax.set_ylim(0.0, 1.0)
    ax.grid()
    
    # Plot all the posterior points
    num_samples = y_pred.shape[0]
    x_plot = np.tile(x, (num_samples, 1))
    ax.plot(x_plot, y_pred, color='k', linewidth=0, marker='o', markersize=1, alpha=0.2)
    return fig

# Plot the full cloud of posterior points to show how the posterior mean fails
plot_post_all(x, y_pred[0:200, :], 'All Posterior Points')

# *************************************************************************************************
# A5 Make a "correct" posterior predictive diagram by taking into account which "cluster" 
# or "regression line" the data is coming from. 
# To do this you will need to sample using the softmax probabilities. 
# A nice way to do this is "Gumbel softmax sampling". 
# See http://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/ for details. 
# Color-code the predictive samples with the gaussian they came from. 
# Superimpose the predictive on the original data. 
# You may want to contrast a prediction from a point estimate at the mean values of the μ and σ traces 
# at a given x (given the picked gaussian) to the "full" posterior predictive obtained from sampling from 
# the entire trace of  μ  and  σ  and  λ . 
# The former diagram may look something like this:

def plot_post_cluster(x_pred, y_pred, cluster, x, y, title):
    """Generate plot with the corrected posterior predictive"""
    fig, ax = plt.subplots(figsize=[12,12])
    ax.set_title(title)
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_xlim(0.0, 1.0)
    ax.set_ylim(0.0, 1.0)
    ax.grid()
    # Plot the three clusters in different colors
    markersize=3
    ax.plot(x_pred[cluster==0], y_pred[cluster==0], color='r', linewidth=0, marker='o', markersize=markersize)
    ax.plot(x_pred[cluster==1], y_pred[cluster==1], color='b', linewidth=0, marker='o', markersize=markersize)
    ax.plot(x_pred[cluster==2], y_pred[cluster==2], color='g', linewidth=0, marker='o', markersize=markersize)
    # Plot the original data too
    ax.plot(x, y, color='k', linewidth=0, marker='o', markersize=markersize, alpha=0.5)
    return fig

# Extract the parameters mu, sigma, and weight from the posterior samples
# These all have shape (num_samples, N, 3)
# Use idx to standardize the identities of the clusters!
mu = trace['mu'][:,:,idx]
sigma = trace['sigma'][:,:,idx]
weight = trace['weight'][:,:,idx]

# Perform a simple "ancestral sampling" style strategy
# (I don't see the need for a fancy Gumbel sampling approach here!)
cluster = np.zeros(num_samples, dtype=np.int8)
x_pred_cl = np.zeros(num_samples)
y_pred_cl = np.zeros(num_samples)
for i in range(num_samples):
    # Draw a random row of the sample data
    row_num = np.random.choice(num_samples)
    # Cycle throught the x's in order
    col_num = i % N
    # Sample the cluster assignments according to the weights here
    cluster_i = np.random.choice(a=K, p=weight[row_num, col_num])
    # Draw a sample from this normal
    x_pred_cl[i] = x[col_num]
    y_pred_cl[i] = np.random.normal(loc = mu[row_num, col_num, cluster_i], 
                                  scale = sigma[row_num, col_num, cluster_i])
    # Save the cluster assignment
    cluster[i] = cluster_i

# Plot the corrected posterior predictor based on sampled clusters
fig = plot_post_cluster(x_pred_cl, y_pred_cl, cluster, x, y, 'Posterior Predictive with Clusters')
# display(fig)
plt.close(fig)

# *************************************************************************************************
# Part B. Mixture Density Network
# A mixture density network (see the enclosed Chapter 5 excerpt from Bishop or 
# https://publications.aston.ac.uk/373/1/NCRG_94_004.pdf) is very closely related to the mixture of experts model. 
# The difference is that we fit the regressions using a neural network where hidden layers 
# are shared amongst the mean, sigma, and mixing probability regressions. 
# (We could have fit 3 separate neural networks in Part A but opted to fit linear regressions for simplicity)
# (More explanation here. You are welcome to take code from here with attribution.)
# *************************************************************************************************

# You job here is to construct a multi-layer perceptron model with 
# a linear hidden layer with 20 units followed by a Tanh activation. 
# After the activation layer, 3 separate linear layers with 
# n_hidden inputs and n_gaussian=3 outputs will complete the network. 
# The probabilities part of the network is then passed through a softmax. 
# The means part is left as is. 
# The sigma part is exponentiated and 0.01 added, as in part A

# Thus the structure looks like:
# input:1 --linear-> n_hidden -> Tanh --linear-->n_gaussians      ...mu
#                            --linear-->n_gaussians->softmax     ...lambda
#                            --linear-->n_gaussians->exp + 0.01  ...sigma
# We then need to use a loss function for the last layer of the network.

# Using the mean-squared-error loss is not appropriate as the expected value of samples drawn from 
# the sampling distribution of the network will not reflect the 3-gaussian structure 
# (this is the essence of the difference between A4 and A5 above). 
# Thus we'll use the negative loss likelihood of the gaussian mixture model explicitly.

# *************************************************************************************************
# B1: Write the network as a class MixtureDensityNetwork which inherits from pytorch nn.Module. 
# Implement a constructor which allows at-least the number of hidden layers to be varied. 
# Also implement the forward method.

class MixtureDensityNetwork(nn.Module):
    """Implement a mixture density network as a torch.nn Module subclass."""
    # Citation: the following example was consulted in developing this class
    # No code was copy / pasted into what is presented here
    # (a reference implementation was temporarily copied for testing purposes)
    # https://github.com/hardmaru/pytorch_notebooks/blob/master/mixture_density_networks.ipynb
    def __init__(self, n_hidden: int, n_gaussian: int):
        # Initialize the parent instance of nn.Module
        super(MixtureDensityNetwork, self).__init__()
        
        # The first layer is linear -> n_hidden followed by a tanh activation
        # z should be viewed as the hidden activations
        self.z = nn.Sequential(nn.Linear(1, n_hidden), nn.Tanh())
        
        # A linear layer to predict mu from z_h
        self.mu = nn.Linear(n_hidden, n_gaussian)
        
        # A linear layer followed by exp with an offset to predict sigma
        self.log_sigma = nn.Linear(n_hidden, n_gaussian)
        
        # A linear layer followed by a softmax to predict the mixture weights
        self.weight_z = nn.Linear(n_hidden, n_gaussian)
    
    def forward(self, x):
        """Forward mode for this network"""
        # Compute the hidden activations z_h
        z = self.z(x)
        # Compute the mean mu and standard deviation sigma
        mu = self.mu(z)
        sigma = torch.exp(self.log_sigma(z)) + sigma_shift
        # Compute the weights
        weight = nn.functional.softmax(self.weight_z(z), dim=-1)
        # Return a tuple with weight, sigma, and mu
        return weight, sigma, mu
    
    def set_x(self, x: np.ndarray):
        """Bind x to the network; passed as a numpy array and converted to an autograd Variable"""
        # Length of data, N
        N: int = len(x)
        # Create torch tensor from x
        x_tensor = torch.from_numpy(np.float32(x).reshape((N,1)))
        # Create torch autograd variable from x_tensor and bind it to the network
        self.x = Variable(x_tensor)
    
    def set_y(self, y: np.ndarray):
        """Bind x to the network; passed as a numpy array and converted to an autograd Variable"""
        # Length of data, N
        N: int = len(y)
        # Create torch tensor from x
        y_tensor = torch.from_numpy(np.float32(y).reshape((N,1)))
        # Create torch autograd variable from y_tensor and bind it to the network
        self.y = Variable(y_tensor)
    
    def bind_data(self, x: np.ndarray, y: np.ndarray):
        """Bind the x and y data to the network as autograd variables."""
        # Dispatch calls to set_x and set_y
        self.set_x(x)
        self.set_y(y)
    
    def gaussian(self, mu, sigma, y_variable):
        """
        Evaluate the probability density of y on the gaussian distribution
        Used to compute the likelihood and the loss function
        """
        # Normalization factor for the gaussian distribution
        norm = 1.0 / np.sqrt(2.0*np.pi)
        # Compute inverse of sigma
        sigma_inv = torch.reciprocal(sigma)
        # The error in standardized z units is (y - mu) / sigma
        # Difference between predicted y and mu has shape (N,num_gaussian)
        # call to y.expand_as(mu) is like numpy broadcasting, expands from (N,) to (N,num_gauassian)
        z = sigma_inv * (y_variable.expand_as(mu) - mu)
        # The probability density is f(z) = norm / sigma * exp(-1/2 z^2)
        return (sigma_inv * torch.exp(-0.5 * z * z)) * norm
    
    def loss_func(self, weight, mu, sigma):
        """Compute the loss function with these weights and parameter values."""
        # Compute the likelihood of the predictions
        like = self.gaussian(mu, sigma, self.y) * weight
        # Add this up over all num_gaussian classes
        like = torch.sum(like, dim=1)
        # The loss on each sample is the negative log likelihood
        loss = -torch.log(like)
        # Return the mean loss across the full data set (all N samples)
        return torch.mean(loss)        
    
    def train(self, num_epochs: int):
        """Train this network with an Adam optimizer"""
        # Initialize array of training losses
        self.loss_history = np.zeros(num_epochs)
        # Use Adam optimizer
        self.optimizer = torch.optim.Adam(self.parameters())   
        # Train over num_epochs epochs
        for epoch in range(num_epochs):
            # Evaluate the network on the full set of x data
            # weight_variable, sigma_variable, mu_variable = network(x_variable)
            weight_variable, sigma_variable, mu_variable = self(self.x)
            # Compute the loss function; this is differentiable thanks to autograd
            loss = self.loss_func(weight_variable,  mu_variable, sigma_variable)
            # Take one step with the optimizer to minimize the loss
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            # Save the loss
            loss_current = loss.item()
            self.loss_history[epoch] = loss_current
            # Status update
            if epoch % 500 == 0:
                print(f'Epoch {epoch:5d}; loss {loss_current:0.3f}')

# *************************************************************************************************
# B2: Train the network using the Adam or similiar optimizer and gradient descent/SGD.
#  Make sure your loss converges and plot this convergence.

# Number of epochs to train the network
num_epochs: int = 10000
# File name for saving the trained network
network_fname = 'mixture_network.pt'

# Load the trained network if available; otherwise train it.
try:
    network = torch.load(network_fname)
    print(f'Loaded trained gaussian mixture network.')
except:    
    # Instantiate a mixture density network with 20 hidden units and 3 gaussians
    network = MixtureDensityNetwork(n_hidden=20, n_gaussian=3)
    # Bind x and y data to the network
    network.bind_data(x, y)
    # Train the network
    network.train(num_epochs)
    # Save the model
    torch.save(network, network_fname)

# Plot the loss history for the mixture density network training
fig, ax = plt.subplots(figsize=[12,8])
ax.set_title('Learning Curve for Mixture Density Network')
ax.set_xlabel('Epoch')
ax.set_ylabel('Loss')
ax.plot(np.arange(num_epochs), network.loss_history, color='b')
ax.grid()
plt.close(fig)

# *************************************************************************************************
# B3: Plot the MLE parameters against x. 
# Make a plot similar to A3 above where you overlay the "means" of the gaussians against the data. 
# Plot traces of the mu/sigma/lambda as an aid in debugging.

def network_params(network, x: np.ndarray):
    """Run the network on new x values; return weight, mu and sigma for these parameters"""
    # Build a tensor with this data x
    N: int = len(x)
    x_tensor = torch.from_numpy(np.float32(x).reshape((N,1)))
    # Extract tensors for weight, sigma, and mu by running the network on these points (not the training data!)
    weight_tensor, sigma_tensor, mu_tensor = network.forward(x_tensor)
    # Convert these to "plain old data" in numpy arrays for plotting
    weight = weight_tensor.data.numpy()
    sigma = sigma_tensor.data.numpy()
    mu = mu_tensor.data.numpy()
    # Sort these so order is constent with previous treatment and red, blue, and green line up    
    idx = np.argsort(np.median(mu, axis=0))[::-1]
    weight = weight[:, idx]
    sigma = sigma[:, idx]
    mu = mu[:, idx]
    # Return weight, sigma, mu for these x values
    return weight, sigma, mu

# Plot the three gaussians in the mixture network model
weight, sigma, mu = network_params(network, x)
fig = plot_gaussians(x, mu, sigma, x, y, 'Mixture Density Network: 3 Gaussians')
# display(fig)
plt.close(fig)

# Plot the weights
fig = plot_weights(x, weight, 'Mixture Density Network Weights')
# display(fig)
plt.close(fig)

# *************************************************************************************************
# B4: Sample from the sampling distributions at the estimated point values of 
# μ and σ (given cluster) to make a plot similar to A5 above

# Sample x with the same data as the original sample
weight, sigma, mu = network_params(network, x)

# Initialize arrays for the clusters and predictions
cluster = np.zeros(N, dtype=np.int8)
x_pred = x
y_pred = np.zeros(N)
# Sample N posterior predictive points
for i in range(N):
    # Sample the cluster based on the probabilities above
    cluster_i = np.random.choice(a=3, p=weight[i])
    # The mean and sigma are in row i and the cluster_i column
    mu_i = mu[i, cluster_i]
    sigma_i = sigma[i, cluster_i]
    # Draw a sample from this normal distribution
    y_pred[i] = np.random.normal(loc=mu_i, scale=sigma_i)
    # Save the cluster that produced this sample
    cluster[i] = cluster_i

# Plot the corrected posterior predictor based on sampled clusters
fig = plot_post_cluster(x_pred, y_pred, cluster, x, y, 'Posterior Predictive with Clusters')
# display(fig)
plt.close(fig)


# *************************************************************************************************
# Part C Variational Mixture Density Network
# We want to implement the Mixture Density Metwork model that we constructed in Part B 
# directly in pymc3 and use variational inference to sample from it. 
# We may need more iterations in order to get convergence as this model will likely not 
# converge as fast as the pytorch equivalent.
# *************************************************************************************************

# current parameter estimates
sd = network.state_dict()
network_wts = dict()
# hidden inputs from data input x
network_wts['z_w'] = sd['z.0.weight'].numpy().squeeze()
network_wts['z_b'] = sd['z.0.bias'].numpy().squeeze()
# mean mu from hidden activation z
network_wts['mu_w'] = sd['mu.weight'].numpy().squeeze()
network_wts['mu_b'] = sd['mu.bias'].numpy().squeeze()
# log_sigma from hidden activation z
network_wts['log_sigma_w'] = sd['log_sigma.weight'].numpy().squeeze()
network_wts['log_sigma_b'] = sd['log_sigma.bias'].numpy().squeeze()
# weight_z from hidden activation z
network_wts['weight_z_w'] = sd['weight_z.weight'].numpy().squeeze()
network_wts['weight_z_b'] = sd['weight_z.bias'].numpy().squeeze()


# *************************************************************************************************
# C1: Write out the equivalent pymc3 version of the MDN and generate posterior samples with ADVI.

# alternative version - reference lecture 24, p. 39
# The number of gaussians
K: int = 3
# The number of hidden units
num_hidden: int = 20

# "warm up": fit a completely trivial deteministic model that replicates the neural network
# and then adds random noise to it.  Why? It's much easier to debug this than the real model!
with pm.Model() as model_det:
    """Deterministic model to test the neural network; for diagnostic purposes only!"""
    # reshape x to (1,N)
    xr = x.reshape((1,N))
    # yr = y.reshape((1,N))
    
    # reshape input weights to (20, 1)
    w_in_a1 = network_wts['z_w'].reshape((num_hidden,1))
    b_in_a1 = network_wts['z_b'].reshape((num_hidden,1))
    # z1 = wx+b; sizes (20,1)*(1,N) = (20,N)
    z1 = tt.add(pm.math.dot(w_in_a1, xr), b_in_a1)
    a1 = pm.math.tanh(z1)
    
    # weights for mu have shape (3,20); (3,20) * (20,N) = (3, N)
    w_a1_mu = network_wts['mu_w']
    # bias for mu have shape (3,1)
    b_a1_mu = network_wts['mu_b'].reshape((K,1))
    # mu = w*a1 + b
    mu_val = tt.add(pm.math.dot(w_a1_mu, a1), b_a1_mu)
    # Save mu in the "normal" orientation as a column vector
    mu = pm.Deterministic('mu', mu_val.T)
    
    # log_sigma analogous to mu
    w_a1_log_sigma = network_wts['log_sigma_w']
    b_a1_log_sigma = network_wts['log_sigma_b'].reshape((K,1))
    log_sigma = tt.add(pm.math.dot(w_a1_log_sigma, a1), b_a1_log_sigma)
    
    # sigma from log_sigma
    sigma_val = tt.add(pm.math.exp(log_sigma), sigma_shift)
    # Save sigma as a column vector
    sigma = pm.Deterministic('sigma', sigma_val.T)
    
    # weight_z analogous to mu and log_sigma
    w_a1_weight_z = network_wts['weight_z_w']
    b_a1_weight_z = network_wts['weight_z_b'].reshape((K,1))
    weight_z = tt.add(pm.math.dot(w_a1_weight_z, a1), b_a1_weight_z).T
    
    # weight from weight_z
    weight_val = softmax(weight_z)
    # Save weight as a column vector
    weight = pm.Deterministic('weight', weight_val)
    # Noise vector; so the model has something to train
    noise = pm.Normal('noise', mu=0.0, sd=1.0, shape=(N,1))
    
    y_obs = pm.NormalMixture('y_obs', w=weight, mu=tt.add(mu, noise), sd=sigma, observed=y)
    
# Number of iterations for test model
num_iters_det = 100000

try:
    advi_det = vartbl['advi_det']
    print(f'Loaded ADVI fit for Deterministic NN Model (for testing / debugging).')
except:
    print(f'Running ADVI fit for Deterministic NN Model...')
    advi_det = pm.ADVI(model=model_det)
    advi_det.fit(n=num_iters_det, obj_optimizer=pm.adam(), 
             callbacks=[CheckParametersConvergence()])
    vartbl['advi_det'] = advi_det
    save_vartbl(vartbl, fname)

# Plot ELBO for deterministic model
fig = plot_elbo(-advi_det.hist[20000:], 100, 'ELBO for ADVI Fit of Deterministic (Testing) Model')

# *************************************************************************************************
def draw_samples(weight, mu, sigma):
    """Draw samples with an ancestral sampling approach"""
    # The number of samples
    num_samples: int = len(weight)
    # Initialize arrays with the cluster and predicted points
    cluster = np.zeros(num_samples, dtype=np.int8)
    x_pred_cl = np.zeros(num_samples)
    y_pred_cl = np.zeros(num_samples)
    # Arrays for the sampled parameters
    weight_cl = np.zeros((num_samples, K))
    mu_cl = np.zeros((num_samples, K))
    sigma_cl = np.zeros((num_samples, K))
    # Iterate over samples
    for i in range(num_samples):
        # Draw a random row of the sample data
        row_num = np.random.choice(num_samples)
        # Cycle throught the x's in order
        col_num = i % N
        # Sample the cluster assignments according to the weights here
        cluster_i = np.random.choice(a=K, p=weight[row_num, col_num])
        # Draw a sample from this normal
        x_pred_cl[i] = x[col_num]
        y_pred_cl[i] = np.random.normal(loc = mu[row_num, col_num, cluster_i], 
                                        scale = sigma[row_num, col_num, cluster_i])
        # Save the cluster assignment
        cluster[i] = cluster_i
        # Save diagnostic
        weight_cl[i] = weight[row_num, col_num]
        mu_cl[i] = mu[row_num, col_num]
        sigma_cl[i] = sigma[row_num, col_num]
    return x_pred_cl, y_pred_cl, cluster, weight_cl, mu_cl, sigma_cl

def plot_post_mu(x_pred_cl, y_pred_cl, cluster, x, y, title):
    """Plot the mean of each cluster at the sample points"""
    fig = plot_post_cluster(x_pred_cl, y_pred_cl, cluster, x, y, title)
    # ad hoc plot of mu
    fig, ax = plt.subplots(figsize=[12,12])
    ax.set_title('mu for Mixture Density Network Samples')
    ax.set_xlim(0,1)
    ax.set_ylim(0,1)
    ax.grid()
    ax.plot(x_pred_cl, mu_cl[:,0], color='r', linewidth=0, marker='o')
    ax.plot(x_pred_cl, mu_cl[:,1], color='b', linewidth=0, marker='o')
    ax.plot(x_pred_cl, mu_cl[:,2], color='g', linewidth=0, marker='o')

# Posterior sample from the deterministic model
num_samples = 2000
trace_det = advi_det.approx.sample(num_samples)

# Extract mu, sigma, and weight for posterior sampling with clusters
mu_det = trace_det['mu']
sigma_det = trace_det['sigma']
weight_det = trace_det['weight']

# Draw samples for the deterministic model
x_pred_cl, y_pred_cl, cluster, weight_cl, mu_cl, sigma_cl = draw_samples(weight_det, mu_det, sigma_det)

# Plot the means by cluster
fig = plot_post_mu(x_pred_cl, y_pred_cl, cluster, x, y, 'Posterior Cluster Means: Deterministic Model')
display(fig)
plt.close(fig)

# Plot the posterior sample
fig = plot_post_cluster(x_pred_cl, y_pred_cl, cluster, x, y, 'Posterior Predictive: Deterministic Model')
display(fig)
plt.close(fig)


# *************************************************************************************************
# after the warm-up, ready for the race...

# ML inputs for neural net discovered by training it in Torch
# Reshape input weights to (20, 1)
init_w_in_a1 = network_wts['z_w'].reshape((num_hidden, 1))
init_b_in_a1 = network_wts['z_b'].reshape((num_hidden, 1))
# z1 = wb+b; sizes (20,1) * (1,N) = (20,N)
# weights for mu have shape (3,20);  (3,20) * (20,N) = (3,N)
init_w_a1_mu = network_wts['mu_w']
init_b_a1_mu = network_wts['mu_b'].reshape((K, 1))
# weights for log_sigma have shape (3,20) and (3,1) for the bias
init_w_a1_log_sigma = network_wts['log_sigma_w']
init_b_a1_log_sigma = network_wts['log_sigma_b'].reshape((K, 1))
# weights for weight_z have shape (3,20) and (3,1) for the bias
init_w_a1_weight_z = network_wts['weight_z_w']
init_b_a1_weight_z = network_wts['weight_z_b'].reshape((K, 1))

# pymc3 model for neural network
with pm.Model() as model_mdn:
    # The number of data points
    N: int = len(x)
    # Reshape x to a 1xN row vector
    xr = tt.reshape(x, (1, N))
    
    # Priors for standard deviations of weights and biases in the network
    sd_w: float = 0.1
    sd_b: float = 0.1
    
    # Weights from input to hidden layer
    w_in_a1 = pm.Normal('w_in_a1', mu=init_w_in_a1, sd=sd_w, 
                        shape=(num_hidden, 1), testval=init_w_in_a1)
    b_in_a1 = pm.Normal('b_in_a1', mu=init_b_in_a1, sd=sd_b, 
                        shape=(num_hidden, 1), testval=init_b_in_a1)

    # Weights from hidden layer to mu
    w_a1_mu = pm.Normal('w_a1_mu', mu=init_w_a1_mu, sd=sd_w, 
                        shape=(K, num_hidden), testval=init_w_a1_mu)
    b_a1_mu = pm.Normal('b_a1_mu', mu=init_b_a1_mu, sd=sd_b, 
                        shape=(K, 1), testval=init_b_a1_mu)
    
    # Weights from hidden layer to log_sigma
    w_a1_log_sigma = pm.Normal('w_a1_log_sigma', mu=init_w_a1_log_sigma, sd=sd_w, 
                               shape=(K, num_hidden), testval=init_w_a1_log_sigma)
    b_a1_log_sigma = pm.Normal('b_a1_log_sigma', mu=init_b_a1_log_sigma, sd=sd_b, 
                               shape=(K, 1), testval=init_b_a1_log_sigma)

    # Weights from hidden layer to mixing weights    
    w_a1_weight_z = pm.Normal('w_a1_weight_z', mu=init_w_a1_weight_z, sd=sd_w, 
                              shape=(K, num_hidden), testval=init_w_a1_weight_z)
    b_a1_weight_z = pm.Normal('b_a1_weight_z', mu=init_b_a1_weight_z, sd=sd_b, 
                              shape=(K, 1), testval=init_b_a1_weight_z)
    
    # z1 is the output of the linear layer applied to the input; this is fed to the activation function
    z1 = tt.add(pm.math.dot(w_in_a1, xr), b_in_a1)
    # Activation a1 is tanh of z1
    a1 = pm.math.tanh(z1)
    
    # Mu is linear in the activation
    mu_val = tt.add(pm.math.dot(w_a1_mu, a1), b_a1_mu)
    # Save mu in the "normal" orientation as a column vector
    mu = pm.Deterministic('mu', mu_val.T)

    # Log Sigma is linear in the activation; sigma is exp of this plus the shift
    log_sigma = tt.add(pm.math.dot(w_a1_log_sigma, a1), b_a1_log_sigma)
    # sigma from log_sigma
    sigma_val = tt.add(pm.math.exp(log_sigma), sigma_shift)
    # Save sigma as a column vector
    sigma = pm.Deterministic('sigma', sigma_val.T)

    # Weights are softmax applied to linear of a1
    # This must be transformed to the "normal" NxK orientation so softmax behaves expected below!
    weight_z = tt.add(pm.math.dot(w_a1_weight_z, a1), b_a1_weight_z).T
    # Save the weights as a column vector
    weight = pm.Deterministic('weight', softmax(weight_z))
   
    # Sample points using a pymc3 NormalMixture
    y_obs = pm.NormalMixture('y_obs', w=weight, mu=mu, sd=sigma, observed=y)

# *************************************************************************************************
# Fit this model
# Number of iterations for ADVI fit of mixture density network
num_iters_mdn: int = 100000
try:
    advi_mdn = vartbl['advi_mdn']
    print(f'Loaded ADVI fit for Mixture Density Model.')
except:
    print(f'Running ADVI fit for Mixture Density Model...')
    advi_mdn = pm.ADVI(model=model_mdn)
    advi_mdn.fit(n=num_iters_mdn, obj_optimizer=pm.adam(), 
             callbacks=[CheckParametersConvergence()])
    vartbl['advi_mdn'] = advi_mdn
    save_vartbl(vartbl, fname)

# Plot the ELBO
fig = plot_elbo(-advi_mdn.hist[:], 100, 'ELBO for ADVI Fit of Mixture Density Model')
display(fig)
plt.close(fig)

# *************************************************************************************************
# C2: Sample from the posterior predictive and produce a diagram like B4 and A5 for this model. 
# Plot traces of the mu/sigma/lambda as an aid in debugging your sampler.

# Number of samples to draw for the mixture density network

# Draw parameter samples (trace)
try:
    trace_mdn = vartbl['trace_mdn']
    print(f'Loaded trace from ADVI fit of Mixture Density Model.')
except:
    print(f'Drawing posterior samples (parameters) from ADVI fit of Mixture Density Model...')
    trace_mdn = advi_mdn.approx.sample(num_samples)
    vartbl['trace_mdn'] = trace_mdn
    save_vartbl(vartbl, fname)

# Trace summary for the mixture density network
summary_mdn = pm.summary(trace_mdn)

# Extract mu, sigma, and weight for posterior sampling with clusters
weight_mdn = trace_mdn['weight']
mu_mdn = trace_mdn['mu']
sigma_mdn = trace_mdn['sigma']

# Draw samples for the deterministic model
x_pred_cl, y_pred_cl, cluster, weight_cl, mu_cl, sigma_cl = draw_samples(weight_mdn, mu_mdn, sigma_mdn)

# Plot the means by cluster
fig = plot_post_mu(x_pred_cl, y_pred_cl, cluster, x, y, 'Posterior Cluster Means: Mixture Density Model')
display(fig)
plt.close(fig)

# Plot the posterior sample
fig = plot_post_cluster(x_pred_cl, y_pred_cl, cluster, x, y, 'Posterior Predictive: Mixture Density Model')
display(fig)
plt.close(fig)

#    # Draw posterior predictive
#    # See lecture 24, p. 33 for example
#    try:
#        pred_mdn = vartbl['pred_mdn']
#        print(f'Loaded posterior predictive from ADVI fit of Mixture Density Model.')
#    except:
#        print(f'Drawing posterior predictive from ADVI fit of Mixture Density Model...')
#        pred_mdn = pm.sample_ppc(trace_mdn, model=model_mdn)
#        vartbl['pred_mdn'] = pred_mdn
#        save_vartbl(vartbl, fname)
#        
#    # Extract y_pred as an array; shape (num_samples, N)
#    y_pred = pred_mdn['y_obs']
#    # Mean and standard deviation of y
#    y_mean = np.mean(y_pred, axis=0)
#    y_std = np.std(y_pred, axis=0)
#    
#    fig = plot_post_mean_std(x, y_mean, y_std, 'Mixture Density Model: Posterior Means +/- 1 SD')
#    


# *************************************************************************************************
# C3: Plot the "mean" regression curves (similar to B3 and A3). 
# Do the "mean" regression curves in this model look the same from those in Part B? 
# If they differ why so?

# Sort three clusters
sort_idx = np.argsort(mu_mdn)

# Compute posterior means from the trace, which includes the intermediate results for mu, sigma, and weight
# mu_mean = np.mean(trace_mdn['mu'][sort_idx], axis=0)
# sigma_mean = np.mean(trace_mdn['sigma'][sort_idx], axis=0)
# weight_mean = np.mean(trace_mdn['weight'][sort_idx], axis=0)

# fig = plot_gaussians(x, mu_mean, sigma_mean, x, y, 'Mean Gaussians in Mixture Density Model')

# Standardize the identities of the three Guassians for consistent colors
# mu, sigma, weight, idx = standardize_gaussians(mu_mean, sigma_mean, weight_mean)
