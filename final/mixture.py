"""
Michael S. Emanuel
Sun Dec  9 23:25:22 2018
"""

import numpy as np
import pymc3 as pm
from theano.tensor.nnet import softmax
import pandas as pd

# *************************************************************************************************
# Q2: Mixture of experts and mixture density networks to solve inverse problems
# *************************************************************************************************
# What if you had to predict a one-to-many function? 
# The data provided below comes from a dataset generated by Chris Bishop (yes that Bishop) 
# to explain the models mentioned in the title above. 
# We have included pdfs from his book which describe these models in some detail. 
# We saw this model earlier in HW where we did an EM like algorithm to obtain a mixture of regressions.

# The data is in one-to-many.csv

# When we plot the data it looks like this. 
# Notice both the uneven sampling (more towards the center), and the "more than one y" for a given x.

# Here the three regression lines work in different regions of f . 
# We want a principled way to sample from this model and to be able to produce posteriors and posterior-predictives.

# There are 3 parts to this model. First the means of the gaussians in the mixture are modeled with linear regression 
# as shown in the picture above. 
# We will also model log(σ) for each gaussian in the mixture as a linear regression as well (σ needs to be positive).

# We now need to model the mixture probabilities, i.e., the probabilities required to choose one or the other gaussian. 
# These mixing probabilities, the λ s will be modeled as a softmax regression 
# (ie do a linear regression and softmax it to get 3 probabilities).

# Load the data
df = pd.read_csv('one-to-many.csv')
y = df.target.values
x = df.target.values

# *************************************************************************************************
# A1 Write a pymc3 model for this problem. 
# For all biases and weights in your regressions, assume N(0,5) priors. 
# Add noise 0.01 to each of the three σ s to make sure you dont have a collapsed 0 width gaussian, 
# ie we want some data in every cluster. 
# (Thus to get the final σ, you will exponentiate your regression for log(σ) and add 0.01.)

with pm.Model() as model:
    # The number of functions
    num_funcs: int = 3
    # Setting for mu and sigma in normal priors
    normal_prior_mu = 0.0
    normal_prior_sd = 5.0

    # The alpha parameter for the three regression lines
    alpha = pm.Normal(name='alpha', mu=normal_prior_mu, sd=normal_prior_sd, shape=num_funcs)
    # The beta parameter for the three regression lines
    beta = pm.Normal(name='beta', mu=normal_prior_mu, sd=normal_prior_sd, shape=num_funcs)
    # The mean of the regression line is a deterministic function of x
    reg_mean = pm.Deterministic(name='reg_mean', alpha + beta * x)
    
    # The intercept of log-sigma for each gaussian
    log_sigma_alpha = pm.Normal(name='log_sigma_alpha', mu=normal_prior_mu, sd=normal_prior_sd, shape=num_funcs)
    # The slope of log-sigma for each gaussian
    log_sigma_beta = pm.Normal(name='log_sigma_beta', mu=normal_prior_mu, sd=normal_prior_sd, shape=num_funcs)

    # log_sigma for each gaussian is deterministic; include 0.01 offset
    log_sigma_shift = 0.01
    log_sigma = pm.Deteriministic(name='log_sigma', log_sigma_alpha + log_sigma_beta * x + log_sigma_shift)
    
    # Sigma for each gaussian is deterministic
    reg_sigma = pm.Deterministic('sigma', pm.math.exp(log_sigma))
    
    # Weighting factors lambda_i are modeled as linear regressions also
    weight_alpha = pm.Normal(name='weight_alpha', mu=normal_prior_mu, sd=normal_prior_sd, shape=num_funcs)
    weight_beta = pm.Normal(name='weight_beta', mu=normal_prior_mu, sd=normal_prior_sd, shape=num_funcs)

    # The weighting functions lambda are a softmax of alpha + beta * x
    weight = pm.Deterministic('weight', softmax(weight_alpha + weight_beta * x))
    
    
    

# *************************************************************************************************
# A2 Fit this model variationally for about 50,000 iterations using the adam optimizer. 
# (obj_optimizer=pm.adam()) Plot the ELBO to make sure you have converged. 
# Print summaries and traceplots for the means,  σ s and probabilities.


# *************************************************************************************************
# A3 Plot the mean posteriors with standard deviations against x. 
# Also produce a diagram like the one above to show the mean"s with standard deviations 
# showing their uncertainty overlaid on the data.


# *************************************************************************************************
# A4 Plot the posterior predictive (mean and variance) as a function of x) for this model 
# (using sample_ppc for example). Why does the posterior predictive look nothing like the data?


# *************************************************************************************************
# A5 Make a "correct" posterior predictive diagram by taking into account which "cluster" 
# or "regression line" the data is coming from. 
# To do this you will need to sample using the softmax probabilities. 
# A nice way to do this is "Gumbel softmax sampling". 
# See http://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/ for details. 
# Color-code the predictive samples with the gaussian they came from. 
# Superimpose the predictive on the original data. 
# You may want to contrast a prediction from a point estimate at the mean values of the μ and σ traces 
# at a given x (given the picked gaussian) to the "full" posterior predictive obtained from sampling from 
# the entire trace of  μ  and  σ  and  λ . 
# The former diagram may look something like this:

